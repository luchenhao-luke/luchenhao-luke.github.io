<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2024/06/29/%E7%A7%8D%E5%AD%90%E7%8F%AD%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/06/29/%E7%A7%8D%E5%AD%90%E7%8F%AD%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="2024-06-29"><a href="#2024-06-29" class="headerlink" title="2024/06/29"></a>2024/06/29</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>gzip</strong> 是一种文件压缩格式，用于减小文件大小。</p><p><strong>JSON</strong> 是一种文本格式，用于表示结构化数据。</p><p><strong>JSON Lines (JSONL)</strong> 是一种文件格式，每行为一个独立的 JSON 对象，适合大型数据集的处理。</p><p>Dolma数据：<a href="https://github.com/allenai/dolma">https://github.com/allenai/dolma</a></p><p>原始语料：一般是 jsonl 格式文件，以 gz 等形式压缩</p><p>2419个gz压缩文件</p><p><img src="/2024/06/29/%E7%A7%8D%E5%AD%90%E7%8F%AD%E7%AC%94%E8%AE%B0/截屏2024-06-29 17.25.12.png" alt="截屏2024-06-29 17.25.12"></p><figure class="highlight python"><figcaption><span>scripts/make_wikipedia.py \</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python scripts/make_wikipedia.py \</span><br><span class="line">  --output wikipedia \</span><br><span class="line">  --date <span class="number">20240501</span> \</span><br><span class="line">  --lang simple \</span><br><span class="line">  --processes <span class="number">32</span></span><br></pre></td></tr></table></figure><blockquote><p><code>nproc</code>查看cpu核数，可根据此调整processes，一般为倍数</p></blockquote><p>生成的wiki文本中的格式如下(wiki_00.gz解压缩之后的文件)：</p><p>{“id”: “109”, “source”: “wikipedia”, “version”: “v0”, “text”: “Compound\n\n<templatestyles src="\"Dmbox/styles.css\"">“, “created”: “2024-05-01T00:00:00.000Z”, “added”: “2024-06-29T09:31:09.415Z”, “metadata”: {“revid”: “5040874”, “url”: “<a href="https://simple.wikipedia.org/wiki?curid=109">https://simple.wikipedia.org/wiki?curid=109</a>“, “length”: 12}}</templatestyles></p><p>…</p><p>{“id”: “93”, “source”: “wikipedia”, “version”: “v0”, “text”: “Boil\n\nBoil might mean:\n<templatestyles src="\"Dmbox/styles.css\"">“, “created”: “2024-05-01T00:00:00.000Z”, “added”: “2024-06-29T09:31:09.415Z”, “metadata”: {“revid”: “170917”, “url”: “<a href="https://simple.wikipedia.org/wiki?curid=93">https://simple.wikipedia.org/wiki?curid=93</a>“, “length”: 16}}</templatestyles></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dolma tag \</span><br><span class="line">    --documents &quot;wikipedia/v0/documents/*&quot; \</span><br><span class="line">    --experiment exp \</span><br><span class="line">    --taggers random_number_v1 \</span><br><span class="line">              cld2_en_paragraph_with_doc_score_v2 \</span><br><span class="line">              ft_lang_id_en_paragraph_with_doc_score_v2 \</span><br><span class="line">              char_length_with_paragraphs_v1 \</span><br><span class="line">              whitespace_tokenizer_with_paragraphs_v1 \</span><br><span class="line">    --processes 32</span><br></pre></td></tr></table></figure><h3 id="dedupelicate"><a href="#dedupelicate" class="headerlink" title="dedupelicate"></a>dedupelicate</h3><p>该 <code>dedupe</code> 命令使用 Bloom 过滤器在属性或段落级别对一组文档进行重复数据删除。</p><h2 id="2024-07-02"><a href="#2024-07-02" class="headerlink" title="2024/07/02"></a>2024/07/02</h2><ul><li>bash脚本下载dolma v1.7的数据集</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #!/bin/bash</span></span><br><span class="line"><span class="comment"># DATA_DIR=&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line"><span class="comment"># PARALLEL_DOWNLOADS=&quot;16&quot;</span></span><br><span class="line"><span class="comment"># DOLMA_VERSION=&quot;v1_7&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># git clone https://huggingface.co/datasets/allenai/dolma</span></span><br><span class="line"><span class="comment"># mkdir -p &quot;$&#123;DATA_DIR&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &quot;dolma/urls/$&#123;DOLMA_VERSION&#125;.txt&quot; | xargs -n 1 -P &quot;$&#123;PARALLEL_DOWNLOADS&#125;&quot; wget -q -P &quot;$DATA_DIR&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置变量</span></span><br><span class="line">DATA_DIR=<span class="string">&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line">PARALLEL_DOWNLOADS=<span class="string">&quot;16&quot;</span></span><br><span class="line">DOLMA_VERSION=<span class="string">&quot;v1_7&quot;</span></span><br><span class="line">LOG_FILE=<span class="string">&quot;download_dolma.log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 克隆 Dolma 数据集的存储库</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/allenai/dolma</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据目录，如果目录不存在的话</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">&quot;<span class="variable">$&#123;DATA_DIR&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从指定版本的 URL 文件中读取 URL 并使用 wget 断点续传并显示进度到数据目录</span></span><br><span class="line"><span class="built_in">cat</span> <span class="string">&quot;dolma/urls/<span class="variable">$&#123;DOLMA_VERSION&#125;</span>.txt&quot;</span> | xargs -n 1 -P <span class="string">&quot;<span class="variable">$&#123;PARALLEL_DOWNLOADS&#125;</span>&quot;</span> wget -c --progress=bar:force -P <span class="string">&quot;<span class="variable">$DATA_DIR</span>&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x download_dolma.sh</span><br><span class="line">$ ./download_dolma.sh</span><br></pre></td></tr></table></figure><ul><li>bash脚本校验本地下载完的dolma数据文件和远程dolma数据文件的大小是否相同</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地文件目录</span></span><br><span class="line">local_dir=<span class="string">&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL 文件</span></span><br><span class="line">url_file=<span class="string">&quot;/home/luchenhao/dolma/urls/v1_7.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 url.txt 中的每一行</span></span><br><span class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> -r url; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># 获取文件名</span></span><br><span class="line">    filename=$(<span class="built_in">basename</span> <span class="string">&quot;<span class="variable">$url</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查远程文件大小</span></span><br><span class="line">    remote_size=$(wget --spider <span class="string">&quot;<span class="variable">$url</span>&quot;</span> 2&gt;&amp;1 | grep Length | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查命令是否成功</span></span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Failed to check remote file: <span class="variable">$url</span>&quot;</span></span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取本地文件大小（以字节为单位）</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        local_size=$(<span class="built_in">du</span> -b <span class="string">&quot;<span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span> | <span class="built_in">cut</span> -f1)</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Local file does not exist: <span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span></span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将本地文件大小和远程文件大小进行比较</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$remote_size</span>&quot;</span> -eq <span class="string">&quot;<span class="variable">$local_size</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;File sizes match for <span class="variable">$filename</span>: <span class="variable">$local_size</span> bytes&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;File size mismatch for <span class="variable">$filename</span>: local = <span class="variable">$local_size</span> bytes, remote = <span class="variable">$remote_size</span> bytes&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span> &lt; <span class="string">&quot;<span class="variable">$url_file</span>&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x check_file_sizes.sh</span><br><span class="line">$ ./check_file_sizes.sh</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">File sizes match for cc_en_tail-0443.json.gz: 2568154213 bytes</span><br><span class="line">File sizes match for cc_en_tail-0444.json.gz: 2035889640 bytes</span><br><span class="line">File size mismatch for cc_news-0000.json.gz: local = 5329144093 bytes, remote = 3531963035 bytes</span><br><span class="line">File size mismatch for cc_news-0001.json.gz: local = 4444522614 bytes, remote = 3708288860 bytes</span><br><span class="line">File sizes match for cc_news-0002.json.gz: 3635572395 bytes</span><br><span class="line">File sizes match for cc_news-0003.json.gz: 3726881590 bytes</span><br><span class="line">File sizes match for cc_news-0004.json.gz: 3103556433 bytes</span><br><span class="line">File size mismatch for cc_news-0000.json.gz: local = 5329144093 bytes, remote = 3526498093 bytes</span><br><span class="line">File size mismatch for cc_news-0001.json.gz: local = 4444522614 bytes, remote = 3610880124 bytes</span><br><span class="line">File size mismatch for cc_news-0002.json.gz: local = 3635572395 bytes, remote = 1166286610 bytes</span><br><span class="line">File size mismatch for cc_news-0000.json.gz: local = 5329144093 bytes, remote = 3358108366 bytes</span><br><span class="line">File sizes match for falcon-0000.json.gz: 1795372067 bytes</span><br><span class="line">File sizes match for falcon-0001.json.gz: 1817261729 bytes</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./check_file_sizes.sh &gt; check_file_sizes.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h2 id="docker配置"><a href="#docker配置" class="headerlink" title="docker配置"></a>docker配置</h2><ul><li>创建</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">docker login 10.200.88.53</span><br><span class="line"></span><br><span class="line">docker pull 10.200.88.53/luchenhao-zhejianglab.com/ragllm:v1.0</span><br><span class="line">docker run -itd --gpus all --hostname=Olmo_8xA100 --shm-size=64g -v /mnt/nas_v2/dolma_v1_7:/root/datasets/dolma_v1_7 -p 28022:22 -p 28088:8888 -p 28080:8080 --name luchenhao_v1 10.200.88.53/luchenhao-zhejianglab.com/ragllm:v1.0 /bin/zsh</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">docker run -it --gpus all --hostname=Olmo_8xA100 --shm-size=64g -v /mnt/nas_v2/dolma_v1_7:/root/datasets/dolma_v1_7 -p 18022:22 -p 18080:8080 -p 18088:8888 --name olmo_lch 10.200.88.53/luchenhao-zhejianglab.com/qwen:v0.4</span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao2 --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 48022:22 -p 48080:8080 -p 48088:8888 -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 58022:22 -p 58080:8080 -p 58088:8888 nvidia/cuda:12.1.0-base-ubuntu22.04</span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 18022:22 -p 18080:8080 -p 18088:8888 -v /mnt_llm:/mnt_llm 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.4</span><br><span class="line"></span><br><span class="line">docker exec -it luchenhao bash</span><br><span class="line"></span><br><span class="line">进入自己的docker</span><br><span class="line">执行命令：apt-get update（更新docker的apt命令）</span><br><span class="line">执行命令：apt-get install ssh（安装ssh）</span><br><span class="line">执行命令：vim /etc/ssh/sshd_config（查询ssh配置）</span><br><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">#填加以下内容：</span><br><span class="line">Port 22</span><br><span class="line">PermitRootLogin yes #允许root用户使用ssh登录</span><br><span class="line"></span><br><span class="line">/etc/init.d/ssh restart</span><br><span class="line"></span><br><span class="line">passwd</span><br><span class="line"></span><br><span class="line">ssh root@10.5.30.42 -p 58022</span><br><span class="line"></span><br><span class="line">docker commit -a &quot;luchenhao&quot; luchenhao 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.1</span><br><span class="line">docker push 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.1</span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 58022:22 -p 58080:8080 -p 58088:8888 -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.2</span><br><span class="line"># 拉取 nvidia/cuda:12.3.0-base-ubuntu22.04 镜像创建一个名为 olmo 的容器</span><br><span class="line"># 不使用 --gpus 会导致镜像无法识别显卡</span><br><span class="line"># SCRATCH_DIR 为训练输出文件位置</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus all --hostname=Olmo_4xA40 --shm-size=64g -v /mnt:/root/datasets -p 18022:22 -p 18080:8080 -p 18088:8888 --name olmo_lch 10.200.88.53/luchenhao-zhejianglab.com/qwen:v0.4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker exec -it luchenhao bash</span><br><span class="line"></span><br><span class="line">curl http://10.200.48.108:28022</span><br><span class="line"></span><br><span class="line">docker stats</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 打镜像</span><br><span class="line">docker run -itd --gpus all --hostname=Olmo_8xH100 --shm-size=32g --name OLMO 10.200.88.53/luchenhao-zhejianglab.com/ragllm:v1.0 /bin/bash</span><br><span class="line">docker exec -it OLMO bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao3 --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 38022:22 -p 38080:8080 -p 38088:8888 -v /usr/local/cuda:/usr/local/cuda -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao_v3 --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 38022:22 -p 38080:8080 -p 38088:8888 -v /data1/luchenhao:/data1/luchenhao -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run --name luchenhao_eval -itd -v /data1/luchenhao:/data1/luchenhao -v /mnt/nas_v2:/mnt --gpus all --shm-size 64G -p 48022:22 -p 48080:8080 -p 48088:8888 opencompass:v6 /bin/bash </span><br><span class="line">docker exec -it luchenhao_eval /bin/bash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name luchenhao_v4 --gpus all --hostname=Olmo_4xA40 --shm-size=64g -p 48022:22 -p 48080:8080 -p 48088:8888 -p 48000:8000 -v /usr/local/cuda:/usr/local/cuda12 -v /data1/luchenhao:/data1/luchenhao -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v0.9</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name LLamaFactory --gpus all --hostname=8xH100 --shm-size=64g -p 26022:22 -v /mnt/nas_v2:/mnt 10.200.88.53/luchenhao-zhejianglab.com/olmo:v1.0</span><br></pre></td></tr></table></figure><ul><li>存储</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop luchenhao_v2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/volume2/<span class="number">2</span>/common/public/dataset</span><br><span class="line">mount -t nfs luchenhao_nas:/volume2/<span class="number">2</span>/common/public/dataset /luchenhao_datasets</span><br><span class="line">/volume2/<span class="number">2</span>/common/public/dataset/HC3</span><br><span class="line">mount -t nfs luchenhao_nas:/volume2/<span class="number">2</span>/common/public/dataset/HC3 /luchenhao_datasets</span><br><span class="line"></span><br><span class="line">sudo mount -t nfs <span class="number">10.15</span><span class="number">.35</span><span class="number">.70</span>:/volume2/<span class="number">2</span>/common/public/dataset/HC3 /mnt/luchenhao</span><br></pre></td></tr></table></figure><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python make_wiki.py \</span><br><span class="line">  --output wikipedia \</span><br><span class="line">  --date 20240501 \</span><br><span class="line">  --lang simple \</span><br><span class="line">  --processes 32</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">dolma tag \</span><br><span class="line">    --taggers random_number_v1 \</span><br><span class="line">    --documents &quot;wikipedia/v0/documents/*&quot; \</span><br><span class="line">    --processes 16</span><br><span class="line">    </span><br><span class="line">dolma tag \</span><br><span class="line">    --experiment sample \</span><br><span class="line">    --documents &quot;wikipedia/v0/documents/*&quot; \</span><br><span class="line">    --taggers random_number_v1 \</span><br><span class="line">              cld2_en_paragraph_with_doc_score_v2 \</span><br><span class="line">              char_length_with_paragraphs_v1 \</span><br><span class="line">              whitespace_tokenizer_with_paragraphs_v1 \</span><br><span class="line">    --processes 16</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">dolma tokens \</span><br><span class="line">    --documents &quot;wikipedia/example0/documents/*.gz&quot; \</span><br><span class="line">    --tokenizer.name_or_path &quot;gpt2&quot; \</span><br><span class="line">    --destination wikipedia/example0/tokens \</span><br><span class="line">    --processes 16</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">有一个json.gz文件，里面的数据存储形式如下，每个样本的存储形式是A，如何将这个json.gz文件中的每一个样本转换成B格式？</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;id&quot;: 1,</span><br><span class="line">        &quot;uniqueKey&quot;: &quot;25c6b97fceef629cd4abca434fc9cddc&quot;,</span><br><span class="line">        &quot;titleUkey&quot;: &quot;7a8b0dd07fda2de10f5ddd50d4c0552a&quot;,</span><br><span class="line">        &quot;dataType&quot;: &quot;科技&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;美团王兴发内部信：王慧文将退出公司具体管理事务&quot;,</span><br><span class="line">        &quot;content&quot;: &quot;网易科技讯1月20日消息，今日美团创始人CEO王兴给全体员工发了一封内部信。&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;id&quot;: 2,</span><br><span class="line">        &quot;uniqueKey&quot;: &quot;d82e7009646e4652d55d10c6cd113d8b&quot;,</span><br><span class="line">        &quot;titleUkey&quot;: &quot;7aa60c892a9177b94004a6857ff11fa2&quot;,</span><br><span class="line">        &quot;dataType&quot;: &quot;国际&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;特朗普弹劾“大戏”进入下半场 激烈交锋正在上演&quot;,</span><br><span class="line">        &quot;content&quot;: &quot;原标题：特朗普弹劾“大戏”进入下半场一场激烈交锋正在上演1月15日，美众议院将弹劾总统条款文件呈交参议院。新华社发 美国国会参议院计划本周正式对总统特朗普弹劾案展开审理。 &quot;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">A:  &#123;</span><br><span class="line">        &quot;id&quot;: 2,</span><br><span class="line">        &quot;uniqueKey&quot;: &quot;d82e7009646e4652d55d10c6cd113d8b&quot;,</span><br><span class="line">        &quot;titleUkey&quot;: &quot;7aa60c892a9177b94004a6857ff11fa2&quot;,</span><br><span class="line">        &quot;dataType&quot;: &quot;国际&quot;,</span><br><span class="line">        &quot;title&quot;: &quot;特朗普弹劾“大戏”进入下半场 激烈交锋正在上演&quot;,</span><br><span class="line">        &quot;content&quot;: &quot;原标题：特朗普弹劾“大戏”进入下半场一场激烈交锋正在上演1月15日，美众议院将弹劾总统条款文件呈交参议院。新华社发 美国国会参议院计划本周正式对总统特朗普弹劾案展开审理。 &quot;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">B:   &#123;</span><br><span class="line">    &quot;id&quot;: &quot;...&quot;,             # MANDATORY: source-specific identifier</span><br><span class="line">    &quot;text&quot;: &quot;foo&quot;,           # MANDATORY: textual content of the document</span><br><span class="line">    &quot;source&quot;: &quot;...&quot;,         # MANDATORY: source of the data, such as peS2o, common-crawl, etc.</span><br><span class="line">    &quot;added&quot;: &quot;...&quot;,          # OPTIONAL: timestamp ai2 acquired this data</span><br><span class="line">    &quot;created&quot;: &quot;...&quot;         # OPTIONAL: timestamp when orig document was created (best-guess if not available)</span><br><span class="line">    &quot;metadata&quot;: &#123;...&#125;        # OPTIONAL: source-specific metadata</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/miniconda3/envs/dolma/lib/python3.10/site-packages/dolma/taggers</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">history | grep ossutil</span><br><span class="line"></span><br><span class="line">ossutil ls oss://zhongziban/datasets/\WuDao</span><br><span class="line"></span><br><span class="line">10B的中文</span><br><span class="line">80B的英文</span><br><span class="line">40B的CC</span><br><span class="line">  </span><br><span class="line"> <span class="number">126.4</span>MB的json.gz、 <span class="number">568.00</span> MB的npy文件 、大约是<span class="number">0.15</span>B的tokens</span><br><span class="line"></span><br><span class="line">total_tokens = total_bytes / bytes_per_token</span><br><span class="line"></span><br><span class="line">File size <span class="keyword">for</span> 100B tokens (int32): <span class="number">372.53</span> GB</span><br><span class="line">File size <span class="keyword">for</span> 100B tokens (int64): <span class="number">745.06</span> GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分超大数据集</span></span><br><span class="line">split -l <span class="number">1000000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/ChinaNews-cn/part-006853-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/ChinaNews-cn/part-006853-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/ChinaNews1_cn_split_</span><br><span class="line"></span><br><span class="line">split -l <span class="number">1000000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/ChinaNews-cn/part-008323-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/ChinaNews-cn/part-008323-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/ChinaNews2_cn_split_</span><br><span class="line"></span><br><span class="line">split -l <span class="number">1000000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/Exam-cn/part-003756-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/Exam-cn/part-003756-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/Exam_cn_split_</span><br><span class="line"></span><br><span class="line">split -l <span class="number">1000000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-000036-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-000036-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/WebText1_cn_split_</span><br><span class="line"></span><br><span class="line">ossutil cp -r /mnt/nas_v2/LLM-pretrain-dataset/MNBVC oss://zhongziban/datasets/MNBVC/</span><br><span class="line"></span><br><span class="line">export https_proxy=http://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57901</span> http_proxy=http://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57901</span> all_proxy=socks5://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57901</span></span><br><span class="line"></span><br><span class="line">nohup dolma -c /tokenizer.yaml tokens &gt; /mnt/LLM-pretrain-dataset/luchenhao/logfile.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"></span><br><span class="line">split -l <span class="number">1000000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-000122-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-000122-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/WebText2_cn_split_</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2024/06/29/%E7%A7%8D%E5%AD%90%E7%8F%AD%E7%AC%94%E8%AE%B0/截屏2024-07-09 13.32.59.png" alt="截屏2024-07-09 13.32.59"></p><p>LLaMA [34] 的预训练数据主要包括超过 80% 的网页数据、 来自 GitHub 和 StackExchange 的 6.5% 代码密集型数据、4.5% 的书籍数据,以及来自 arXiv 的 2.5% 科学数据, 这个数据配比成为了训练大语言模型的一个重要参考。<br>根据这个比例,网页数据在现有预训练数据占据了较大的比重,为大语言模型提供了丰富的世界知识。此外,也可以为实现不同的目的来设计特定的数据混合配比。例如,专业的代码模型 CodeGen [94] 大幅增加了代码数据的比例。值得注意的是,即使是在这样的专业模型中,依然需要混合一定的网页数据来提供或者保留通用的语义知识。】】</p><h2 id="语料分类情况下进行词元化"><a href="#语料分类情况下进行词元化" class="headerlink" title="语料分类情况下进行词元化"></a>语料分类情况下进行词元化</h2><div class="table-container"><table><thead><tr><th>语料</th><th>预期配比</th><th>英文中配比</th><th>文件数</th><th>tokens</th><th>路径</th></tr></thead><tbody><tr><td>中文</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>WuDao</td><td></td><td></td><td>7</td><td>2.66B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/WuDao/tokens/train</td></tr><tr><td>WuDaoa_all</td><td></td><td></td><td></td><td>38.21B</td><td></td></tr><tr><td>MNBVC</td><td></td><td></td><td></td><td>0.65B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/MNBVC/MNBVC/tokens</td></tr><tr><td>Baike</td><td></td><td></td><td>6</td><td>0.54B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/Baike/tokens</td></tr><tr><td>Wikipedia-cn</td><td></td><td></td><td>1</td><td>0.13B</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CC</td><td>30</td><td>45.77%</td><td>31</td><td>31.92B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/CC/sample/tokens</td></tr><tr><td>C4</td><td>10</td><td>17.31%</td><td>12</td><td>9.49B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/C4/tokens/sample</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reddit</td><td>5</td><td>14.13%</td><td>5</td><td>5.14B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/reddit/tokens</td></tr><tr><td>RefinedWeb</td><td>5</td><td>10.7%</td><td>6</td><td>5.54B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/RefinedWeb/tokens</td></tr><tr><td>StackExchange</td><td>2.5</td><td>6.42%</td><td>4</td><td>2.64B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/StackExchange/tokens</td></tr><tr><td>books</td><td>2.5</td><td>3.72%</td><td>2</td><td>2.90B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/books/tokens</td></tr><tr><td>arxiv</td><td>2.5</td><td>1.95%</td><td>8</td><td>2.5B</td><td>/mnt/LLM-pretrain-dataset/luchenhao/arxiv/tokens</td></tr><tr><td>Starcoder</td><td>2.5</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TOTAL</td><td></td><td></td><td></td><td>136.97B</td></tr></tbody></table></div><h2 id="语料不分类情况下进行词元化"><a href="#语料不分类情况下进行词元化" class="headerlink" title="语料不分类情况下进行词元化"></a>语料不分类情况下进行词元化</h2><h2 id="Dolma-Pipeline"><a href="#Dolma-Pipeline" class="headerlink" title="Dolma Pipeline"></a>Dolma Pipeline</h2><ol><li><p>python /reformat.py</p></li><li><p>dolma -c /taggers.yaml tag</p></li><li><p>dolma -c /dedupe.yaml dedupe</p></li><li><p>dolma -c /mix.yaml mix</p></li><li><p>dolma -c /tokenizer.yaml tokens  -&gt; npy文件</p></li><li><p>/OLMo-1B.yamldata: </p><p> ​    paths:- /mnt/geogpt-gpfs/llmcourse/public/datasets/npy_data/RedPajamaGithub/0199_00000.npy</p><p> ​          - /mnt/geogpt-gpfs/llmcourse/public/datasets/npy_data/RedPajamaGithub/1087_00000.npy</p></li><li><p>CUDA_VISIBLE_DEVICES=1 torchrun —nproc_per_node=1 /OLMo/scripts/train.py configs/OLMo-1B.yaml —save-folder=olmo-run</p></li></ol><h2 id="数据下载-python-reformat-py"><a href="#数据下载-python-reformat-py" class="headerlink" title="数据下载 python /reformat.py"></a>数据下载 python /reformat.py</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #!/bin/bash</span></span><br><span class="line"><span class="comment"># DATA_DIR=&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line"><span class="comment"># PARALLEL_DOWNLOADS=&quot;16&quot;</span></span><br><span class="line"><span class="comment"># DOLMA_VERSION=&quot;v1_7&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># git clone https://huggingface.co/datasets/allenai/dolma</span></span><br><span class="line"><span class="comment"># mkdir -p &quot;$&#123;DATA_DIR&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &quot;dolma/urls/$&#123;DOLMA_VERSION&#125;.txt&quot; | xargs -n 1 -P &quot;$&#123;PARALLEL_DOWNLOADS&#125;&quot; wget -q -P &quot;$DATA_DIR&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置变量</span></span><br><span class="line">DATA_DIR=<span class="string">&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line">PARALLEL_DOWNLOADS=<span class="string">&quot;16&quot;</span></span><br><span class="line">DOLMA_VERSION=<span class="string">&quot;v1_7&quot;</span></span><br><span class="line">LOG_FILE=<span class="string">&quot;download_dolma.log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 克隆 Dolma 数据集的存储库</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/allenai/dolma</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据目录，如果目录不存在的话</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">&quot;<span class="variable">$&#123;DATA_DIR&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从指定版本的 URL 文件中读取 URL 并使用 wget 断点续传并显示进度到数据目录</span></span><br><span class="line"><span class="built_in">cat</span> <span class="string">&quot;dolma/urls/<span class="variable">$&#123;DOLMA_VERSION&#125;</span>.txt&quot;</span> | xargs -n 1 -P <span class="string">&quot;<span class="variable">$&#123;PARALLEL_DOWNLOADS&#125;</span>&quot;</span> wget -c --progress=bar:force -P <span class="string">&quot;<span class="variable">$DATA_DIR</span>&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup ./download_dolma.sh &gt; script_output.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地文件目录</span></span><br><span class="line">local_dir=<span class="string">&quot;/mnt/nas_v2/dolma_v1_7&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL 文件</span></span><br><span class="line">url_file=<span class="string">&quot;/home/luchenhao/dolma/urls/v1_7.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 url.txt 中的每一行</span></span><br><span class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> -r url; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># 获取文件名</span></span><br><span class="line">    filename=$(<span class="built_in">basename</span> <span class="string">&quot;<span class="variable">$url</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查远程文件大小</span></span><br><span class="line">    remote_size=$(wget --spider <span class="string">&quot;<span class="variable">$url</span>&quot;</span> 2&gt;&amp;1 | grep Length | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查命令是否成功</span></span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Failed to check remote file: <span class="variable">$url</span>&quot;</span></span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取本地文件大小（以字节为单位）</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="string">&quot;<span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        local_size=$(<span class="built_in">du</span> -b <span class="string">&quot;<span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span> | <span class="built_in">cut</span> -f1)</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Local file does not exist: <span class="variable">$local_dir</span>/<span class="variable">$filename</span>&quot;</span></span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将本地文件大小和远程文件大小进行比较</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$remote_size</span>&quot;</span> -eq <span class="string">&quot;<span class="variable">$local_size</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;File sizes match for <span class="variable">$filename</span>: <span class="variable">$local_size</span> bytes&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;File size mismatch for <span class="variable">$filename</span>: local = <span class="variable">$local_size</span> bytes, remote = <span class="variable">$remote_size</span> bytes&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span> &lt; <span class="string">&quot;<span class="variable">$url_file</span>&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup ./check_file_sizes.sh &gt; script_output2.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure><h2 id="数据转换（关键key添加、转换为Jsonl、压缩）"><a href="#数据转换（关键key添加、转换为Jsonl、压缩）" class="headerlink" title="数据转换（关键key添加、转换为Jsonl、压缩）"></a>数据转换（关键key添加、转换为Jsonl、压缩）</h2><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ history | grep reformat</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to_b_format</span>(<span class="params">a_sample</span>):</span><br><span class="line">    b_sample = &#123;</span><br><span class="line">        <span class="string">&quot;id&quot;</span>: <span class="built_in">str</span>(a_sample[<span class="string">&quot;id&quot;</span>]),</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: a_sample[<span class="string">&quot;content&quot;</span>],</span><br><span class="line">        <span class="string">&quot;source&quot;</span>: <span class="string">&quot;WuDaoCorpus2&quot;</span>,  <span class="comment"># 可以根据实际情况调整</span></span><br><span class="line">        <span class="string">&quot;added&quot;</span>: datetime.now().isoformat(),  <span class="comment"># 当前时间</span></span><br><span class="line">        <span class="string">&quot;created&quot;</span>: <span class="literal">None</span>,  <span class="comment"># 假设创建时间不确定，可以根据实际情况调整</span></span><br><span class="line">        <span class="string">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;uniqueKey&quot;</span>: a_sample[<span class="string">&quot;uniqueKey&quot;</span>],</span><br><span class="line">            <span class="string">&quot;titleUkey&quot;</span>: a_sample[<span class="string">&quot;titleUkey&quot;</span>],</span><br><span class="line">            <span class="string">&quot;dataType&quot;</span>: a_sample[<span class="string">&quot;dataType&quot;</span>],</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: a_sample[<span class="string">&quot;title&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> b_sample</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">json2Dolmaformat</span>(<span class="params">sourcePath, desFile</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(sourcePath,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(desFile, <span class="string">&#x27;wt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fn:</span><br><span class="line">        <span class="keyword">for</span> segment <span class="keyword">in</span> tqdm(data, desc=<span class="string">&quot;Processing and writing segments&quot;</span>, unit=<span class="string">&quot;segment&quot;</span>):</span><br><span class="line">            b_format_data = convert_to_b_format(segment)</span><br><span class="line">            json.dump(b_format_data, fn)</span><br><span class="line">            fn.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    sourcePath = <span class="string">&#x27;/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/part-2021009337.json&#x27;</span>  <span class="comment"># 修改为实际文件路径</span></span><br><span class="line">    desFile = <span class="string">&#x27;/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents/part-2021009337_reformat.jsonl.gz&#x27;</span>  <span class="comment"># 修改为实际输出文件路径</span></span><br><span class="line">    json2Dolmaformat(sourcePath, desFile)</span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python /reformat_all.py</span><br></pre></td></tr></table></figure><h2 id="Tagger"><a href="#Tagger" class="headerlink" title="Tagger"></a>Tagger</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dolma tag --documents &quot;$&#123;HOME&#125;/perplexity/v2/documents/*/*/*.gz&quot; --taggers uniseg_length_paragraphs_with_empty_v1 not_alphanum_paragraph_v1 --processes 188</span></span><br><span class="line"><span class="comment"># dolma tag --documents &quot;$&#123;HOME&#125;/perplexity/v2_small/documents/*/*/*.gz&quot; --taggers uniseg_length_paragraphs_with_empty_v1 not_alphanum_paragraph_v1 --processes 188</span></span><br><span class="line"><span class="comment"># dolma tag --documents &quot;$&#123;HOME&#125;/perplexity/v3/documents/*/*/*.gz&quot; --taggers uniseg_length_paragraphs_with_empty_v1 not_alphanum_paragraph_v1 --processes 188</span></span><br><span class="line"><span class="comment"># dolma tag --documents &quot;$&#123;HOME&#125;/perplexity/v2_small_subset/documents/*/*/*.gz&quot; --taggers uniseg_length_paragraphs_with_empty_v1 not_alphanum_paragraph_v1 --processes 188</span></span><br><span class="line"></span><br><span class="line"><span class="attr">documents:</span></span><br><span class="line">  <span class="comment"># - /mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/*.json</span></span><br><span class="line">  <span class="comment"># - /mnt/dolma_v1_7/documents/*.gz</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents/*.gz</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">taggers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">c4_v2</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ft_lang_id_en_doc_v2</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">uniseg_length_paragraphs_with_empty_v1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">not_alphanum_paragraph_v1</span></span><br><span class="line">  <span class="comment"># - tokenizer_repetitions_v1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">char_length_strip_ws_v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">processes:</span> <span class="number">188</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dolma -c /tagger.yaml tag</span><br></pre></td></tr></table></figure><h2 id="Deduplicate"><a href="#Deduplicate" class="headerlink" title="Deduplicate"></a>Deduplicate</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">documents:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents/*.gz</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dedupe:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">wanjuan</span></span><br><span class="line">  <span class="attr">documents:</span></span><br><span class="line">    <span class="attr">attribute_name:</span> <span class="string">bff_duplicates</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">$.text</span></span><br><span class="line">  <span class="attr">skip_empty:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">bloom_filter:</span></span><br><span class="line">  <span class="attr">read_only:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">estimated_doc_count:</span> <span class="number">49679</span></span><br><span class="line">  <span class="comment"># size_in_bytes: 104857600  # 100 MB; smaller causes too many FPs</span></span><br><span class="line">  <span class="attr">desired_false_positive_rate:</span> <span class="number">1e-15</span></span><br><span class="line">  <span class="attr">file:</span> <span class="string">$&#123;oc.env:HOME&#125;/perplexity/filters/paloma_documents.bin</span></span><br><span class="line"></span><br><span class="line"><span class="attr">processes:</span> <span class="number">188</span></span><br></pre></td></tr></table></figure><h2 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h2><p>在JSONPath表达式中，<code>$</code>表示根元素，<code>@</code>表示当前元素。JSONPath是一种用于从JSON文档中提取数据的查询语言，类似于XPath用于XML。这个特定的JSONPath表达式用于筛选出符合特定条件的对象。让我们逐部分分析这个表达式：</p><h3 id="JSONPath表达式解析"><a href="#JSONPath表达式解析" class="headerlink" title="JSONPath表达式解析"></a>JSONPath表达式解析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$@.attributes[?(@.bff_duplicates &amp;&amp; @.bff_duplicates[0] &amp;&amp; @.bff_duplicates[0][2] &gt;= 1.0)]</span><br></pre></td></tr></table></figure><h4 id="分段解释"><a href="#分段解释" class="headerlink" title="分段解释"></a>分段解释</h4><ol><li><p><strong><code>$</code></strong>:</p><ul><li>根元素。表示整个JSON文档的起点。</li></ul></li><li><p><strong><code>@</code></strong>:</p><ul><li>当前元素。在过滤表达式中，<code>@</code>表示当前处理的对象。</li></ul></li><li><p><strong><code>.attributes</code></strong>:</p><ul><li>访问当前元素的<code>attributes</code>字段。假设每个对象都有一个<code>attributes</code>字段。</li></ul></li><li><p><strong><code>[?()]</code></strong>:</p><ul><li>过滤操作。里面的内容定义了过滤条件。</li></ul></li><li><p><strong><code>@.bff_duplicates</code></strong>:</p><ul><li>访问<code>attributes</code>对象的<code>bff_duplicates</code>字段。</li></ul></li><li><p><strong><code>@.bff_duplicates[0]</code></strong>:</p><ul><li>访问<code>bff_duplicates</code>数组的第一个元素。</li></ul></li><li><p><strong><code>@.bff_duplicates[0][2] &gt;= 1.0</code></strong>:</p><ul><li>检查<code>bff_duplicates</code>数组第一个元素的第三个值是否大于或等于1.0。</li></ul></li></ol><h4 id="综合解释"><a href="#综合解释" class="headerlink" title="综合解释"></a>综合解释</h4><p>这个JSONPath表达式的整体意思是：</p><ul><li>选择<code>attributes</code>字段中的对象。</li><li>检查<code>bff_duplicates</code>字段是否存在且不为空。</li><li>检查<code>bff_duplicates</code>数组中的第一个元素是否存在且不为空。</li><li>检查<code>bff_duplicates</code>数组第一个元素的第三个值是否大于或等于1.0。</li></ul><p>如果所有这些条件都满足，则这个<code>attributes</code>对象被选中。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>假设你有如下JSON文档：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;attributes&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bff_duplicates&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">1.2</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在这个示例中：</p><ul><li><code>bff_duplicates</code>存在。</li><li><code>bff_duplicates</code>数组的第一个元素存在。</li><li>第一个元素的第三个值是<code>1.2</code>，满足大于或等于<code>1.0</code>的条件。</li></ul><p>因此，这个对象会被选中。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;attributes&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bff_duplicates&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">1.2</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>如果<code>bff_duplicates</code>数组为空或者其第一个元素的第三个值小于<code>1.0</code>，则这个对象不会被选中。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个JSONPath表达式用于在JSON文档中筛选出包含特定条件的<code>attributes</code>对象，特别是那些<code>bff_duplicates</code>数组的第一个元素的第三个值大于或等于<code>1.0</code>的对象。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mix command operates on one or more stream; each can correspond to a different data source</span></span><br><span class="line"><span class="comment"># and can have its own set of filters and transformations</span></span><br><span class="line"><span class="attr">streams:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">getting-started</span> <span class="comment"># name of the stream; this will be used as a prefix for the output files</span></span><br><span class="line">      <span class="attr">documents:</span> <span class="comment"># the documents to mix; note how we use a glob pattern to match all documents</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents/*.gz</span></span><br><span class="line">      <span class="comment"># this is the directory where the output will be written</span></span><br><span class="line">      <span class="comment"># note how the toolkit will try to create files of size ~1GB</span></span><br><span class="line">      <span class="attr">output:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents</span> <span class="comment">#输出路径，指定了输出文件的位置。</span></span><br><span class="line">        <span class="attr">max_size_in_bytes:</span> <span class="string">1_000_000_000</span> <span class="comment">#单个输出文件的最大大小，单位为字节。这里设置为 1,000,000,000 字节（即 1 GB）</span></span><br><span class="line">      <span class="attr">attributes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">ft_lang_id_en_doc_v2</span> <span class="comment"># load the attributes from the taggers</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">wanjuan</span> <span class="comment"># load the attributes from the deduper</span></span><br><span class="line">      <span class="attr">filter:</span></span><br><span class="line">        <span class="attr">include:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;$.attributes[?(@.ft_lang_id_en_doc_v2__ft_lang_id_en_doc_v2__not_en[0][2] &lt; 0.99)]&quot;</span></span><br><span class="line">          <span class="comment">#  JSONPath 表达式, 从 JSON 数据结构中筛选出具有 paloma_documents_bff_duplicates 属性的 attributes 数组元素，并且该属性的第一个元素的第三个值（索引为 2）大于或等于 1.0。</span></span><br><span class="line">        <span class="attr">exclude:</span></span><br><span class="line">          <span class="comment"># - &quot;$.attributes[?(@.exp__whitespace_tokenizer_with_paragraphs_v1__document[0][2] &lt; 50)]&quot;</span></span><br><span class="line">          <span class="comment"># - &quot;$.attributes[?(@.exp__ft_lang_id_en_paragraph_with_doc_score_v2__doc_en[0][2] &lt;= 0.5)]&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;$@.attributes[?(@.bff_duplicates &amp;&amp; @.bff_duplicates[0] &amp;&amp; @.bff_duplicates[0][2] &gt;= 1.0)]&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># span_replacement:</span></span><br><span class="line">      <span class="comment">#   - span: &quot;$.attributes.exp__cld2_en_paragraph_with_doc_score_v2__not_en&quot;</span></span><br><span class="line">      <span class="comment">#     min_score: 0.1 #这是一个分数阈值，只有当 span 的分数大于等于这个值时，才会进行替换。</span></span><br><span class="line">      <span class="comment">#     replacement: &#x27;&#x27; #替换成空字符串</span></span><br><span class="line"></span><br><span class="line"><span class="attr">processes:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">destination:</span> <span class="string">/mnt/LLM-pretrain-dataset/luchenhao/C4/tokens</span></span><br><span class="line"><span class="attr">documents:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">/mnt/LLM-pretrain-dataset/luchenhao/C4/*.json.gz</span></span><br><span class="line"><span class="attr">processes:</span> <span class="number">188</span></span><br><span class="line"><span class="attr">seed:</span> <span class="number">3920</span></span><br><span class="line"><span class="attr">max_size:</span> <span class="string">21_474_836_480</span></span><br><span class="line"><span class="attr">dtype:</span> <span class="string">uint32</span></span><br><span class="line"></span><br><span class="line"><span class="attr">tokenizer:</span></span><br><span class="line">  <span class="attr">name_or_path:</span> <span class="string">Qwen/Qwen2-7B-Instruct</span></span><br><span class="line">  <span class="attr">bos_token_id:</span> <span class="number">151644</span></span><br><span class="line">  <span class="attr">eos_token_id:</span> <span class="number">151645</span></span><br><span class="line">  <span class="attr">pad_token_id:</span> <span class="number">151643</span></span><br><span class="line">  <span class="attr">segment_before_tokenization:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup dolma -c /tokenizer.yaml tokens &gt; /mnt/LLM-pretrain-dataset/luchenhao/logfile.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure><h2 id="分词器解码校验"><a href="#分词器解码校验" class="headerlink" title="分词器解码校验"></a>分词器解码校验</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">local_tokenizer_path = <span class="string">&quot;/llm-practice/OLMo-0.3.0/tokenizers/tokenizer.json&quot;</span></span><br><span class="line"><span class="comment"># data = np.load(&quot;/mnt/LLM-pretrain-dataset/WuDaoCorpus2.0/WuDaoCorpus2.0_base_200G/WuDaoCorpus2.0_base_200G/documents/part-0-00000.npy&quot;, allow_pickle=True)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_tokenize</span>():</span><br><span class="line">    <span class="comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;/llm-practice/OLMo-0.3.0/tokenizers&quot;)</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Qwen/Qwen2-7B-Instruct&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;/mnt/LLM-pretrain-dataset/luchenhao/C4/tokens/part-001-00000.npy&quot;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = np.fromfile(f, dtype=np.uint32)</span><br><span class="line">        <span class="comment"># print(data)</span></span><br><span class="line">        <span class="built_in">print</span>(data.size)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;The file contains <span class="subst">&#123;data.size / <span class="number">1e9</span>:<span class="number">.2</span>f&#125;</span> billion tokens.&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(tokenizer.decode(data[:<span class="number">10000</span>]))</span><br><span class="line">check_tokenize()</span><br></pre></td></tr></table></figure><h2 id="tar文件解压"><a href="#tar文件解压" class="headerlink" title="tar文件解压"></a>tar文件解压</h2><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xvzf /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/Exam-cn/part-<span class="number">003756</span>-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/Exam-cn/part-<span class="number">003756</span>-a894b46e.jsonl.tar.gz</span><br></pre></td></tr></table></figure><h2 id="tar文件切分"><a href="#tar文件切分" class="headerlink" title="tar文件切分"></a>tar文件切分</h2><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ split -l <span class="number">500000</span> --additional-suffix=.jsonl /mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-<span class="number">000036</span>-a894b46e.jsonl.tar.gz/OpenDataLab___WanJuan1_dot_0/raw/nlp/CN/WebText-cn/part-<span class="number">000036</span>-a894b46e.jsonl /mnt/LLM-pretrain-dataset/luchenhao/WanJuan/documents/WebText1_cn_split_</span><br></pre></td></tr></table></figure><h2 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_lines_in_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> f)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_lines_from_file</span>(<span class="params">input_path, output_path, fraction=<span class="number">0.1</span>, seed=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        random.seed(seed)</span><br><span class="line"></span><br><span class="line">    total_lines = count_lines_in_file(input_path)</span><br><span class="line">    sample_size = <span class="built_in">int</span>(total_lines * fraction)</span><br><span class="line">    </span><br><span class="line">    sampled_lines = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f_in:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f_in, total=total_lines, desc=<span class="string">&quot;Reading lines&quot;</span>):</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; fraction:</span><br><span class="line">                sampled_lines.append(line)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(sampled_lines) &gt;= sample_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(output_path, <span class="string">&#x27;wt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f_out:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(sampled_lines, total=sample_size, desc=<span class="string">&quot;Writing lines&quot;</span>):</span><br><span class="line">            f_out.write(line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入和输出文件路径</span></span><br><span class="line">input_file = <span class="string">&#x27;/1GB_ad.jsonl&#x27;</span></span><br><span class="line">output_file = <span class="string">&#x27;/mnt/LLM-pretrain-dataset/WanJuan1_dot_0/raw/nlp/CN/ChinaNews-cn/1GB_ad.jsonl&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按比例采样并保存为新的 .jsonl.gz 文件</span></span><br><span class="line">sample_lines_from_file(input_file, output_file, fraction=<span class="number">0.05</span>, seed=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h2 id="token量计算"><a href="#token量计算" class="headerlink" title="token量计算"></a>token量计算</h2><p>一个 <code>.npy</code> 文件的大小与其包含的 token 数量之间的关系取决于每个 token 所占的存储空间。为了估算 <code>568.00 MB</code> 的 <code>.npy</code> 文件中包含 <code>0.15B</code>（即 <code>150,000,000</code>）个 tokens，首先需要了解每个 token 的字节大小。</p><p>假设 <code>.npy</code> 文件中存储的是一个整数数组，每个 token 对应一个整数，通常使用 <code>int32</code> 或 <code>int64</code> 类型来存储。</p><h3 id="估算过程"><a href="#估算过程" class="headerlink" title="估算过程"></a>估算过程</h3><ol><li><p><strong>计算每个 token 的字节大小</strong>：</p><ul><li><code>int32</code> 类型：每个整数占用 4 字节</li><li><code>int64</code> 类型：每个整数占用 8 字节</li></ul></li><li><p><strong>计算总 token 数量</strong>：</p><ul><li>假设 <code>.npy</code> 文件中每个 token 占用 <code>int32</code> 类型（4 字节）</li><li>文件大小为 <code>568.00 MB</code>，即 <code>568 * 1024 * 1024 = 595591168</code> 字节</li></ul><p>计算 token 数量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_tokens = total_bytes / bytes_per_token</span><br></pre></td></tr></table></figure></li><li><p><strong>实际计算</strong>：</p><ul><li>使用 <code>int32</code> 类型时，每个 token 占用 4 字节：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_tokens = <span class="number">595591168</span> / <span class="number">4</span></span><br></pre></td></tr></table></figure></li><li>使用 <code>int64</code> 类型时，每个 token 占用 8 字节：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_tokens = <span class="number">595591168</span> / <span class="number">8</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h3 id="计算示例"><a href="#计算示例" class="headerlink" title="计算示例"></a>计算示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">total_bytes = <span class="number">568</span> * <span class="number">1024</span> * <span class="number">1024</span>  <span class="comment"># 568 MB in bytes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For int32</span></span><br><span class="line">bytes_per_token_int32 = <span class="number">4</span></span><br><span class="line">total_tokens_int32 = total_bytes / bytes_per_token_int32</span><br><span class="line"></span><br><span class="line"><span class="comment"># For int64</span></span><br><span class="line">bytes_per_token_int64 = <span class="number">8</span></span><br><span class="line">total_tokens_int64 = total_bytes / bytes_per_token_int64</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total tokens (int32): <span class="subst">&#123;total_tokens_int32:<span class="number">.2</span>e&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total tokens (int64): <span class="subst">&#123;total_tokens_int64:<span class="number">.2</span>e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>假设使用 <code>int32</code> 类型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">total_bytes = <span class="number">568</span> * <span class="number">1024</span> * <span class="number">1024</span>  <span class="comment"># 568 MB in bytes</span></span><br><span class="line">bytes_per_token_int32 = <span class="number">4</span></span><br><span class="line">total_tokens_int32 = total_bytes / bytes_per_token_int32</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total tokens (int32): <span class="subst">&#123;total_tokens_int32:<span class="number">.2</span>e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total tokens (int32): <span class="number">1.49e+08</span></span><br></pre></td></tr></table></figure></p><p>即大约 <code>149,000,000</code> 个 tokens，与 <code>0.15B</code> tokens 非常接近。</p><p>假设使用 <code>int64</code> 类型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">total_bytes = <span class="number">568</span> * <span class="number">1024</span> * <span class="number">1024</span>  <span class="comment"># 568 MB in bytes</span></span><br><span class="line">bytes_per_token_int64 = <span class="number">8</span></span><br><span class="line">total_tokens_int64 = total_bytes / bytes_per_token_int64</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total tokens (int64): <span class="subst">&#123;total_tokens_int64:<span class="number">.2</span>e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total tokens (int64): <span class="number">7.45e+07</span></span><br></pre></td></tr></table></figure></p><p>即大约 <code>74,500,000</code> 个 tokens，少于 <code>0.15B</code> tokens。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>对于一个 <code>568.00 MB</code> 的 <code>.npy</code> 文件，如果每个 token 占用 <code>int32</code>（4 字节），那么大约包含 <code>0.15B</code>（150,000,000）个 tokens。这与估算的结果一致。因此，可以合理地推测 <code>.npy</code> 文件中每个 token 占用 <code>4</code> 字节。</p><h2 id="token量和npy文件大小的转换"><a href="#token量和npy文件大小的转换" class="headerlink" title="token量和npy文件大小的转换"></a>token量和npy文件大小的转换</h2><p>要计算存储 100B 个 tokens 需要多大的 <code>.npy</code> 文件，我们需要了解每个 token 的字节大小。通常，tokens 是以整数形式存储的，可以是 <code>int32</code> 或 <code>int64</code> 类型。我们将分别计算使用 <code>int32</code> 和 <code>int64</code> 类型存储 tokens 所需的文件大小。</p><h3 id="使用-int32-类型存储-tokens"><a href="#使用-int32-类型存储-tokens" class="headerlink" title="使用 int32 类型存储 tokens"></a>使用 <code>int32</code> 类型存储 tokens</h3><p>每个 <code>int32</code> 整数占用 4 字节。</p><h4 id="计算公式："><a href="#计算公式：" class="headerlink" title="计算公式："></a>计算公式：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_size_int32 = number_of_tokens * bytes_per_token</span><br></pre></td></tr></table></figure><h4 id="实际计算："><a href="#实际计算：" class="headerlink" title="实际计算："></a>实际计算：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">number_of_tokens = <span class="number">100</span> * <span class="number">10</span>**<span class="number">9</span>  <span class="comment"># 100 billion tokens</span></span><br><span class="line">bytes_per_token_int32 = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">file_size_int32 = number_of_tokens * bytes_per_token_int32  <span class="comment"># in bytes</span></span><br><span class="line">file_size_int32_gb = file_size_int32 / (<span class="number">1024</span> ** <span class="number">3</span>)  <span class="comment"># convert to GB</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;File size for 100B tokens (int32): <span class="subst">&#123;file_size_int32_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="使用-int64-类型存储-tokens"><a href="#使用-int64-类型存储-tokens" class="headerlink" title="使用 int64 类型存储 tokens"></a>使用 <code>int64</code> 类型存储 tokens</h3><p>每个 <code>int64</code> 整数占用 8 字节。</p><h4 id="计算公式：-1"><a href="#计算公式：-1" class="headerlink" title="计算公式："></a>计算公式：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_size_int64 = number_of_tokens * bytes_per_token</span><br></pre></td></tr></table></figure><h4 id="实际计算：-1"><a href="#实际计算：-1" class="headerlink" title="实际计算："></a>实际计算：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">number_of_tokens = <span class="number">100</span> * <span class="number">10</span>**<span class="number">9</span>  <span class="comment"># 100 billion tokens</span></span><br><span class="line">bytes_per_token_int64 = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">file_size_int64 = number_of_tokens * bytes_per_token_int64  <span class="comment"># in bytes</span></span><br><span class="line">file_size_int64_gb = file_size_int64 / (<span class="number">1024</span> ** <span class="number">3</span>)  <span class="comment"># convert to GB</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;File size for 100B tokens (int64): <span class="subst">&#123;file_size_int64_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="完整代码示例"><a href="#完整代码示例" class="headerlink" title="完整代码示例"></a>完整代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the number of tokens and bytes per token for int32 and int64</span></span><br><span class="line">number_of_tokens = <span class="number">100</span> * <span class="number">10</span>**<span class="number">9</span>  <span class="comment"># 100 billion tokens</span></span><br><span class="line">bytes_per_token_int32 = <span class="number">4</span></span><br><span class="line">bytes_per_token_int64 = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate file size for int32</span></span><br><span class="line">file_size_int32 = number_of_tokens * bytes_per_token_int32  <span class="comment"># in bytes</span></span><br><span class="line">file_size_int32_gb = file_size_int32 / (<span class="number">1024</span> ** <span class="number">3</span>)  <span class="comment"># convert to GB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate file size for int64</span></span><br><span class="line">file_size_int64 = number_of_tokens * bytes_per_token_int64  <span class="comment"># in bytes</span></span><br><span class="line">file_size_int64_gb = file_size_int64 / (<span class="number">1024</span> ** <span class="number">3</span>)  <span class="comment"># convert to GB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;File size for 100B tokens (int32): <span class="subst">&#123;file_size_int32_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;File size for 100B tokens (int64): <span class="subst">&#123;file_size_int64_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><p>运行上述代码后，你会得到以下输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">File size <span class="keyword">for</span> 100B tokens (int32): <span class="number">372.53</span> GB</span><br><span class="line">File size <span class="keyword">for</span> 100B tokens (int64): <span class="number">745.06</span> GB</span><br></pre></td></tr></table></figure><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><ul><li>如果使用 <code>int32</code> 类型（每个 token 占用 4 字节），存储 100B 个 tokens 需要大约 372.53 GB 的 <code>.npy</code> 文件。</li><li>如果使用 <code>int64</code> 类型（每个 token 占用 8 字节），存储 100B 个 tokens 需要大约 745.06 GB 的 <code>.npy</code> 文件。</li></ul><p>根据实际需求选择合适的数据类型和存储空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv /mnt/LLM-pretrain-dataset/WuDaoCorpus2<span class="number">.0</span>/WuDaoCorpus2<span class="number">.0</span>_base_200G/WuDaoCorpus2<span class="number">.0</span>_base_200G/documents/*_dolma.jsonl.gz /mnt/LLM-pretrain-dataset/luchenhao/WuDao/documents</span><br><span class="line"></span><br><span class="line">ls /mnt/LLM-pretrain-dataset/luchenhao/CC | shuf -n <span class="number">60</span> | xargs -I &#123;&#125; mv /mnt/LLM-pretrain-dataset/luchenhao/CC/&#123;&#125; /mnt/LLM-pretrain-dataset/luchenhao/CC/sample/</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 /OLMo/scripts/train.py /OLMo/configs/official/OLMo-1B.yaml --save-folder=olmo-run-test-v4 --save_overwrite</span><br></pre></td></tr></table></figure><p>在分布式训练中，有多种并行策略来提高训练效率和处理更大规模的模型。主要的策略包括数据并行 (DP)、模型并行 (MP)、流水线并行 (PP) 和完全分片数据并行 (FSDP)。以下是这些策略的详细解释及其关系：</p><h3 id="数据并行-Data-Parallel-DP"><a href="#数据并行-Data-Parallel-DP" class="headerlink" title="数据并行 (Data Parallel, DP)"></a>数据并行 (Data Parallel, DP)</h3><p><strong>概念</strong>：</p><ul><li>在数据并行中，每个设备（如 GPU）都拥有整个模型的一个副本。每个设备处理不同的 mini-batch 数据。</li><li>在每个设备上独立计算梯度，然后聚合所有设备上的梯度，更新所有模型副本的参数。</li></ul><p><strong>优点</strong>：</p><ul><li>实现简单，扩展性好。</li></ul><p><strong>缺点</strong>：</p><ul><li>每个设备需要存储整个模型，因此对于非常大的模型，单个设备的内存可能不足。</li></ul><p><strong>示例</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(rank)</span><br><span class="line">ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure></p><h3 id="模型并行-Model-Parallel-MP"><a href="#模型并行-Model-Parallel-MP" class="headerlink" title="模型并行 (Model Parallel, MP)"></a>模型并行 (Model Parallel, MP)</h3><p><strong>概念</strong>：</p><ul><li>在模型并行中，模型被分割成多个部分，每个部分在不同的设备上运行。</li><li>适用于单个设备内存不足以容纳整个模型的情况。</li></ul><p><strong>优点</strong>：</p><ul><li>可以处理非常大的模型。</li></ul><p><strong>缺点</strong>：</p><ul><li>通信开销大，编程复杂。</li></ul><p><strong>示例</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ModelPart1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 一部分模型在设备0上</span></span><br><span class="line">        <span class="keyword">return</span> x.to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelPart2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 另一部分模型在设备1上</span></span><br><span class="line">        <span class="keyword">return</span> x.to(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model_part1 = ModelPart1().to(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">model_part2 = ModelPart2().to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h3 id="流水线并行-Pipeline-Parallel-PP"><a href="#流水线并行-Pipeline-Parallel-PP" class="headerlink" title="流水线并行 (Pipeline Parallel, PP)"></a>流水线并行 (Pipeline Parallel, PP)</h3><p><strong>概念</strong>：</p><ul><li>将模型分成多个阶段，每个阶段在不同的设备上运行，类似于流水线作业。</li><li>每个阶段处理一个 mini-batch 的不同部分。</li></ul><p><strong>优点</strong>：</p><ul><li>可以处理非常大的模型，且相对高效。</li></ul><p><strong>缺点</strong>：</p><ul><li>需要复杂的同步机制，编程复杂。</li></ul><p><strong>示例</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.distributed.pipeline.sync <span class="keyword">import</span> Pipe</span><br><span class="line"></span><br><span class="line">model = nn.Sequential(part1, part2)</span><br><span class="line">model = Pipe(model, chunks=<span class="number">8</span>)</span><br></pre></td></tr></table></figure></p><h3 id="完全分片数据并行-Fully-Sharded-Data-Parallel-FSDP"><a href="#完全分片数据并行-Fully-Sharded-Data-Parallel-FSDP" class="headerlink" title="完全分片数据并行 (Fully Sharded Data Parallel, FSDP)"></a>完全分片数据并行 (Fully Sharded Data Parallel, FSDP)</h3><p><strong>概念</strong>：</p><ul><li>FSDP 是一种结合了数据并行和模型并行的策略，将模型参数和优化器状态完全分片到多个设备上。</li><li>训练时，每个设备只保存一部分参数和优化器状态，前向和后向传播时才聚合这些参数。</li></ul><p><strong>优点</strong>：</p><ul><li>高效利用内存，可以处理更大的模型。</li><li>自动管理参数和优化器状态的分片和聚合。</li></ul><p><strong>缺点</strong>：</p><ul><li>实现复杂，需要合适的库支持（如 PyTorch 的 <code>torch.distributed.fsdp</code>）。</li></ul><p><strong>示例</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributed.fsdp <span class="keyword">as</span> fsdp</span><br><span class="line"></span><br><span class="line">model = fsdp.FullyShardedDataParallel(model)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure></p><h3 id="总结及关系"><a href="#总结及关系" class="headerlink" title="总结及关系"></a>总结及关系</h3><ul><li><strong>DP</strong>：适用于可以在单个设备上存储整个模型，但需要处理大量数据的情况。</li><li><strong>MP</strong>：适用于单个设备无法容纳整个模型的情况，将模型切分到多个设备上。</li><li><strong>PP</strong>：类似于模型并行，但将模型切分为多个阶段，每个阶段处理一个 mini-batch 的不同部分。</li><li><strong>FSDP</strong>：结合了数据并行和模型并行的优点，能够高效地利用内存和计算资源来处理超大规模的模型。</li></ul><p>这些并行策略可以根据具体的需求和硬件资源进行选择和组合使用，以提高训练效率和模型规模。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">@dataclass</span><br><span class="line">class TrainConfig(BaseConfig):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    OLMo training configuration.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    run_name: Optional[str] = None</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    The name of the run.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    seed: int = 6198</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Used to seed all initial RNG states.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    epoch: Optional[int] = None</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Increment this when starting a new epoch.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def autocast_precision(self) -&gt; torch.dtype:</span><br><span class="line">        if self.precision == &quot;amp_bf16&quot;:</span><br><span class="line">            return torch.bfloat16</span><br><span class="line">        elif self.precision == &quot;amp_fp16&quot;:</span><br><span class="line">            return torch.float16</span><br><span class="line">        elif self.precision == &quot;fp32&quot;:</span><br><span class="line">            return torch.float32</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(f&quot;Unexpected precision type &#x27;&#123;self.precision&#125;&#x27;&quot;)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def fsdp_precision(self) -&gt; MixedPrecision:</span><br><span class="line">        if self.fsdp is not None:</span><br><span class="line">            if self.fsdp.precision == FSDPPrecision.pure:</span><br><span class="line">                return MixedPrecision(</span><br><span class="line">                    param_dtype=self.autocast_precision,</span><br><span class="line">                    reduce_dtype=self.autocast_precision,</span><br><span class="line">                    buffer_dtype=self.autocast_precision,</span><br><span class="line">                )</span><br><span class="line">            elif self.fsdp.precision == FSDPPrecision.mixed:</span><br><span class="line">                return MixedPrecision(</span><br><span class="line">                    param_dtype=self.autocast_precision,</span><br><span class="line">                    reduce_dtype=torch.float32,</span><br><span class="line">                    buffer_dtype=self.autocast_precision,</span><br><span class="line">                )</span><br><span class="line">            else:</span><br><span class="line">                raise NotImplementedError(f&quot;&#123;self.fsdp.precision&#125;&quot;)</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(&quot;self.fsdp is None!&quot;)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def update_legacy_settings(cls, config: D) -&gt; D:</span><br><span class="line">        new_config = config.copy()</span><br><span class="line">        if om.is_dict(new_config):</span><br><span class="line">            assert isinstance(new_config, DictConfig)</span><br><span class="line"></span><br><span class="line">            if hasattr(new_config, &quot;activation_checkpointing&quot;):</span><br><span class="line">                if new_config.activation_checkpointing is False:</span><br><span class="line">                    new_config.activation_checkpointing = None</span><br><span class="line">                if new_config.activation_checkpointing is True:</span><br><span class="line">                    new_config.activation_checkpointing = ActivationCheckpointingStrategy.whole_layer</span><br><span class="line"></span><br><span class="line">            if hasattr(new_config, &quot;optimizer&quot;):</span><br><span class="line">                new_config.optimizer = OptimizerConfig.update_legacy_settings(new_config.optimizer)</span><br><span class="line"></span><br><span class="line">        return new_config</span><br></pre></td></tr></table></figure><h2 id="SFT"><a href="#SFT" class="headerlink" title="SFT"></a>SFT</h2><p>物理机可以用deepspeed，容器很难用deepspeed</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TMPDIR=/home/luchenhao/tmp pip install -e &quot;.[all]&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"> 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 26500/26655 [4:52:05&lt;01:18,  1.97it/s[INFO|trainer.py:3478] 2024-07-16 11:49:24,113 &gt;&gt; Saving model checkpoint to /data1/luchenhao/full/sft/checkpoint-26500                                                                                                                                                                        </span><br><span class="line">[INFO|configuration_utils.py:472] 2024-07-16 11:49:24,114 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/checkpoint-26500/config.json</span><br><span class="line">[INFO|configuration_utils.py:769] 2024-07-16 11:49:24,115 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/checkpoint-26500/generation_config.json</span><br><span class="line">[INFO|modeling_utils.py:2698] 2024-07-16 11:49:32,468 &gt;&gt; The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /data1/luchenhao/full/sft/checkpoint-26500/model.safetensors.index.json.</span><br><span class="line">[INFO|tokenization_utils_base.py:2574] 2024-07-16 11:49:32,469 &gt;&gt; tokenizer config file saved in /data1/luchenhao/full/sft/checkpoint-26500/tokenizer_config.json</span><br><span class="line">[INFO|tokenization_utils_base.py:2583] 2024-07-16 11:49:32,469 &gt;&gt; Special tokens file saved in /data1/luchenhao/full/sft/checkpoint-26500/special_tokens_map.json</span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1281, &#x27;grad_norm&#x27;: 5.426765441894531, &#x27;learning_rate&#x27;: 4.5072165447301864e-10, &#x27;epoch&#x27;: 2.98, &#x27;num_input_tokens_seen&#x27;: 123258640&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1119, &#x27;grad_norm&#x27;: 3.4162933826446533, &#x27;learning_rate&#x27;: 3.9069845541889196e-10, &#x27;epoch&#x27;: 2.98, &#x27;num_input_tokens_seen&#x27;: 123307016&#125;                                                                                                                                                 </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0351, &#x27;grad_norm&#x27;: 5.188170909881592, &#x27;learning_rate&#x27;: 3.349621975623496e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123351704&#125;                                                                                                                                                   </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0771, &#x27;grad_norm&#x27;: 5.914969444274902, &#x27;learning_rate&#x27;: 2.8351297649387154e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123400920&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0588, &#x27;grad_norm&#x27;: 5.686376094818115, &#x27;learning_rate&#x27;: 2.363508804514858e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123442360&#125;                                                                                                                                                   </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1131, &#x27;grad_norm&#x27;: 4.420042514801025, &#x27;learning_rate&#x27;: 1.9347599031965814e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123496496&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0396, &#x27;grad_norm&#x27;: 5.733197212219238, &#x27;learning_rate&#x27;: 1.5488837963123504e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123540568&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0212, &#x27;grad_norm&#x27;: 4.386143207550049, &#x27;learning_rate&#x27;: 1.205881145655008e-10, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123588848&#125;                                                                                                                                                   </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0803, &#x27;grad_norm&#x27;: 4.8516411781311035, &#x27;learning_rate&#x27;: 9.057525394873257e-11, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123633792&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1515, &#x27;grad_norm&#x27;: 4.2364044189453125, &#x27;learning_rate&#x27;: 6.484984925475557e-11, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123678416&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.0729, &#x27;grad_norm&#x27;: 5.19443416595459, &#x27;learning_rate&#x27;: 4.341194460355525e-11, &#x27;epoch&#x27;: 2.99, &#x27;num_input_tokens_seen&#x27;: 123728800&#125;                                                                                                                                                    </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1528, &#x27;grad_norm&#x27;: 5.180673122406006, &#x27;learning_rate&#x27;: 2.6261576762109943e-11, &#x27;epoch&#x27;: 3.0, &#x27;num_input_tokens_seen&#x27;: 123777840&#125;                                                                                                                                                   </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1465, &#x27;grad_norm&#x27;: 4.9111151695251465, &#x27;learning_rate&#x27;: 1.3398775143835807e-11, &#x27;epoch&#x27;: 3.0, &#x27;num_input_tokens_seen&#x27;: 123826624&#125;                                                                                                                                                  </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1316, &#x27;grad_norm&#x27;: 4.621127128601074, &#x27;learning_rate&#x27;: 4.8235618094194525e-12, &#x27;epoch&#x27;: 3.0, &#x27;num_input_tokens_seen&#x27;: 123876224&#125;                                                                                                                                                   </span><br><span class="line">&#123;&#x27;loss&#x27;: 1.1654, &#x27;grad_norm&#x27;: 5.439855098724365, &#x27;learning_rate&#x27;: 5.359514654301734e-13, &#x27;epoch&#x27;: 3.0, &#x27;num_input_tokens_seen&#x27;: 123915040&#125;                                                                                                                                                    </span><br><span class="line">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26655/26655 [4:53:48&lt;00:00,  1.87it/s][INFO|trainer.py:3478] 2024-07-16 11:51:07,029 &gt;&gt; Saving model checkpoint to /data1/luchenhao/full/sft/checkpoint-26655</span><br><span class="line">[INFO|configuration_utils.py:472] 2024-07-16 11:51:07,030 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/checkpoint-26655/config.json</span><br><span class="line">[INFO|configuration_utils.py:769] 2024-07-16 11:51:07,031 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/checkpoint-26655/generation_config.json</span><br><span class="line">[INFO|modeling_utils.py:2698] 2024-07-16 11:51:15,234 &gt;&gt; The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /data1/luchenhao/full/sft/checkpoint-26655/model.safetensors.index.json.</span><br><span class="line">[INFO|tokenization_utils_base.py:2574] 2024-07-16 11:51:15,234 &gt;&gt; tokenizer config file saved in /data1/luchenhao/full/sft/checkpoint-26655/tokenizer_config.json</span><br><span class="line">[INFO|tokenization_utils_base.py:2583] 2024-07-16 11:51:15,235 &gt;&gt; Special tokens file saved in /data1/luchenhao/full/sft/checkpoint-26655/special_tokens_map.json</span><br><span class="line">[INFO|trainer.py:2383] 2024-07-16 11:51:28,566 &gt;&gt; </span><br><span class="line"></span><br><span class="line">Training completed. Do not forget to share your model on huggingface.co/models =)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;train_runtime&#x27;: 17650.2314, &#x27;train_samples_per_second&#x27;: 36.245, &#x27;train_steps_per_second&#x27;: 1.51, &#x27;train_loss&#x27;: 1.4209418270874596, &#x27;epoch&#x27;: 3.0, &#x27;num_input_tokens_seen&#x27;: 123942712&#125;                                                                                                         </span><br><span class="line">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26655/26655 [4:54:10&lt;00:00,  1.51it/s]</span><br><span class="line">[INFO|trainer.py:3478] 2024-07-16 11:51:28,572 &gt;&gt; Saving model checkpoint to /data1/luchenhao/full/sft</span><br><span class="line">[INFO|configuration_utils.py:472] 2024-07-16 11:51:28,573 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/config.json</span><br><span class="line">[INFO|configuration_utils.py:769] 2024-07-16 11:51:28,574 &gt;&gt; Configuration saved in /data1/luchenhao/full/sft/generation_config.json</span><br><span class="line">[INFO|modeling_utils.py:2698] 2024-07-16 11:51:36,766 &gt;&gt; The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /data1/luchenhao/full/sft/model.safetensors.index.json.</span><br><span class="line">[INFO|tokenization_utils_base.py:2574] 2024-07-16 11:51:36,767 &gt;&gt; tokenizer config file saved in /data1/luchenhao/full/sft/tokenizer_config.json</span><br><span class="line">[INFO|tokenization_utils_base.py:2583] 2024-07-16 11:51:36,767 &gt;&gt; Special tokens file saved in /data1/luchenhao/full/sft/special_tokens_map.json</span><br><span class="line">***** train metrics *****</span><br><span class="line">  epoch                    =      2.9998</span><br><span class="line">  num_input_tokens_seen    =   123942712</span><br><span class="line">  total_flos               = 743656272GF</span><br><span class="line">  train_loss               =      1.4209</span><br><span class="line">  train_runtime            =  4:54:10.23</span><br><span class="line">  train_samples_per_second =      36.245</span><br><span class="line">  train_steps_per_second   =        1.51</span><br><span class="line">Figure saved at: /data1/luchenhao/full/sft/training_loss.png</span><br><span class="line">Figure saved at: /data1/luchenhao/full/sft/training_eval_loss.png</span><br><span class="line">Figure saved at: /data1/luchenhao/full/sft/training_eval_accuracy.png</span><br><span class="line">[INFO|trainer.py:3788] 2024-07-16 11:51:37,252 &gt;&gt; </span><br><span class="line">***** Running Evaluation *****</span><br><span class="line">[INFO|trainer.py:3790] 2024-07-16 11:51:37,252 &gt;&gt;   Num examples = 11224</span><br><span class="line">[INFO|trainer.py:3793] 2024-07-16 11:51:37,252 &gt;&gt;   Batch size = 1</span><br><span class="line">100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1871/1871 [00:49&lt;00:00, 37.81it/s]</span><br><span class="line">***** eval metrics *****</span><br><span class="line">  epoch                   =     2.9998</span><br><span class="line">  eval_accuracy           =     0.6514</span><br><span class="line">  eval_loss               =     1.5619</span><br><span class="line">  eval_runtime            = 0:00:49.51</span><br><span class="line">  eval_samples_per_second =    226.692</span><br><span class="line">  eval_steps_per_second   =     37.789</span><br><span class="line">  num_input_tokens_seen   =  123942712</span><br><span class="line">[INFO|modelcard.py:449] 2024-07-16 11:52:26,802 &gt;&gt; Dropping the following result as it does not have all the necessary fields:</span><br><span class="line">&#123;&#x27;task&#x27;: &#123;&#x27;name&#x27;: &#x27;Causal Language Modeling&#x27;, &#x27;type&#x27;: &#x27;text-generation&#x27;&#125;, &#x27;metrics&#x27;: [&#123;&#x27;name&#x27;: &#x27;Accuracy&#x27;, &#x27;type&#x27;: &#x27;accuracy&#x27;, &#x27;value&#x27;: 0.6513998945712532&#125;]&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">conda create -n opencompass python=3.10 -y</span><br><span class="line">conda activate opencompass</span><br><span class="line"></span><br><span class="line">git clone https://github.com/open-compass/opencompass opencompass</span><br><span class="line">cd opencompass</span><br><span class="line"></span><br><span class="line">在opencompass/requirements/runtime.txt的最后⼀⾏添加ai2-olmo</span><br><span class="line"></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e .</span><br><span class="line"></span><br><span class="line">cd opencompass</span><br><span class="line">wget https://github.com/opencompass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip</span><br><span class="line"></span><br><span class="line">unzip OpenCompassData-core-20240207.zip</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#ModuleNotFoundError: No module named &#x27;human_eval&#x27;</span><br><span class="line">git clone git@github.com:open-compass/human-eval.git</span><br><span class="line">cd human-eval &amp;&amp; pip install -e .</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup llamafactory-cli train examples/train_full/llama3_full_sft_ds3_v2_step1.yaml &gt; /LLaMA-Factory/examples/train_full/script_output.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h2 id="SFT数据样本量"><a href="#SFT数据样本量" class="headerlink" title="SFT数据样本量"></a>SFT数据样本量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup llamafactory-cli train examples/train_full/llama3_full_sft_ds3_v4.yaml &gt; /home/luchenhao/LLaMA-Factory/examples/train_full/script_output.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ERROR: pip&#x27;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</span><br><span class="line">llava 1.2.2.post1 requires tokenizers==0.15.1, but you have tokenizers 0.19.1 which is incompatible.</span><br><span class="line">llava 1.2.2.post1 requires torch==2.1.2, but you have torch 2.3.1 which is incompatible.</span><br><span class="line">llava 1.2.2.post1 requires torchvision==0.16.2, but you have torchvision 0.18.1 which is incompatible.</span><br><span class="line">llava 1.2.2.post1 requires transformers==4.37.2, but you have transformers 4.43.2 which is incompatible.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">python3 -m llava.serve.controller --host 0.0.0.0 --port 20000</span><br><span class="line"></span><br><span class="line">python3 -m llava.serve.gradio_web_server --controller http://localhost:20000 --model-list-mode reload --share</span><br><span class="line"></span><br><span class="line">python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:20000 --port 40000 --worker http://localhost:40000 --model-path /mnt/LLM-pretrain-dataset/luchenhao/table_llava/checkpoints/llava-v1.5-1b-sft-with-table/checkpoint-7500 --load-4bit</span><br><span class="line"></span><br><span class="line">python3 -m llava.serve.model_worker --host 10.5.30.42 --controller http://localhost:8000 --port 8888 --worker http://localhost:8888 --model-path /mnt/LLM-pretrain-dataset/luchenhao/table_llava/checkpoints/llava-v1.5-1b-sft-with-table/checkpoint-7500 --load-4bit</span><br><span class="line"></span><br><span class="line">python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:20000 --port 40000 --worker http://localhost:40000 --model-path /mnt/LLM-pretrain-dataset/luchenhao/table_llava/checkpoints/llava-v1.5-1b-sft-with-table/checkpoint-7500 --load-4bit</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>|  | sub-groups of the agri-food industry   | eastern ontario | northern ontario        |\n||————————————————————|————————-|————————————-|———————-|\n|  | percent                                |\n|  | input and service supply               | 2.9             | 2.1                     | 2.9           | 1.3  |\n|  | food, beverage, and tobacco processing | 9.7             | 6.0                     | 3.0           | 3.3  |\n|  | food retail and wholesale              | 35.3            | 31.3                    | 39.1          | 37.3 |\n|  | food service                           | 52.1            | 60.6                    | 55.0          | 58.1 |\n”写一段代码讲这里面的“| xxx        |</p><p>“Table:\n\n| Year | Title | Role | Channel |\n| —- | —- | —- | —- |\n| 2015 | Kuch Toh Hai Tere Mere Darmiyaan | Sanjana Kapoor | Star Plus |\n| 2016 | Kuch Rang Pyar Ke Aise Bhi | Khushi | Sony TV |\n| 2016 | Gangaa | Aashi Jhaa | &amp;TV |\n| 2017 | Iss Pyaar Ko Kya Naam Doon 3 | Meghna Narayan Vashishth | Star Plus |\n| 2017–18 | Tu Aashiqui | Richa Dhanrajgir | Colors TV |\n| 2019 | Laal Ishq | Pernia | &amp;TV |\n| 2019 | Vikram Betaal Ki Rahasya Gatha | Rukmani/Kashi | &amp;TV |\n| 2019 | Shaadi Ke Siyape | Dua | &amp;TV |\n\nConduct table question answering task based on the given table about ‘Shagun Sharma’ with the table title ‘Television’.\nWhat TV shows was Shagun Sharma seen in 2019?”,</p><h2 id="镜像打包"><a href="#镜像打包" class="headerlink" title="镜像打包"></a>镜像打包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">docker create --name LLamaFactory --gpus all -v /mnt/nas_v2:/mnt -e SCRATCH_DIR=/mnt/nas_v2/LLM-pretrain-dataset/luchenhao/tmp -it nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker create --name LLamaFactory --shm-size=32g --gpus all -v /mnt_llm/luchenhao:/mnt_llm/luchenhao -e SCRATCH_DIR=/mnt_llm/luchenhao/tmp -it nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br><span class="line">docker start -i LLamaFactory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cp /etc/apt/sources.list /etc/apt/sources.list.1</span><br><span class="line">sed -i &#x27;s|http://archive.ubuntu.com/ubuntu/|http://mirrors.tuna.tsinghua.edu.cn/ubuntu/|g&#x27; /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install python3.10 python3-pip git</span><br><span class="line">pip install --upgrade pip </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd /mnt/nas_v2/LLM-pretrain-dataset/luchenhao/LLaMA-Factory</span><br><span class="line">pip install --no-cache-dir -e &quot;.[torch,metrics]&quot; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">docker exec -it -w /mnt/LLM-pretrain-dataset/luchenhao/LLaMA-Factory LLamaFactory llamafactory-cli train examples/train_full/llama3_full_sft_ds3_v6.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker exec -it -w /mnt/LLM-pretrain-dataset/luchenhao/LLaMA-Factory LLamaFactory nohup llamafactory-cli train examples/train_full/llama3_full_sft_ds3_v6.yaml &gt; examples/train_full/script_output9.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name LLamaFactory1 --gpus all --hostname=LLamaFactory --shm-size=32g -v /mnt/nas_v2:/mnt/nas_v2 -w /mnt/nas_v2/LLM-pretrain-dataset/luchenhao/LLaMA-Factory nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"> </span><br><span class="line">docker exec -it LLamaFactory llamafactory-cli train examples/train_full/llama3_full_sft_ds3_v6.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd --name LLaVA --gpus all --hostname=LLaVA --shm-size=64g -v /mnt/nas_v2:/mnt/nas_v2 nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br><span class="line">cp /etc/apt/sources.list /etc/apt/sources.list.1</span><br><span class="line">sed -i &#x27;s|http://archive.ubuntu.com/ubuntu/|http://mirrors.tuna.tsinghua.edu.cn/ubuntu/|g&#x27; /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install python3.10 python3-pip</span><br><span class="line">pip install --upgrade pip </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd /mnt/nas_v2/LLM-pretrain-dataset/luchenhao/LLaVA</span><br><span class="line">pip install --no-cache-dir -e . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install --no-cache-dir -e &quot;.[train]&quot; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install --no-cache-dir  flash-attn --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">docker exec -it llava sh scripts/v1_5/table_llava_scripts/pretrain_table_llava.sh</span><br></pre></td></tr></table></figure><h2 id="RSPrompter推理"><a href="#RSPrompter推理" class="headerlink" title="RSPrompter推理"></a>RSPrompter推理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br><span class="line">docker create --name RSPrompter_Lch --shm-size=64g --gpus all -v /mnt_llm/RSPrompter:/mnt_llm/RSPrompter -it nvidia/cuda:12.1.0-devel-ubuntu22.04</span><br><span class="line"></span><br><span class="line">docker start -i RSPrompter_Lch</span><br><span class="line"></span><br><span class="line">cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">sed -i &#x27;s|http://archive.ubuntu.com/ubuntu/|http://mirrors.tuna.tsinghua.edu.cn/ubuntu/|g&#x27; /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install locales</span><br><span class="line">locale-gen en_US.UTF-8</span><br><span class="line">apt-get install vim</span><br><span class="line">vi ~/.bashrc</span><br><span class="line"></span><br><span class="line">export LANG=en_US.UTF-8</span><br><span class="line">export LC_CTYPE=en_US.UTF-8</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">apt-get install python3.10 python3-pip git</span><br><span class="line">pip install --upgrade pip </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd /mnt_llm/hzh/LLaVA/</span><br><span class="line">pip install --no-cache-dir -e . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install --no-cache-dir -e &quot;.[train]&quot; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install --no-cache-dir  flash-attn --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h3 id="检查磁盘挂载情况"><a href="#检查磁盘挂载情况" class="headerlink" title="检查磁盘挂载情况"></a>检查磁盘挂载情况</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br><span class="line">du -h --max-depth=1 | sort -h</span><br><span class="line">du -sh * | sort -h</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="安装语言环境"><a href="#安装语言环境" class="headerlink" title="安装语言环境"></a>安装语言环境</h2><p>这个错误提示表明系统尝试设置 <code>LC_CTYPE</code> 为 <code>en_US.UTF-8</code>，但是系统没有找到或无法使用这个语言环境。可以通过以下步骤来修复这个问题：</p><h3 id="1-重新生成-en-US-UTF-8-语言环境"><a href="#1-重新生成-en-US-UTF-8-语言环境" class="headerlink" title="1. 重新生成 en_US.UTF-8 语言环境"></a>1. 重新生成 <code>en_US.UTF-8</code> 语言环境</h3><p>首先，确保你有权限使用 <code>locale-gen</code> 命令。如果你的系统缺少这个工具，可以通过安装 <code>locales</code> 包来修复。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install locales</span><br></pre></td></tr></table></figure><p>然后，生成 <code>en_US.UTF-8</code> 语言环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo locale-gen en_US.UTF-8</span><br></pre></td></tr></table></figure><p>最后，更新系统的语言环境设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-locale LANG=en_US.UTF-8</span><br></pre></td></tr></table></figure><h3 id="2-验证并应用更改"><a href="#2-验证并应用更改" class="headerlink" title="2. 验证并应用更改"></a>2. 验证并应用更改</h3><p>重启你的终端或执行以下命令使更改生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/default/locale</span><br></pre></td></tr></table></figure><h3 id="3-检查语言环境设置"><a href="#3-检查语言环境设置" class="headerlink" title="3. 检查语言环境设置"></a>3. 检查语言环境设置</h3><p>你可以使用以下命令检查当前的语言环境设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locale</span><br></pre></td></tr></table></figure><p>如果上述步骤无法解决问题，确保 <code>/etc/locale.gen</code> 文件中包含 <code>en_US.UTF-8 UTF-8</code> 这一行，并运行 <code>sudo locale-gen</code> 来重新生成语言环境。</p><h3 id="4-进一步调试"><a href="#4-进一步调试" class="headerlink" title="4. 进一步调试"></a>4. 进一步调试</h3><p>如果问题仍然存在，可以尝试手动设置语言环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LC_ALL=en_US.UTF-8</span><br><span class="line"><span class="built_in">export</span> LANG=en_US.UTF-8</span><br><span class="line"><span class="built_in">export</span> LANGUAGE=en_US.UTF-8</span><br></pre></td></tr></table></figure><p>这些命令在当前会话中临时生效，确保你的 <code>.bashrc</code> 或 <code>.zshrc</code> 文件中正确设置了这些环境变量。</p><h2 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apt-get install zsh </span><br><span class="line">chsh -s /bin/zsh</span><br><span class="line">apt-get install curl</span><br><span class="line">sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</span><br><span class="line"></span><br><span class="line">1. ys-conda样式要复制一份到themes里面</span><br><span class="line">2. 复制以前的zshrc</span><br><span class="line">3. git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions</span><br><span class="line">4. git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting </span><br><span class="line">（https://zhuanlan.zhihu.com/p/441676276）</span><br><span class="line">（https://juejin.cn/post/7279720525362315264）</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="安装代理"><a href="#安装代理" class="headerlink" title="安装代理"></a>安装代理</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://blog.myxuechao.com/post/36#%E5%9B%9B%E3%80%81%E9%85%8D%E7%BD%AEClash%E5%8F%AF%E8%A7%86%E5%8C%96%E9%9D%A2%E6%9D%BF</span><br><span class="line">https://ikuuu.pw/user/tutorial?os=linux&amp;client=clash##</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/06/11/Graph%20RAG%20%E4%B8%8E%20Text2Cypher%20%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
      <url>/2024/06/11/Graph%20RAG%20%E4%B8%8E%20Text2Cypher%20%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<p><strong>Graph RAG 与 Text2Cypher 的对比</strong></p><p>基于图谱的 LLM 的另一种有趣方法是 Text2Cypher，即自然语言生成图查询。这种方法不依赖于实体的子图检索，而是将任务/问题翻译成一个面向答案的特定图查询，和我们常说的 Text2SQL 本质是一样的。</p><p>Text2Cypher 和 Graph RAG 这两种方法主要在其检索机制上有所不同。Text2Cypher 根据知识图谱的 Schema 和给定的任务生成图形模式查询，而 （Sub）Graph RAG 获取相关的子图以提供上下文。两者都有其优点，大家可以通过这个 demo ，更直观理解他们的特点</p><p>无论你采取哪种方法，过程中的关键步骤是将你的知识库组织到你的图中。LLMGraphTransformer 就是这么一个强大的库，他可以将非结构化知识映射到你的图数据库变得更容易，下文字中，我们会用 Neo4j 作为用例来实现。</p><p>LangChain 有一个特别活跃的生态系统，包括库、预建组件和连接器，使得将 LLM 集成到你的应用中变得更容易。它的最新发布之一确实是朝着 GraphRAG 的方向：LLMGraphTransformer。</p><p><strong>CypherChain</strong></p><p>LangGraph的实现方式是把之前基于AgentExecutor的黑盒调用过程用一种新的形式来构建：状态图（StateGraph）。把基于LLM的任务（比如RAG、代码生成等）细节用Graph进行精确的定义（定义图的节点与边），最后基于这个图来编译生成应用；在任务运行过程中，维持一个中央状态对象(state)，会根据节点的跳转不断更新，状态包含的属性可自行定义。</p><p>从三元组到子图</p><ol><li>Do retrieval as Vector and Graph RAG</li><li>Answer synthesis based on <strong>both related chunks and SubGraphs</strong></li></ol><p><img src="/Users/luke/Library/Application Support/typora-user-images/截屏2024-06-06 10.26.04.png" alt="截屏2024-06-06 10.26.04"></p><p>大语言模型面试题：</p><ol><li><p>bert的架构：仅有编码器的transformer架构</p></li><li><p>说一个早期的预训练模型：ELMO</p></li><li><p>早期预训练模型的缺点：</p><ul><li>传统序列神经网络的长文本建模能力较弱 -&gt; self-attention建模长程序列关系</li><li>不容易并行训练 -&gt; transformer可以通过GPU或TPU进行加速训练，为研发大语言模型提供了可并行优化的神经网络架构</li></ul></li><li><p>GPT1的架构：仅有解码器的transformer架构</p></li><li><p>transformer编码器和解码器架构的优缺点：</p><ul><li>编码器架构更适合解决自然语言理解任务（完形填空）</li><li>解码器架构更适合解决自然语言生成任务（文本摘要）</li></ul></li><li><p>什么时候开始确立了“预训练=微调”的任务求解范式：</p><ul><li>从ELMo、BERT、GPT1开始</li></ul></li><li><p>解释一下“扩展法则”：通过增加模型参数规模或者数据规模，通常会带来下游任务的模型性能提升，这种现象通常被称为“扩展法则”</p></li><li><p>解释一下“涌现能力”：涌现能力是大模型具有但小模型没有的能力，例如gpt3可以通过“上下文学习”的方式利用少样本数据解决下游任务，而gpt2则不具备这一能力。且为了区别这一能力上的差异，才出现了“大语言模型”这一词（上下文学习、思维链都是大模型有但小模型没有的能力）</p></li><li><p>说几个大语言模型的能力特点：</p><ol><li>具有较为丰富的世界知识</li><li>具有较强的通用任务解决能力</li><li>具有较好的复杂任务推理能力</li><li>具有较强的人类指令遵循能力</li><li>具有较好的人类对齐能力</li><li>具有可拓展的工具使用能力</li></ol></li><li><p>大语言模型的关键技术有哪些？</p><ul><li>规模扩展：模型参数规模和高质量数据规模的扩展</li><li>数据工程：数据全面采集、数据清洗、数据配比</li><li>高效预训练: 分布式训练软件（deepspeed和Megatron-lm）、混合精度训练</li><li>能力激发：指令微调和提示工程</li><li>人类对齐：RLHF</li><li>工具使用</li></ul></li><li><p>大模型所需要的最小参数规模是多少？目前还没有明确的标准，通常指百亿以上的模型，也有部分工作认为经过大规模数据预训练的数十亿参数级别的模型也可以称之为大语言模型（如LLAMA-7B）</p></li><li><p>大语言模型的构建过程有几个阶段？三个阶段，分别是大规模预训练、指令微调和人类对其</p></li><li><p>现有大语言模型的技术路径是什么？“解码器架构+预测下一个词”</p></li><li><p>训练百亿模型需要多大规模的集群和多大规模的词元？百卡规模的算力集群（A100_80G）和2~3T的词元</p></li><li><p>预训练过程中的经验性技术有哪些？数据配比、学习率调整、早期发现模型的异常行为</p></li><li><p>解释一下“指令微调”和“人类对齐”：1.指令微调sft即使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好的掌握通过问答形式进行任务求解的能力（来源于机器学习中的模仿学习），指令微调很难起到知识注入作用，只能起到能力激发作用，数千条或数万条高质量指令微调数据可以达到不错的微调效果；2. 人类对齐rlhf通过使用强化学习加强模型的对齐能力，需要训练一个符合人类价值观的奖励模型，并且需要标注人员针对大语言模型生成的多条输出进行偏好排序，使用偏好排序的数据训练奖励模型，用于判断模型的输出质量</p></li><li><p>对比一下KM扩展法则和Chinchilla扩展法则：随着算力预算的增加，KM倾向于将更大的预算分配给模型规模的增加，CC则主张模型规模和数据规模等比例关系增加（数据规模大概是参数规模的五倍），但最近发现transformer架构有较好的数据扩展性，还没有实验能够验证特定参数规模的语言模型的饱和数据规模</p></li><li><p>大模型代表性的涌现能力有哪些？上下文学习（举例）、指令遵循（sft）、逐步推理（思维链）</p></li><li><p>GPT系列模型的几个关键要素？1. 训练能够准确预测下一个词的transformer（只包含解码器），2. 扩展语言模型的规模和预训练数据的规模</p></li><li><p>数据预处理包含哪些步骤：数据清洗（过滤html等标签）、质量过滤、去重、隐私过滤、词元化（分词）、数据调度（也叫数据混合）</p></li><li><p>典型的数据混合比例：LLaMA [34] 的预训练数据主要包括超过 80% 的网页数据、 来自 GitHub 和 StackExchange 的 6.5% 代码密集型数据、4.5% 的书籍数据,以及来自 arXiv 的 2.5% 科学数据,</p></li><li><p>数据混合策略：1. 增加数据源的多样性 2. 数据课程（多阶段多种数据训练方法）3.训练多个候选的1B的小语言模型来判断最优配比策略（最终,YuLan 的预训练阶段共使用了 1,680B 词元,其中包括 1,380B 英文数据,280B 中文数据,以及20B 的多语数据）</p></li><li><p>介绍下数据课程并举例说明：为了学习某些特定的技能,按照技能依赖顺序编排对应数据集的学习方法(例如,基本技能 → 目标技能)比直接在相关的特定语料库上学习效果更好。</p><ul><li><p>代码能力：举例1：CodeLLaMA [151],能够更为有效地执行代码任务。采用的数据为: 2T 通用词元 → 500B 代码密集型词元。 这里, 使用符号 “→” 来表示数据课程中的数据顺序,指的是大语言模型首先用 2T 网页数据词元进行训练,随后用 500B 代码数据词元训练。CodeLLaMA 还提供了一个面向 Python 语言的特定代码大模型, 即 CodeLLaMA-Python,采用了如下的数据训练课程:2T 通用词元 → 500B 代码相关的词元 → 100B Python 代码相关的词元。</p></li><li><p>长文本能力：举例2：CodeLLaMA 将 LLaMA-2 的上下文窗口从 4K 扩展到了 100K,所采用的数据课程为:2.5T 词元,4K 上下文窗口 → 20B 词元,16K 上下文窗口。通过使用这种训练序列长度由短到长的数据课程,能够使模型获得较好的长文本建模能力,同时可以节省长文本模型的训练时间。</p></li></ul></li><li><p>大语言模型和transformer的区别：与 BERT 等早期的预训练语言模型相比,大语言模型的特点是使用了更长的向量维度、 更深的层数, 进而包含了更大规模的模型参数, 并主要使用解码器架构, 对于 Transformer 本身的结构与配置改变并不大。</p></li><li><p>多头自注意力机制的用处：作为对比,循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态, 因此在处理较长序列的时候,常常会出现梯度爆炸或者梯度消失的问题。而在卷积神经网络中,只有位于同一个卷积核的窗口中的词元可以直接进行交互,通过堆叠层数来实现远距离词元间信息的交换。</p></li><li><p>多头自注意力机制的优点：自注意力机制能够直接建模序列中任意两个位置之间的关系,进而有效捕获长程依赖关系,具有更强的序列建模能力。另一个主要的优势是,自注意力的计算过程对于基于硬件的并行优化(如 GPU、TPU 等)非常友好, 因此能够支持大规模参数的高效优化。</p></li><li><p>Transformer 中的前馈神经网络由两个线性变换和一个非线性激活函数组成ReLU</p></li><li><p>在注意力和前馈网络后,模型使用层归一化和残差连接来加强模型的训练稳定度</p><ul><li><p>在注意力和前馈网络后,模型使用层归一化和残差连接来加强模型的训练稳定度。其中,残差连接(Residual Connection)将输入与该层的输出相加,实现了信息在不同层的跳跃传递,从而缓解梯度爆炸和消失的问题</p></li><li><p>而 LayerNorm 则对数据进行重新放缩,提升模型的训练稳定性</p></li></ul></li><li><p>批次归一化(Batch Normalization, BN)[161] 是一种广泛采用的归一化方法。然而,该方法难以处理可变长度的序列数据和小批次数据。</p></li><li><p>硬件优化的注意力机制有哪些？FlashAttention 通过矩阵分块计算以及减少内存读写次数的方式,提高注意力分数的计算效率;PagedAttention 则针对增量解码阶段,对于 KV 缓存进行分块存储,并优化了计算方式,增大了并行计算度,从而提高了计算效率。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/06/05/RAG%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2024/06/05/RAG%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="Rag面试题"><a href="#Rag面试题" class="headerlink" title="Rag面试题"></a>Rag面试题</h2><h6 id="1-LLMs-存在模型幻觉问题，请问如何处理？"><a href="#1-LLMs-存在模型幻觉问题，请问如何处理？" class="headerlink" title="1.LLMs 存在模型幻觉问题，请问如何处理？"></a>1.LLMs 存在模型幻觉问题，请问如何处理？</h6><p>检索+LLM。先用问题在领域数据库里检索到候选答案，再用LLM对答案进行加工。</p><h6 id="2-基于LLM-向量库的文档对话-思路是怎么样？"><a href="#2-基于LLM-向量库的文档对话-思路是怎么样？" class="headerlink" title="2.基于LLM+向量库的文档对话 思路是怎么样？"></a>2.基于LLM+向量库的文档对话 思路是怎么样？</h6><ol><li>加载文件</li><li>读取文本</li><li>文本分割</li><li>文本向量化</li><li>问句向量化</li><li>在文本向量中匹配出与问句向量最相似的top k个</li><li>匹配出的文本作为上下文和问题一起添加到 prompt 中</li><li>提交给 LLM 生成回答</li></ol><h6 id="3-基于LLM-向量库的文档对话-核心技术是什么？"><a href="#3-基于LLM-向量库的文档对话-核心技术是什么？" class="headerlink" title="3.基于LLM+向量库的文档对话 核心技术是什么？"></a>3.基于LLM+向量库的文档对话 核心技术是什么？</h6><p>基于LLM+向量库的文档对话 核心技术：embedding</p><p>思路：将用户知识库内容经过 embedding 存入向量知识库，然后用户每一次提问也会经过 embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 promt 提交给 LLM 回答</p><h6 id="4-基于LLM-向量库的文档对话-prompt-模板-如何构建？"><a href="#4-基于LLM-向量库的文档对话-prompt-模板-如何构建？" class="headerlink" title="4.基于LLM+向量库的文档对话 prompt 模板 如何构建？"></a>4.基于LLM+向量库的文档对话 prompt 模板 如何构建？</h6><p>已知信息：</p><p>{context}</p><p>根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。</p><p>问题是：{question}</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/11/Self-RAG/"/>
      <url>/2024/04/11/Self-RAG/</url>
      
        <content type="html"><![CDATA[<p>尽管大型语言模型（LLMs）具有非凡的能力，但由于其完全依赖于所封装的参数知识，因此经常会产生包含不准确事实的回复。检索增强生成（RAG）是一种通过检索相关知识来增强大型语言模型的特别方法，可以减少此类问题。但是，如果不加区分地检索并纳入固定数量的检索段落，而不管检索是否必要或段落是否相关，就会降低 LM 的通用性，或导致生成无益的响应。我们引入了一个名为 “自我反思检索-增强生成”（SELF-RAG）的新框架，通过检索和自我反思来提高 LM 的质量和事实性。 我们的框架训练一个单一的任意 LM，该 LM 可按需自适应性地检索段落，并使用特殊标记（称为反思标记）生成和反思检索到的段落及其自身的生成。反思标记的生成使 LM 在推理阶段具有可控性，使其能够根据不同的任务要求调整自己的行为。实验表明，SELF - RAG（7B 和 13B 参数）在各种任务中的表现明显优于最先进的 LLM 和检索增强模型。具体来说，SELF-RAG 在开放域质量保证、推理和事实验证任务上的表现优于 ChatGPT 和检索增强的 Llama2-chat，而且与这些模型相比，它在提高长篇小说的事实性和引用准确性方面也有显著提高1。</p><p>尽管最先进的LLMs模型随着模型规模和数据量的增加（Ouyang等人，2022），但仍持续面临事实性错误的问题（Mallen等人，2023；Min等人，2023）。检索增强生成（RAG）方法（图1左；Lewis等人，2020；Guu等人，2020）通过向LLMs的输入中添加相关检索段落，减少了知识密集型任务中的事实性错误（Ram等人，2023；Asai等人，2023a）。然而，这些方法可能会限制LLMs的通用性，或者引入不必要的或与主题无关的段落，导致生成质量低下（Shi等人，2023），因为它们不加区分地检索段落，而不考虑事实依据是否有益。此外，输出并不保证与检索到的相关段落一致（Gao等人，2023），因为模型没有被明确训练去利用和遵循提供的段落中的事实。</p><p>本工作提出了自我反思式检索增强生成（SELF-RAG）方法，旨在通过按需检索和自我反思，提高LLMs的生成质量，包括其事实准确性，同时不损害其通用性。我们以端到端的方式训练任意语言模型，使其学会在给定任务输入的情况下，通过生成任务输出和中间特殊标记（即反思标记）来反思自己的生成过程。反思标记被分类为检索标记和批评标记，分别表示需要检索和生成质量（图1右）。</p><p>具体来说，给定输入提示和先前的生成内容，SELF-RAG首先判断在继续生成过程中是否有必要通过检索段落进行增强。如果需要，它会输出一个检索标记以按需调用检索器模型（步骤1）。随后，SELF-RAG并行处理多个检索到的段落，评估其相关性，然后生成相应的任务输出（步骤2）。接下来，它生成批评标记以批评自己的输出并选择最佳的一个（步骤3），评判标准包括事实性和整体质量。这一过程与传统的RAG方法（图1左）不同，后者在检索和生成阶段之间没有明确的反馈和自我修正机制，始终以固定数量的文档进行检索，而不考虑实际是否需要检索（例如，下图示例无需事实知识），并且从未对生成质量进行二次审视。此外，SELF-RAG为每个段落提供引用，并对其输出是否得到所引段落支持进行自我评估，从而便于事实核查。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/11/C-RAG/"/>
      <url>/2024/04/11/C-RAG/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​    大型语言模型（LLM）不可避免地会产生幻觉，因为生成文本的准确性不能仅靠模型所包含的参数知识来保证。尽管检索增强生成（RAG）是对 LLM 的一种实用补充，但它在很大程度上依赖于检索文档的相关性，这让人担心如果检索出错，模型会如何表现。为此，我们提出了<strong>纠正</strong> <strong>检索</strong> <strong>增强</strong> <strong>生成</strong>（CRAG）来提高生成的鲁棒性。具体来说，我们设计了一个轻量级检索评估器，用于评估检索到的查询文档的整体质量，并返回一个置信度，在此基础上触发不同的知识检索操作。由于从静态和有限的语料库中检索只能返回次优文档，因此利用大规模网络搜索作为扩展，以增强检索结果。此外，还为检索到的文档设计了一种先分解再重组的算法，以选择性地关注关键信息并过滤掉其中的无关信息。CRAG 即插即用，可与各种基于 RAG 的方法无缝结合。在四个数据集上进行的实验表明，CRAG 可以显著提高基于 RAG 方法的性能。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​    大语言模型（LLMs）吸引了越来越多的关注，并在理解指令和生成流畅语言文本方面表现出令人印象深刻的能力（Brown 等，2020；欧阳等，2022；Touvron 等，2023a）。然而，由于 LLMs 与事实错误作斗争（Mallenet 等人，2023 年；闵等人，2023 年），并且无法仅凭其所包含的参数知识来确保生成文本的准确性（张等人，2023 年 b；穆尔盖等人，2023 年），因此他们不可避免地会产生幻觉（吉等人，2023 年）。</p><blockquote><p>图 1：这些例子表明，低质量的检索器容易引入大量无关信息，妨碍生成者获取准确的知识，并可能误导他们。</p></blockquote><p>​    先前的研究已经引入了检索技术，以纳入相关知识和增强生成，例如检索增强生成（RAG）（Lewis 等人，2020 年）。在这一框架中，通过预置从外部知识语料库检索到的相关文档来增强模型的输入（Guuet 等人，2020 年）。虽然 RAG 可作为 LLM 的实用补充，但其有效性取决于检索文档的相关性和准确性（Li 等人，2022 年；Tan 等人，2022 年）。在检索可能失败或返回不准确结果的情况下，生成对所检索知识的严重依赖引起了人们对模型行为和性能的极大关注（Shi 等人，2023 年）。如图 1 所示，低质量的检索器容易引入大量无关信息，阻碍模型获取准确知识，并可能误导模型，导致幻觉等问题（Zhang 等，2023b）。然而，大多数传统的 RAG 方法会不加区分地纳入检索到的文档，而不管这些文档是否相关（Rony 等，2022 年）。此外，当前的方法在检索和使用过程中大多将完整的文档作为参考知识。但是，这些检索到的文档中的相当一部分文本往往对生成并不重要，而这些文本本不应该同样被参考并参与到 RAG 中。</p><p>​    鉴于上述问题，本文特别研究了检索器返回不准确结果的情况。本文提出了一种名为 “<strong>修正</strong> <strong>检索-增强</strong> <strong>生成</strong>（CRAG）”的方法，用于自我修正检索器的结果，并提高增强生成对文档的利用率。 设计了一种轻量级检索评估器，用于评估查询所检索文档的整体质量。 它是 RAG 的重要组成部分，通过审查和评估检索文档的相关性和可靠性，为信息生成做出贡献。   在量化置信度的基础上，可以触发{正确、不正确、模糊}等不同的知识检索操作。对于后两种操作，大规模网络搜索<a href="#bookmark18">（Piktus</a> <a href="#bookmark18">等人，2021 年；</a><a href="#bookmark19">Komeili 等人，2022 年）作为</a>一种战略扩展被<a href="#bookmark19">整合进来</a>，因为从静态和有限的语料库中检索只能返回在范围和多样性方面次优的文档。 实施这种扩展是为了扩大检索信息的范围，利用网络的扩展性和动态性来补充和丰富最初获得的文档。此外，为了消除检索文档中对 RAG 无益的冗余内容，在整个检索和利用过程中，我们精心设计了一种分解—再分解算法。 该算法确保对检索到的信息进行细化，优化关键见解的提取，尽量减少非必要元素的包含，从而提高检索数据的利用率。</p><p>​    CRAG 即插即用，并<a href="#bookmark12">在 RAG（Lewis 等人，</a> <a href="#bookmark12">2020 年）和</a>Self-RAG<a href="#bookmark20">（Asai 等人，2023 年）</a>中进行了实验<a href="#bookmark12">实施</a>，<a href="#bookmark20">以证明其</a>对基于 RAG 的方法的适应性。在<a href="#bookmark8">PopQA（Mallen 等人，2023 年）、</a><a href="#bookmark22">Biog</a> <a href="#bookmark8">-raphy</a><a href="#bookmark9">（Min 等人，2023 年）、Pub Health（</a><a href="#bookmark21">Zhang</a>等人，2023 年<a href="#bookmark21">a</a>）和 Arc-Challenge （<a href="#bookmark22">Bhakthavatsalam 等</a>人，<a href="#bookmark22">2021</a> 年）这<a href="#bookmark8">四个数据集上</a>的结果表明，CRAG 可以显著提高标准 RAG 和最先进的 Self-RAG 的性能，证明了它在短表和长表生成任务中的通用性。为方便他人复制我们的结果，我们将在稍后公布所有源代码。</p><p>​    总之，我们在本文中的贡献有三个方面：1）本文研究了检索器返回不准确结果的情况，并就我们所知，首次尝试为 RAG 设计纠正策略，以提高其鲁棒性。2) 提出了一种名为 CRAG 的即插即用方法，以提高自动自我纠正能力和检索文档的有效利用率。 3) 实验结果广泛证明了 CRAG 对基于 RAG 的方法的适应性及其在长短格式生成任务中的通用性。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><ol><li><p><strong>语言学习者的幻觉</strong> 虽然语言学习者在理解指令和生成流畅的语言文本方面表现出了令人印象深刻的能力<a href="#bookmark23">（Bang等人，</a> <a href="#bookmark23">2023；</a> <a href="#bookmark24">Qin等</a>人，<a href="#bookmark24">2023；</a> <a href="#bookmark25">Zhong</a>等人，2023<a href="#bookmark25">），但</a>其中最严重的问题<a href="#bookmark25">之一</a>是幻觉。 许多研究发现<a href="#bookmark10">（Zhang 等，2023b；</a><a href="#bookmark26">Shuster</a>等，<a href="#bookmark26">2021</a>），被激活的过时信息或错误知识都会严重导致幻觉。大规模不规范的训练数据收集、高质量采样数据比例低、输入空间的数据分配不完善等诸多现实因素都可能影响 LLMs 并加剧问题。因此，在大多数实际应用中，缺乏准确和具体的知识显然会导致误导甚至不准确的生成，这将严重损害用户的体验。 <strong>检索增强生成</strong> <a href="#bookmark12">RAG（Lewis</a> <a href="#bookmark12">等人，</a> <a href="#bookmark12">2020；</a> <a href="#bookmark13">Guu 等人，</a> <a href="#bookmark13">2020）被认为是</a>解决上述问题的有效方法，它利用检索文档增强了生成式 LM 的输入问题。 它通常从特定的语料库（如维基百科）中提供额外的知识源，这大大提高了生成式 LM 在各种任务中的性能，尤其是在知识密集型任务中。 所提出的方法一般利用信息检索为生成式 LLM 提供包含相关知识的文档。早期的研究要么采用 稀疏或密集检索器，并将其置于预先训练好的语言模型的前端，专门用于生成响应。尽管如此，上述方法通常会忽略一个问题：<em>如果检索出错了怎么办？</em>因为引入检索的目的是确保生成式 LM 能够获得相关的准确知识。 如果检索到的文档不相关，检索系统甚至会加剧 LM 所犯的事实错误。</p></li><li><p><strong>先进的 RAG</strong> 近年来，在原始 RAG 的基础上发展出了许多先进的方法。 考虑到检索对于某些查询来说有时是不必要的，反之，在许多情况下，不进行检索的响应甚至会更加准确。 Self-RAG<a href="#bookmark20">（Asai et al</a>. <a href="#bookmark27">Yoran 等人（2023 年）设计了一个 NLI 模型</a>来识别无关上下文并提高鲁棒性。 SAIL<a href="#bookmark28">（Luo 等人，</a> <a href="#bookmark28">2023</a> 年）根据指令进行调整，在结构化之前插入检索到的文档。而 Toolformer<a href="#bookmark29">（Schick 等人，2023</a> 年）则针对调用维基百科等 API 进行了预先训练。此外，在某些长文本生成任务中，需要多次使用外部知识，因此应关注何时检索。<a href="#bookmark30">Jiang 等人</a> <a href="#bookmark30">（2023 年）</a>在长文本生成中<a href="#bookmark30">主动预测未来内容，并决定</a>何时检索和检索什么。</p></li><li><a href="#bookmark29">与最近的研究</a>（<a href="#bookmark29">Schick 等人，</a>2023 年<a href="#bookmark29">；</a> <a href="#bookmark28">Luo 等</a>人，<a href="#bookmark28">2023 年；</a> <a href="#bookmark20">Asai</a>等人，<a href="#bookmark20">2023</a> 年）<a href="#bookmark29">相比，与</a>我们的工作最相关的<a href="#bookmark29">研究</a>有一个主要区别。 这些方法的目标是利用检索作为增强生成的有用工具或检索是否必要，而本研究特别研究了检索器返回不准确结果的情况。据我们所知，本文首次尝试探索和设计 RAG 的纠正策略，以提高其生成的鲁棒性。</li></ol><h2 id="3-任务制定"><a href="#3-任务制定" class="headerlink" title="3 任务制定"></a>3 任务制定</h2><p>根据之前的工作（Lewis 等人，2020；<a href="#bookmark20">Asai</a>等人，2023），给定输入X和包含大量知识文档 C = {d1 ,…, dN } 的可访问语料库，系统有望生成输出 Y。整个框架通常分为检索器 R 和生成器 G。检索器 R 的目的是从语料库 C 中检索出与输入 X 相关的前 K 篇文档 D = {dr1 ,…,dr k }，生成器 G 负责根据输入 X 和检索结果 D 生成输出 Y。</p><p>这个框架可以表述为，</p><script type="math/tex; mode=display">P(Y \mid X)=P(D \mid X) P(Y, D \mid X) .</script><p>这表明，检索器和生成器是紧密耦合的，表现出较低的风险容忍度。任何不成功的检索都会导致不满意的响应，而不管生成器的能力有多强大。这正是本文的重点，即提高生成器的鲁棒性。</p><h2 id="4-CRAG"><a href="#4-CRAG" class="headerlink" title="4 CRAG"></a><strong>4 CRAG</strong></h2><p><strong>4.1 模型推理概述</strong></p><p>图<a href="#bookmark31">2</a>和算法 1 展示了 CRAG 的推理概述，它设计了纠正策略来提高生成的鲁棒性。给定输入查询和从任何检索器检索到的文档，构建一个轻量级检索评估器来估计检索文档与输入查询的相关性得分（第<a href="#bookmark32">4.2</a> 节）。相关性得分被量化为总共三个置信度，然后触发相应的操作： {正确、不正确、模糊}（第<a href="#bookmark33">4.3</a> 节）。如果触发了 “正确 “操作，检索到的文档将被重新细化为更精确的知识条带。  这一细化操作包括知识解定位、过滤和重新组合（第<a href="#bookmark34">4.4</a> 节）。如果触发 “不正确 “操作，检索到的文档将被丢弃。取而代之的是网络搜索，并将其视为更正的补充知识源（第<a href="#bookmark35">4.5</a> 节）。最后，在无法做出正确或错误的判断时，就会触发一个软性的、平衡的、将两者结合在一起的 “模糊 “操作。在优化检索结果后，可以采用任意生成模型。</p><p><strong>4.2 检索评价器</strong></p><p>在使用检索到的文档之前，人们自然会怀疑它们是否准确，这一点非常重要，因为通过这种方式可以识别出不相关或误导性的信息。不可否认，检索评价器的准确性对整个系统的性能起着关键作用，因为它影响着后续流程的结果。我们的目标是修正检索到的不相关文档。具体来说，我们<a href="#bookmark36">采用</a>T5-large<a href="#bookmark36">（Raffel</a> <a href="#bookmark36">等人，2020 年）对检索</a>评估器进行<a href="#bookmark36">初始化</a>和微调。 用于微调评价器的相关性信号可以从现有数据集中收集。有关微调步骤的更多详情，请参阅附录<a href="#bookmark37">B.2</a>。对于每个问题，一般会检索到 10 个文档。问题与每个单个文档作为输入进行串联，然后评价器分别预测每个问题-文档对的相关性得分。我们还尝试提示 ChatGPT 识别检索相关性以进行比较，但其表现不佳，详见第<a href="#bookmark38">5.5</a>节。根据这些计算出的相关性分数，我们会对检索是否与操作触发相关联做出最终判断。  与对 LLaMA-2 进行<a href="#bookmark20">指令</a>调整的 Self-RAG 的批评者模型<a href="#bookmark20">（Asai 等人，</a> <a href="#bookmark20">2023</a> 年）相比（7B），CRAG 中设计的评估器具有相当轻便的优势（0.77B）。</p><p><strong>4.3 动作触发</strong></p><p>为了修正不相关文档并根据需要完善目标文档，应该有区别地执行相应的操作。根据上述每个检索到的文档的置信度得分，我们设计了三种类型的操作，并根据设定的上下阈值进行触发。如果可信度得分高于上阈值</p><p>如果低于下阈值，则识别为不正确。  否则，执行模糊处理。每个检索到的文档都要单独进行处理，并最终进行整合。</p><p><strong>正确</strong> 在这里，当<em>至少有一份检索到</em> 的文档的置信度得分高于上阈值时，检索结果就被认为是正确的。如果是这样，说明检索结果中有相关文档。即使能找到相关文档，该文档中也不可避免地存在一些噪声知识条。   为了提取该文档中最关键的知识条，我们将进一步设计一种知识细化方法，该方法将在第<a href="#bookmark34">4.4</a>节中详细阐述。</p><p><strong>不正确</strong> 此外，当<em>所有</em> 检索到的文档的置信度得分都低于下限阈值时，检索就被认为是不正确的。这表明所有检索到的文档都被认为是不相关的，对生成没有帮助。   因此，我们需要寻找新的知识来源进行修正。 在这里，我们引入了网络搜索，从互联网上进行搜索，这在第<a href="#bookmark35">4.5</a>节中有详细阐述<a href="#bookmark35">。</a> 这一纠正措施有助于克服无法参考可靠知识的尴尬难题。</p><p><strong>模棱两可</strong> 除上述两种情况外，其余情况将被归入 “模棱两可 “的中间操作。由于检索评估者对自己的判断并不自信，因此正确和不正确两种类型的处理知识会相互结合，互为补充。 采用这种软性调节策略可大大有助于增强系统的稳健性和复原力，促进建立一个适应性更强的框架，以实现最佳性能。</p><p><strong>4.4 知识提炼</strong></p><p>对于检索到的相关文档，我们设计了一种先分解再组合的知识提炼方法，以进一步提取其中最关键的知识条。 首先，通过启发式规则将每个检索到的文档分割成细粒度的知识条，详情见附录<a href="#bookmark37">B.2</a>。然后，使用第<a href="#bookmark32">4.2</a>节中微调过的检索评价器来计算每个知识条的相关性得分。 根据这些分数，不相关的知识条带被过滤掉，而相关的知识条带则按顺序通过连接重新组合，即内部知识。</p><p><strong>4.5 网络搜索</strong></p><p>如果检索到的结果都是不相关的，那么寻求外部知识的补充就极为重要。</p><p>假定都是不相关的，那么寻求补充性外部知识就显得极为重要。  由于从静态和有限的语料库中检索只能返回在范围和多样性方面次优的文档，因此大规模网络搜索<a href="#bookmark18">（Piktus 等</a>人，<a href="#bookmark18">2021 年；</a> <a href="#bookmark19">Komeili</a>等人<a href="#bookmark19">，2022 年）作为</a>RAG 的<a href="#bookmark19">战略扩展被整合进来</a>。具体来说，输入内容会被 ChatGPT 改写成由关键词组成的查询，以模拟搜索引擎的日常使用。   改写提示见附录<a href="#bookmark39">A</a>。<a href="#bookmark40">2</a>此外，我们还利用 URL 链接来浏览网页、转录网页内容，并采用与第<a href="#bookmark34">4.4</a>节相同的知识提炼方法来获取相关的网络知识，即外部知识。</p><p><strong>5 实验</strong></p><p>我们通过实验广泛展示了 CRAG 对基于 RAG 的方法的适应性及其在短表和长表生成任务中的通用性。</p><p><strong>5.1 任务、数据集和指标</strong></p><p>CRAG 在四个数据集上进行了评估，包括<strong>PopQA</strong> <a href="#bookmark8">（Mallen 等</a>人，<a href="#bookmark8">2023</a> 年）（<em>短表</em>生成）、<strong>Biography</strong> <a href="#bookmark9">（Min 等</a>人，<a href="#bookmark9">2023</a> 年）（<em>长表</em>生成）<strong>、PubHealth</strong> （<a href="#bookmark21">Zhang 等人，2023 年a）（</a><em>真假</em> 问题）和<strong>Arc-Challenge</strong>  （<a href="#bookmark22">Bhaktha</a>- vatsalam 等人，<a href="#bookmark21">2023</a>年<a href="#bookmark22">b</a>）。</p><p><a href="#bookmark22">vatsalam 等</a>人，<a href="#bookmark22">2021 年）（<em>多选题</em> ）</a>。根据之前的工作，PopQA、PubHealth 和 Arc-Challenge 采用准确率作为评价指标。传记》采用<a href="#bookmark9">FactScore（Min 等人，</a> <a href="#bookmark9">2023 年）</a>作为评价指标。读者可参阅附录<a href="#bookmark42">B.1 了解</a>更多详情。</p><p><strong>5.2 基线</strong></p><p>我们主要将 CRAG 与不带检索和带检索的方法进行了比较，后者包括标准 RAG 和高级 RAG。</p><p><strong>无检索基线。</strong>我们评估了一些公共 LLM：LLaMA2-7B<a href="#bookmark43">,13B（Touvron 等</a>人，<a href="#bookmark43">2023b</a>）、指令调整模型 Alpaca-7B,<a href="#bookmark43">13B</a>（<a href="#bookmark44">Dubois</a> 等人，2023）和 CoVE65B（<a href="#bookmark45">Dhuliawala</a> <a href="#bookmark45">等人，2023）</a>，后者<a href="#bookmark45">引入了迭代工程</a>来改进 LLM 世代的事实性。此外，还包括 LLaMA2-chat13B 和 ChatGPT 等实用 LLM。</p><p><strong>标准 RAG。</strong> 我们对标准<a href="#bookmark12">RAG（Lewis 等人，2020 年）</a>进行了评估，在标准<a href="#bookmark12">RAG</a>中，<a href="#bookmark12">LLM</a>使用与我们系统中相同的检索器，在查询前加上检索到的最前沿文档<a href="#bookmark12">来生成</a>输出。   在这里，我们采用了几种公开指令调整的 LLM，包括 LLaMA2-<a href="#bookmark43">7B, 13B (Touvron et al.,2023b)、Alpaca-7B,13</a>B (<a href="#bookmark44">Dubois</a>et al.,<a href="#bookmark44">2023</a>)，以及 Self-RAG 中指令调整的 LLaMA2-7B<a href="#bookmark20">(Asai et al.,2023</a>)。</p><p><strong>高级 RAG。</strong><a href="#bookmark28">(1) SAIL（Luo 等人，2023 年）</a>在 Alpaca 指令调优数据上对 LM 进行了指令调优，并在指令前插入了最高检索文档。  (2) Self-RAG（<a href="#bookmark20">Asai</a>等人，<a href="#bookmark20">2023</a> 年）在指令调谐数据上对 LLaMA2 进行了调谐，该数据包含几组由 GPT-4 标记的反射标记<a href="#bookmark46">（OpenAI，</a> <a href="#bookmark46">2023</a> 年）。 (3) 继<a href="#bookmark20">Asai 等人</a> <a href="#bookmark20">（2023 年）</a>之后<a href="#bookmark20">，我们还</a>引用了使用私人数据训练的检索增强基线的结果： Ret-ChatGPT和Ret- LLaMA-chat采用了上述相同的增强技术，perplexity.ai则是基于InstructGPT的生产搜索系统。</p><p><strong>5.3 结果</strong></p><p>表<a href="#bookmark41">1</a>列出了四个数据集的结果。将所提方法与标准 RAG 相结合的模型命名为 CRAG，与 Self RAG 相结合的模型命名为 Self-CRAG。读者可参阅附录<a href="#bookmark37">B.2 了解</a>我们所提方法的更多实施细节。从这些结果中，我们可以得出以下结论：</p><p><em>首先，我们提出的方法可以显著提高 RAG 和 Self-RAG 的性能。</em></p><p>具体来说，当基于<em>SelfRAG-LaMA2-7b</em> 时，CRAG 在 PopQA 上的准确率为 19.0%，在 Biography 上的 FactScore 为 14.9%，在 PubHealth 上的准确率为 36.6%，在 Arc-Challenge 上的准确率为 8.1%；当基于<em>LLaMA2-hf-7b</em> 时，CRAG 在 PopQA 上的准确率为 2.1%，在 Biography 上的 FactScore 为 2.8%，在 Arc-Challenge 上的准确率为 2.0%。与目前最先进的 Self RAG 相比，当基于<em>LLaMA2-hf-7b</em> 时，Self-CRAG 在 PopQA 上的准确率为 20.0%，在 Biography 上的 FactScore 为 36.9%，在 Arc-Challenge 上的准确率为 4.0%；当基于<em>SelfRAG-LaMA2-7b</em> 时，Self-CRAG 在 PopQA 上的准确率为 6.9%，在 Biography 上的 FactScore 为 5.0%，在 PubHealth 上的准确率为 2.4%。这些结果表明了 CRAG 的适应性，即插即用，可用于基于 RAG 的方法。</p><p><em>其次，所提出的方法在各种基因迭代任务中具有很强的通用性。</em>  特别是表<a href="#bookmark41">1</a>中报告的这些基准<a href="#bookmark41">分别</a>代表了不同的实际场景，包括短格式实体生成（PopQA）、长格式生成（Bi- ography）和封闭集任务（PubHealth、Arc- Challenge）。 这些结果验证了 CRAG 的一贯有效性。它在一系列特定任务中的通用性凸显了其在不同场景中的强大能力和通用性。</p><p><em>第三，拟议方法在替换底层 LLM 生成器时表现出更大的灵活性。</em> 我们可以看到，当底层 LLM 从<em>SelfRAG-LLaMA2-7b</em> 变为<em>LLaMA2-hf-7b</em> 时，CRAG 仍然表现出极具竞争力的性能，而 Self-RAG 的性能则大幅下降，甚至在多个基准测试中表现逊于标准 RAG。 出现这些结果的原因是，Self-RAG 需要使用人类或 LLM 注释的数据进行指令调整，才能学会根据需要输出特殊的批注标记，而这种能力在普通 LLM 中是学不到的。 CRAG 对这种能力没有任何要求。 可以想象，当将来有更先进的 LLM 时，它们可以很容易地与 CRAG 相结合，而对于 Self-RAG 来说，额外的指令调整仍然是必要的。</p><p>5.4 消融研究</p><ul><li><p>每个触发动作的影响。   </p><p>  为了进一步验证检索评估器中所设计的触发动作的有效性，我们对拟议方法中移除的每个单一动作进行了消减测试，如表 2 所示。我们在 PopQA 数据集上进行了评估，以证明在准确性方面的性能变化。 具体来说，当删除 “正确或不正确 “动作时，该动作会与 “模糊 “动作合并，这样原本触发 “正确或不正确 “动作的比例就会触发 “模糊 “动作。 另一方面，当删除 “模糊 “操作时，只有一个阈值能让所有输入查询明确触发 “正确 “或 “不正确”。   从这些结果可以看出，无论删除哪个操作，性能都会下降，这说明每个操作都有助于提高生成的鲁棒性。</p></li></ul><ul><li><p>每个知识利用操作的影响。  </p><p>  表 3 说明了删除关键知识利用操作后性能的变化情况。在 PopQA 数据集上对准确率进行评估时，分别删除了文档细化、搜索查询重写和外部知识选择等知识利用操作。取消文档细化表示将原始检索文档直接输入到后续生成器中，与大多数现有工作一样。此外，去掉搜索查询重写表示在知识搜索过程中，问题不会被重写成由关键词组成的查询。最后，去掉知识选择指的是所有搜索到的网页内容都被视为外部知识，没有经过选择。 这些结果有助于得出这样的结论，即无论取消哪种知识利用操作，最终系统的性能都会下降，这表明每种知识利用操作都有助于提高知识的利用率。</p></li></ul><p>5.5 检索评价器的准确性<br>检索评估器的质量在很大程度上决定了整个系统的性能。鉴于文档检索结果，我们评估了检索评价器能否准确判断这些结果的整体质量。在 PopQA 数据集上，我们的检索评价器和商业 LLM ChatGPT 对文档检索结果的评估准确率如表 4 所示。实验中使用的 ChatGPT、ChatGPT-CoT 和 ChatGPT-few-shot 的提示信息可参见附录 A。结果表明，基于 T5 的轻量级再三值评价器在所有情况下的表现都明显优于具有竞争力的 ChatGPT。</p><p>5.6 对检索性能的鲁棒性<br>为了进一步验证所提方法对检索性能的鲁棒性，我们研究了不同检索性能下生成性能的变化情况。我们故意随机删除一部分准确的检索结果，以模仿低质量的检索结果，并评估其性能变化情况。图3展示了Self-RAG和Self-CRAG在PopQA数据集上的性能变化。  可以看出，随着检索性能的下降，Self-RAG 和 Self-CRAG 的生成性能也随之下降，这表明生成器在很大程度上依赖于检索器的质量。 此外，随着检索性能的下降，Self-CRAG 的生成性能比 Self-RAG 下降得更小。这些结果表明，在提高检索性能的稳健性方面，Self-CRAG 优于 Self-RAG。</p><p>6 结论<br>本文研究的问题是，如果检索出错，基于 RAG 的方法就会受到挑战，从而向生成式 LM 暴露出不准确和误导性的知识。   为了提高生成的鲁棒性，本文提出了修正检索增强生成方法（Corrective Retrieval Augmented Generation）。从根本上说，轻量级检索评估器是为了有区别地估计和触发三种知识检索行动。 通过进一步利用网络搜索和优化知识利用，CRAG 显著提高了自动自我修正和高效利用检索文档的能力。实验广泛证明了 CRAG 对基于 RAG 的方法的适应性，以及在长短格式生成任务中的通用性。</p><p>局限性<br>虽然我们主要从纠正的角度提出了改进 RAG 框架的建议，但如何更准确、更有效地检测和纠正错误知识仍有待进一步研究。尽管 CRAG 可以与各种基于 RAG 的方法无缝结合，但对检索评估器进行微调是不可避免的。  此外，网络搜索带来的潜在偏差也值得关注。  互联网资源的质量可能会有很大差异，如果不充分考虑这些数据，可能会给生成的输出结果带来噪音或误导性信息。未来的工作将进一步探索更加稳定可靠的检索增强方法。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/10/RGA%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/"/>
      <url>/2024/04/10/RGA%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="总体"><a href="#总体" class="headerlink" title="总体"></a>总体</h2><ol><li><a href="https://github.com/liguodongiot/llm-action">大模型总体技术原理github</a></li><li><a href="https://modelscope.cn/models?name=%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE1.5%20chat&amp;page=1">通义千问魔搭库</a></li></ol><h2 id="文档抽取器"><a href="#文档抽取器" class="headerlink" title="文档抽取器"></a>文档抽取器</h2><ol><li><a href="https://github.com/RapidAI/RapidStructure/blob/main/docs/README_Layout.md">RapidStructure</a></li></ol><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><ol><li><a href="https://github.com/KMnO4-zx/huanhuan-chat/tree/master/generation_dataset">小说对话微调案例</a></li><li><p><a href="https://github.com/KMnO4-zx/extract-dialogue?tab=readme-ov-file">基于Kor的对话抽取案例</a></p></li><li><p><a href="https://blog.csdn.net/qq_35812205/article/details/131965333">SFT数据介绍</a></p></li><li><a href="https://eyurtsev.github.io/kor/">数据抽取工具Kor主页</a></li><li><a href="https://python.langchain.com/docs/use_cases/extraction/">langchain的输出解析器</a></li><li><a href="https://github.com/QwenLM/Qwen/blob/main/README_CN.md">Qwen官方微调教程</a></li><li><a href="https://www.bilibili.com/video/BV1jc411t7is/?spm_id_from=333.788&amp;vd_source=8872e34898650237d1ceecc7b8095c56">qwen微调视频教程</a></li></ol><h2 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h2><ol><li><a href="https://github.com/AkariAsai/self-rag/tree/main">self-rag代码</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/04/07/%E5%BE%AE%E8%B0%83qwen-14b%E5%81%9A%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"/>
      <url>/2024/04/07/%E5%BE%AE%E8%B0%83qwen-14b%E5%81%9A%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --gpus <span class="built_in">all</span> --hostname=RAGLLM_8xA100 --shm-size=32g -v /mnt/self-define/luchenhao/Qwen-14B:/data/shared/Qwen/Qwen-Chat/ -p <span class="number">18022</span>:<span class="number">22</span> -p <span class="number">18080</span>:<span class="number">8080</span> -p <span class="number">18088</span>:<span class="number">8888</span> --name qwen_luchenhao <span class="number">10.101</span><span class="number">.12</span><span class="number">.128</span>/luchenhao/qwen:latest /<span class="built_in">bin</span>/bash</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus <span class="built_in">all</span> --hostname=Qwen_8xA100 --shm-size=32g -w /data/shared/Qwen -v /mnt/self-define/luchenhao:/root/llm_embed_models -p <span class="number">18022</span>:<span class="number">22</span> -p <span class="number">18080</span>:<span class="number">8080</span> -p <span class="number">18088</span>:<span class="number">8888</span> --name qwen_luchenhao <span class="number">10.101</span><span class="number">.12</span><span class="number">.128</span>/luchenhao/qwen:v0<span class="number">.2</span> /<span class="built_in">bin</span>/zsh</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus <span class="built_in">all</span> --hostname=Qwen_8xA100 --shm-size=32g -w /data/shared/Qwen -v /mnt/self-define/luchenhao:/root/llm_embed_models -p <span class="number">18022</span>:<span class="number">22</span> -p <span class="number">18080</span>:<span class="number">8080</span> -p <span class="number">18088</span>:<span class="number">8888</span> --name qwen_luchenhao --entrypoint /usr/sbin/sshd <span class="number">10.101</span><span class="number">.12</span><span class="number">.128</span>/luchenhao/qwen:v0<span class="number">.3</span> -D</span><br><span class="line"></span><br><span class="line">docker commit -a <span class="string">&quot;luchenhao&quot;</span> qwen_luchenhao <span class="number">10.101</span><span class="number">.12</span><span class="number">.128</span>/luchenhao/qwen:v0<span class="number">.4</span></span><br><span class="line"></span><br><span class="line">docker push <span class="number">10.101</span><span class="number">.12</span><span class="number">.128</span>/luchenhao/qwen:v0<span class="number">.4</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">进入自己的docker</span><br><span class="line">执行命令：apt-get update（更新docker的apt命令）</span><br><span class="line">执行命令：apt-get install ssh（安装ssh）</span><br><span class="line">执行命令：vim /etc/ssh/sshd_config（查询ssh配置）</span><br><span class="line">修改参数：PermitRootLogin参数修改为yes，保存</span><br><span class="line">执行命令：passwd，随后输入自己docker的ssh密码，输入两次，如下所示：</span><br><span class="line">执行命令：cd /var/run</span><br><span class="line">执行命令：mkdir /var/run/sshd</span><br><span class="line">执行命令：/usr/sbin/sshd -D &amp;（后台启动ssh服务）</span><br><span class="line">到此，自己docker内的ssh服务已启动</span><br><span class="line"></span><br><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">#填加以下内容：</span><br><span class="line">Port 22</span><br><span class="line">PermitRootLogin yes #允许root用户使用ssh登录</span><br><span class="line"></span><br><span class="line">/etc/init.d/ssh restart</span><br><span class="line"></span><br><span class="line">passwd</span><br><span class="line"></span><br><span class="line">ssh root@10.5.30.42 -p 18022</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Host 我的容器</span><br><span class="line">  HostName 10.5.30.42</span><br><span class="line">  Port 28028</span><br><span class="line">  User root</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/03/31/RAG%E4%BB%8B%E7%BB%8D/"/>
      <url>/2024/03/31/RAG%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="rag解决的问题"><a href="#rag解决的问题" class="headerlink" title="rag解决的问题"></a>rag解决的问题</h2><ol><li><p>解决大模型时效性</p></li><li><p>可以构建私有库（公开的大模型训练数据集上没有特征）</p></li><li><p>长尾（有一些数据用来训练的次数比较少，会被信息淹没）这种数据用来做rag可能比sft的效果要好</p><blockquote><p>sft和rag的区别有哪些？</p><ol><li>sft更新调度的周期比较长，新标注的数据进来要重新训练；rag是轻量的update</li><li>rag有可解释性，有据可查，source追寻</li><li>准确率比较难提升，有时候甚至是负向</li></ol></blockquote></li></ol><h2 id="意图识别-优质私域数据库-极强的检索能力"><a href="#意图识别-优质私域数据库-极强的检索能力" class="headerlink" title="意图识别+优质私域数据库+极强的检索能力"></a>意图识别+优质私域数据库+极强的检索能力</h2><h3 id="意图识别：走rag？走kg？走google-search？"><a href="#意图识别：走rag？走kg？走google-search？" class="headerlink" title="意图识别：走rag？走kg？走google search？"></a>意图识别：走rag？走kg？走google search？</h3><h3 id="优质私域数据库：索引、结构化、多样性"><a href="#优质私域数据库：索引、结构化、多样性" class="headerlink" title="优质私域数据库：索引、结构化、多样性"></a>优质私域数据库：索引、结构化、多样性</h3><h3 id="极强的检索能力：召回、排序"><a href="#极强的检索能力：召回、排序" class="headerlink" title="极强的检索能力：召回、排序"></a>极强的检索能力：召回、排序</h3><p><code>query生成能力是非常重要的</code></p><p><img src="/2024/03/31/RAG%E4%BB%8B%E7%BB%8D/截屏2024-03-31 15.18.16.png" alt="截屏2024-03-31 15.18.16"></p><h2 id="意图识别"><a href="#意图识别" class="headerlink" title="意图识别"></a>意图识别</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/03/28/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/03/28/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<ol><li><p>变量名</p></li><li><p>字符串大小写</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">&#x27;luke lu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(name.title())</span><br><span class="line">name.upper()</span><br><span class="line">name.lower()</span><br></pre></td></tr></table></figure></li><li><p>字符串中使用变量</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">first_name = &#x27;ada&#x27;</span><br><span class="line">last_name = &#x27;lu&#x27;</span><br><span class="line">full_name = f&quot;&#123;first_name&#125; and &#123;last_name&#125;&quot;</span><br><span class="line">print(full_name)</span><br></pre></td></tr></table></figure></li><li><p>字符串删除空白</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = &#x27;python &#x27;</span><br><span class="line">a= a.rstrip()</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">a = &#x27; python &#x27;</span><br><span class="line">a.rstrip()</span><br><span class="line">a.lstrip()</span><br><span class="line">a.strip()</span><br><span class="line"></span><br><span class="line">print(&quot;python&#x27;s is xxxx&quot;)</span><br><span class="line">print(&#x27;ptthon&#x27;s is xxxx&#x27;)</span><br></pre></td></tr></table></figure></li><li><p>数学运算</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2 ** 3</span><br><span class="line">#除法或者别的计算里面只要有浮点数，结果也是浮点数</span><br><span class="line">数字中可以存在下划线</span><br><span class="line"></span><br><span class="line">同一行代码中可以给多个变量赋值</span><br><span class="line">x, y, z = 0,0,0</span><br></pre></td></tr></table></figure></li><li><p>大写表示常量</p></li><li><p>注释</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">单行注释：#</span><br><span class="line">多行注释：&#x27;&#x27;&#x27;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/03/15/%E5%A6%82%E4%BD%95%E5%B0%86docker%E5%AE%B9%E5%99%A8%E6%89%93%E6%88%90%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2024/03/15/%E5%A6%82%E4%BD%95%E5%B0%86docker%E5%AE%B9%E5%99%A8%E6%89%93%E6%88%90%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="开启ssh服务"><a href="#开启ssh服务" class="headerlink" title="开启ssh服务"></a>开启ssh服务</h2><ol><li><p>保证自己的docker暴露ssh服务22端口,jupyter服务8888端口，需要在新建docker时增加参数，命令如下所示：</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --gpus all --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/self-define/luchenhao:/root/llm_embed_models -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luchenhao registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7 bash</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus all --hostname=RAGLLM_8xA100 --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/self-define/luchenhao:/root/llm_embed_models -p 28022:22 -p 28080:8080 -p 28088:8888 -p 28077:8077 -p 28066:8066 -p 28099:8099 --name ragllm_luchenhao_v3 10.101.12.128/luchenhao/ragllm:v1.0</span><br><span class="line"></span><br><span class="line">--entrypoint /bin/zsh -w /root</span><br><span class="line">--entrypoint /usr/sbin/sshd -D</span><br><span class="line">--entrypoint /root/.zshrc </span><br><span class="line">--entrypoint /root/start_service.sh</span><br><span class="line"></span><br><span class="line">docker run -it --gpus all --hostname=RAGLLM_8xA100 --shm-size=32g -w /root -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/self-define/luchenhao:/root/llm_embed_models -p 28022:22 -p 28080:8080 -p 28088:8888 -p 28077:8077 -p 28066:8066 -p 28099:8099 --name ragllm_luchenhao_v3 luchenhao/ragllm:v1 /bin/zsh</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus all --hostname=RAGLLM_8xA100 --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/self-define/luchenhao:/root/llm_embed_models -p 28022:22 -p 28080:8080 -p 28088:8888 -p 28077:8077 -p 28066:8066 -p 28099:8099 --name ragllm_luchenhao_v2 luchenhao/ragllm:v1 zsh</span><br><span class="line"></span><br><span class="line">docker run -itd --gpus all -p 8000:3000 -e MILVUS_URL=10.5.30.42:19530 --name attu_luchenhao zilliz/attu:v2.3.8</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker stop ragllm_luchenhao_v1.2</span><br><span class="line">docker rm ragllm_luchenhao_v1</span><br><span class="line">docker exec -it ragllm_luchenhao_v1.2 zsh</span><br><span class="line">docker commit -a &quot;luchenhao&quot; ragllm_luchenhao_v2 10.101.12.128/luchenhao/ragllm:v2</span><br><span class="line">docker commit -a &quot;luchenhao&quot; qwen_luchenhao 10.101.12.128/luchenhao/qwen:v0.3mi</span><br><span class="line">docker tag milvusdb/milvus:v2.4.0-rc.1-gpu harbor.alkaidos.cn/luchenhao/milvus:v1.0-gpu</span><br><span class="line">docker push harbor.alkaidos.cn/luchenhao/ragllm:v1.0</span><br><span class="line">docker push harbor.alkaidos.cn/luchenhao/milvus:v1.0-gpu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -it --gpus all --hostname=RAGLLM_8xA100 --shm-size=32g -v /mnt/self-define/luchenhao/Qwen-14B:/data/shared/Qwen/Qwen-Chat/ -p 18022:22 -p 18080:8080 -p 18088:8888 --name qwen_luchenhao_v1 10.101.12.128/luchenhao/qwen:1.0-cu121 /bin/bash</span><br></pre></td></tr></table></figure></li></ol><p>进入自己的docker<br>执行命令：apt-get update（更新docker的apt命令）<br>执行命令：apt-get install ssh（安装ssh）<br>执行命令：vim /etc/ssh/sshd_config（查询ssh配置）<br>修改参数：PermitRootLogin参数修改为yes，保存<br>执行命令：passwd，随后输入自己docker的ssh密码，输入两次，如下所示：<br>执行命令：cd /var/run<br>执行命令：mkdir /var/run/sshd<br>执行命令：/usr/sbin/sshd -D &amp;（后台启动ssh服务）<br>到此，自己docker内的ssh服务已启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">#填加以下内容：</span><br><span class="line">Port 22</span><br><span class="line">PermitRootLogin yes #允许root用户使用ssh登录</span><br><span class="line"></span><br><span class="line">/etc/init.d/ssh restart</span><br><span class="line"></span><br><span class="line">passwd</span><br><span class="line"></span><br><span class="line">ssh root@10.5.30.42 -p 58022</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host 我的容器</span><br><span class="line">  HostName 10.5.30.42</span><br><span class="line">  Port 28028</span><br><span class="line">  User root</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions</span><br><span class="line"></span><br><span class="line">git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting</span><br><span class="line"></span><br><span class="line">plugins=(git z zsh-autosuggestions zsh-syntax-highlighting)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#安装jupyter</span><br><span class="line">pip install jupyter</span><br><span class="line"></span><br><span class="line">#生成jupyter配置文件，这个会生成配置文件.jupyter/jupyter_notebook_config.py</span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line"></span><br><span class="line">nohup jupyter lab --allow-root &gt; jupyterlab.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 找出包含jupyter的进程</span><br><span class="line">ps -aux | grep jupyter</span><br><span class="line"></span><br><span class="line"># 会列出很多，然后找到 jupyter lab相关的进程， 一般情况下第二列就是pid, 如，我的进程是 9608，然后再用如下命令即可杀掉进程</span><br><span class="line">kill -9 9608</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 做milvus用的</span><br><span class="line">sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br><span class="line">sudo chmod +x /usr/local/bin/docker-compose</span><br><span class="line">docker-compose --version</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export https_proxy=http://127.0.0.1:55719 http_proxy=http://127.0.0.1:55719 all_proxy=socks5://127.0.0.1:55719</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -P 28022 &#x27;/Users/luke/Downloads/lowcode-project&#x27; root@10.5.30.42:/root</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/03/06/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E6%96%B0%E9%85%8D%E7%BD%AE%E4%B8%80%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2024/03/06/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E6%96%B0%E9%85%8D%E7%BD%AE%E4%B8%80%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux下创建新用户"><a href="#Linux下创建新用户" class="headerlink" title="Linux下创建新用户"></a>Linux下创建新用户</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 -m 命令来指定新建用户目录，使用 -s 来指定分配的终端为bash</span></span><br><span class="line">$ sudo useradd -m -s /bin/bash [user_name]</span><br><span class="line">$ sudo passwd [user_name]</span><br></pre></td></tr></table></figure><h2 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub luchenhao@10.105.10.51</span><br></pre></td></tr></table></figure><h2 id="添加sudo权限"><a href="#添加sudo权限" class="headerlink" title="添加sudo权限"></a>添加sudo权限</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> u+w /etc/sudoers</span><br><span class="line">vi /etc/sudoers </span><br><span class="line">找到这行 root ALL=(ALL) ALL,在他下面添加lee ALL=(ALL) ALL(这里的lee是你的用户名)</span><br><span class="line"><span class="built_in">chmod</span> u-w /etc/sudoers</span><br></pre></td></tr></table></figure><h2 id="安装Zsh"><a href="#安装Zsh" class="headerlink" title="安装Zsh"></a>安装Zsh</h2><p><a href="https://sysin.org/blog/linux-zsh/">https://sysin.org/blog/linux-zsh/</a></p><h2 id="配置config文件"><a href="#配置config文件" class="headerlink" title="配置config文件"></a>配置config文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host 10.11.12.218</span><br><span class="line">  HostName 10.11.12.218</span><br><span class="line">  User luchenhao</span><br><span class="line">  PreferredAuthentications publickey</span><br><span class="line">  IdentityFile /Users/luke/.ssh/id_rsa</span><br></pre></td></tr></table></figure><h2 id="docker添加私有仓库"><a href="#docker添加私有仓库" class="headerlink" title="docker添加私有仓库"></a>docker添加私有仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 在daemon.json里添加insecure-registries</span><br><span class="line">$ vim /etc/docker/daemon.json</span><br><span class="line">&quot;insecure-registries&quot;: [&quot;https://harbor.dubhe.ai&quot;,&quot;harbor.alkaidos.cn&quot;,&quot;10.101.12.128&quot;,&quot;10.101.12.129&quot;]</span><br></pre></td></tr></table></figure><h2 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker login 10.101.12.128</span><br><span class="line">docker pull 10.101.12.128/wxzhang/ragllm:v1.7</span><br><span class="line"></span><br><span class="line">在git上面添加公钥（https://zhuanlan.zhihu.com/p/454666519）</span><br><span class="line"></span><br><span class="line">git clone https://github.com/CodexDive/LLMChat.git</span><br><span class="line">or</span><br><span class="line">git clone git@github.com:CodexDive/LLMChat.git</span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7</span><br></pre></td></tr></table></figure><h2 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g --platform linux/amd64 -v /Users/luke/LLMChat:/root/LLMChat -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luke 10.101.12.128/wxzhang/ragllm:v1.3 bash</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/nas/luchenhao:/root/llm_embed_models -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luchenhao registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7 bash</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -v /mnt/self-define/luchenhao:/root/llm_embed_models -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luchenhao registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7 bash</span><br><span class="line"></span><br><span class="line">docker exec -it ragllm_luchenhao bash</span><br><span class="line">docker exec -it ragllm_luchenhao zsh</span><br><span class="line"></span><br><span class="line">conda activate ragllm</span><br><span class="line"></span><br><span class="line">cd /root/LLMChat</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">sh start.sh # 一键启动</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">sh start_webui.sh <span class="comment">#启动ui</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/BAAI/bge-reranker-large</span><br><span class="line">git lfs pull --exclude=&quot;*.safetensors&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">##~/.pip/pip.conf</span><br><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host = https://pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -p xxxx:22 -p xxxx:8888 --runtime=nvidia -it --name xiechong_ssh_test -v /home/training:/data xxxxx /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#修改~/.zshrc文件</span><br><span class="line">export PATH=&quot;/root/miniconda3/envs/chatchat/bin:$PATH&quot;</span><br><span class="line">conda activate chatchat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x27;argon2:$argon2id$v=19$m=10240,t=10,p=8$GcOtNkalvYPXPNR5uQh5Gg$mSHuTZz7KYQzq9U69FUIqTZZ1vdHFEKnP6UXbuWbJng&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker commit -a &quot;luchenhao&quot; ragllm_luchenhao luchenhao/ragllm:v1</span><br><span class="line">docker commit -a &quot;luchenhao&quot; qwen_luchenhao 10.101.12.128/luchenhao/qwen:v0.3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Langchain-Chatchat知识库接口代码详解</title>
      <link href="/2024/03/04/Langchain-Chatchat%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/"/>
      <url>/2024/03/04/Langchain-Chatchat%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本质上是在Langchain基础上封装的一层聊天服务，可以对接底层多种离线LLM和在线的LLM（也可以对接自定义的在线LLM）。提供基于<a href="https://so.csdn.net/so/search?q=知识库&amp;spm=1001.2101.3001.7020">知识库</a>聊天功能相关的一系列API。</p></blockquote><p><img src="/2024/03/04/Langchain-Chatchat%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/大模型-9533543.png" alt="大模型"></p><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── configs</span><br><span class="line">│   ├── basic<span class="emphasis">_config.py</span></span><br><span class="line"><span class="emphasis">│   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   ├── kb_</span>config.py</span><br><span class="line">│   ├── model<span class="emphasis">_config.py</span></span><br><span class="line"><span class="emphasis">│   ├── prompt_</span>config.py</span><br><span class="line">│   └── server<span class="emphasis">_config.py</span></span><br><span class="line"><span class="emphasis">├── document_</span>loaders</span><br><span class="line">│   ├── FilteredCSVloader.py</span><br><span class="line">│   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   ├── mydocloader.py</span><br><span class="line">│   ├── myimgloader.py</span><br><span class="line">│   ├── mypdfloader.py</span><br><span class="line">│   ├── mypptloader.py</span><br><span class="line">│   ├── ocr.py</span><br><span class="line">├── img</span><br><span class="line">│   ├── chatchat<span class="emphasis">_icon_</span>blue<span class="emphasis">_square_</span>v2.png</span><br><span class="line">│   ├── llm<span class="emphasis">_application_</span>icon.png</span><br><span class="line">│   ├── logo-long-chatchat-trans-v2.png</span><br><span class="line">│   └── logo.png</span><br><span class="line">├── init<span class="emphasis">_database.py</span></span><br><span class="line"><span class="emphasis">├── init_</span>database.sh</span><br><span class="line">├── knowledge<span class="emphasis">_base</span></span><br><span class="line"><span class="emphasis">│   ├── info.db</span></span><br><span class="line"><span class="emphasis">│   ├── samples</span></span><br><span class="line"><span class="emphasis">│   │   ├── content</span></span><br><span class="line"><span class="emphasis">│   │   │   └── test.txt</span></span><br><span class="line"><span class="emphasis">│   │   └── vector_</span>store</span><br><span class="line">│   │       └── bge-large-zh</span><br><span class="line">│   └── 政策知识库</span><br><span class="line">│       ├── content</span><br><span class="line">│       │   ├── 10.关于加快全域创新策源地建设推动.docx</span><br><span class="line">│       └── vector<span class="emphasis">_store</span></span><br><span class="line"><span class="emphasis">│           └── bge-large-zh</span></span><br><span class="line"><span class="emphasis">│               ├── index.faiss</span></span><br><span class="line"><span class="emphasis">│               └── index.pkl</span></span><br><span class="line"><span class="emphasis">├── main.py</span></span><br><span class="line"><span class="emphasis">├── README.md</span></span><br><span class="line"><span class="emphasis">├── requirements.txt</span></span><br><span class="line"><span class="emphasis">├── server</span></span><br><span class="line"><span class="emphasis">│   ├── api.py</span></span><br><span class="line"><span class="emphasis">│   ├── callback_</span>handler</span><br><span class="line">│   │   ├── conversation<span class="emphasis">_callback_</span>handler.py</span><br><span class="line">│   ├── chat</span><br><span class="line">│   │   ├── chat.py</span><br><span class="line">│   │   ├── completion.py</span><br><span class="line">│   │   ├── file<span class="emphasis">_chat.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── knowledge_</span>base<span class="emphasis">_chat.py</span></span><br><span class="line"><span class="emphasis">│   │   └── utils.py</span></span><br><span class="line"><span class="emphasis">│   ├── db</span></span><br><span class="line"><span class="emphasis">│   │   ├── base.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── models</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── conversation_</span>model.py</span><br><span class="line">│   │   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_base_</span>model.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_file_</span>model.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_metadata_</span>model.py</span><br><span class="line">│   │   │   ├── message<span class="emphasis">_model.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── repository</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── conversation_</span>repository.py</span><br><span class="line">│   │   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_base_</span>repository.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_file_</span>repository.py</span><br><span class="line">│   │   │   ├── knowledge<span class="emphasis">_metadata_</span>repository.py</span><br><span class="line">│   │   │   ├── message<span class="emphasis">_repository.py</span></span><br><span class="line"><span class="emphasis">│   │   └── session.py</span></span><br><span class="line"><span class="emphasis">│   ├── embeddings_</span>api.py</span><br><span class="line">│   ├── knowledge<span class="emphasis">_base</span></span><br><span class="line"><span class="emphasis">│   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── kb_</span>api.py</span><br><span class="line">│   │   ├── kb<span class="emphasis">_cache</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── base.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── faiss_</span>cache.py</span><br><span class="line">│   │   ├── kb<span class="emphasis">_doc_</span>api.py</span><br><span class="line">│   │   ├── kb<span class="emphasis">_service</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── base.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── default_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── es_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── faiss_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── milvus_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── pg_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   │   └── zilliz_</span>kb<span class="emphasis">_service.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── kb_</span>summary</span><br><span class="line">│   │   │   ├── base.py</span><br><span class="line">│   │   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   │   │   └── summary<span class="emphasis">_chunk.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── kb_</span>summary<span class="emphasis">_api.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── migrate.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── model</span></span><br><span class="line"><span class="emphasis">│   │   │   ├── kb_</span>document<span class="emphasis">_model.py</span></span><br><span class="line"><span class="emphasis">│   │   └── utils.py</span></span><br><span class="line"><span class="emphasis">│   ├── llm_</span>api.py</span><br><span class="line">│   ├── memory</span><br><span class="line">│   │   ├── conversation<span class="emphasis">_db_</span>buffer<span class="emphasis">_memory.py</span></span><br><span class="line"><span class="emphasis">│   ├── model_</span>workers</span><br><span class="line">│   │   ├── azure.py</span><br><span class="line">│   │   ├── base.py</span><br><span class="line">│   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   │   └── qwen.py</span><br><span class="line">│   ├── reranker</span><br><span class="line">│   │   └── reranker.py</span><br><span class="line">│   ├── static</span><br><span class="line">│   │   ├── favicon.png</span><br><span class="line">│   │   ├── redoc.standalone.js</span><br><span class="line">│   │   ├── swagger-ui-bundle.js</span><br><span class="line">│   │   └── swagger-ui.css</span><br><span class="line">│   └── utils.py</span><br><span class="line">├── shutdown<span class="emphasis">_all.sh</span></span><br><span class="line"><span class="emphasis">├── start.sh</span></span><br><span class="line"><span class="emphasis">├── start_</span>webui.sh</span><br><span class="line">├── tests</span><br><span class="line">│   ├── api</span><br><span class="line">│   │   ├── test<span class="emphasis">_kb_</span>api.py</span><br><span class="line">│   │   ├── test<span class="emphasis">_kb_</span>api<span class="emphasis">_request.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── test_</span>kb<span class="emphasis">_summary_</span>api.py</span><br><span class="line">│   │   ├── test<span class="emphasis">_llm_</span>api.py</span><br><span class="line">│   │   ├── test<span class="emphasis">_server_</span>state<span class="emphasis">_api.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── test_</span>stream<span class="emphasis">_chat_</span>api.py</span><br><span class="line">│   │   └── test<span class="emphasis">_stream_</span>chat<span class="emphasis">_api_</span>thread.py</span><br><span class="line">│   ├── custom<span class="emphasis">_splitter</span></span><br><span class="line"><span class="emphasis">│   │   └── test_</span>different<span class="emphasis">_splitter.py</span></span><br><span class="line"><span class="emphasis">│   ├── document_</span>loader</span><br><span class="line">│   │   ├── test<span class="emphasis">_imgloader.py</span></span><br><span class="line"><span class="emphasis">│   │   └── test_</span>pdfloader.py</span><br><span class="line">│   ├── kb<span class="emphasis">_vector_</span>db</span><br><span class="line">│   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   │   ├── test<span class="emphasis">_faiss_</span>kb.py</span><br><span class="line">│   │   ├── test<span class="emphasis">_milvus_</span>db.py</span><br><span class="line">│   │   └── test<span class="emphasis">_pg_</span>db.py</span><br><span class="line">│   ├── samples</span><br><span class="line">│   │   ├── ocr<span class="emphasis">_test.jpg</span></span><br><span class="line"><span class="emphasis">│   │   └── ocr_</span>test.pdf</span><br><span class="line">│   ├── test<span class="emphasis">_migrate.py</span></span><br><span class="line"><span class="emphasis">│   └── test_</span>online<span class="emphasis">_api.py</span></span><br><span class="line"><span class="emphasis">├── text_</span>splitter</span><br><span class="line">│   ├── ali<span class="emphasis">_text_</span>splitter.py</span><br><span class="line">│   ├── chinese<span class="emphasis">_recursive_</span>text<span class="emphasis">_splitter.py</span></span><br><span class="line"><span class="emphasis">│   ├── chinese_</span>text<span class="emphasis">_splitter.py</span></span><br><span class="line"><span class="emphasis">│   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   └── zh_</span>title<span class="emphasis">_enhance.py</span></span><br><span class="line"><span class="emphasis">├── webui_</span>pages</span><br><span class="line">│   ├── dialogue</span><br><span class="line">│   │   ├── dialogue.py</span><br><span class="line">│   │   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   ├── <span class="strong">__init__</span>.py</span><br><span class="line">│   ├── knowledge<span class="emphasis">_base</span></span><br><span class="line"><span class="emphasis">│   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   ├── knowledge_</span>base.py</span><br><span class="line">│   ├── model<span class="emphasis">_config</span></span><br><span class="line"><span class="emphasis">│   │   ├── <span class="strong">__init__</span>.py</span></span><br><span class="line"><span class="emphasis">│   │   └── model_</span>config.py</span><br><span class="line">│   └── utils.py</span><br><span class="line">└── webui.py</span><br></pre></td></tr></table></figure><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">主函数start_main_server: （主进程fork出7个子进程，每个模型起一个子进程model worker）</span><br><span class="line">-&gt; run_controller：fastchat框架三组件之一，提供了模型纳管、切换和对外服务能力，核心代码在create_controller_app中，fastchat controller用于管理分布式gpu workers</span><br><span class="line">-&gt; run_model_worker：fastchat框架三组件之一，是维护本地大模型实例的web服务，通过create_model_worker_app启动大模型实例</span><br><span class="line">-&gt; run_openai_api：fastchat框架三组件之一，提供openai兼容的api服务，可以通过langchain或openai包像访问chatgpt一样访问各家大模型服务</span><br><span class="line">-&gt; run_api_server：基于fastapi框架，对外提供大模型对话、知识库对话、搜索引擎对话、知识库管理和知识库内文件管理等api服务</span><br><span class="line">-&gt; run_webui：以子进程的方式启动webui.py，对外提供webui界面</span><br><span class="line">————————————————</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="几个重要的py"><a href="#几个重要的py" class="headerlink" title="几个重要的py"></a>几个重要的py</h2><ol><li><p>run_webui -&gt; webui.py</p></li><li><p>run_api_server -&gt; server/api.py: 知识库问答、模型管理</p></li><li><p>knowledge_base_chat.py</p><ul><li>query查询语句</li><li>knowledge_base_name知识库名称</li><li>top_k匹配向量数</li><li>score_threshold知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右</li><li>history历史对话记录</li><li>model_nameLLM 模型名称</li><li>local_doc_url是否返回原知识文件路径</li></ul></li></ol><h2 id="知识库对话"><a href="#知识库对话" class="headerlink" title="知识库对话"></a>知识库对话</h2><p>知识库对话功能的后端是在server/chat/knowledge_base_chat.py方法中实现的。</p><p>主要输入参数：</p><ul><li>query查询语句</li><li>knowledge_base_name知识库名称</li><li>top_k匹配向量数，即本地知识库召回</li><li>score_threshold知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右</li><li>history历史对话记录</li><li>model_nameLLM 模型名称</li><li>local_doc_url是否返回原知识文件路径</li></ul><p>在方法内部，首先通过第一步多线程search_docs方法查询向量数据库，召回top_k个相似文档。然后通过换行符拼接成context，这就是输入大模型的知识上下文。随后使用模型代理和历史对话记录，初始化chain对象。最后将查询语句和上下文信息输入chain对象，通过asyncio库异步调用大模型进行处理。末尾部分，将知识出处放入返回的json中。</p><ol><li><p>第一步</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docs = <span class="keyword">await</span> run_in_threadpool(search_docs,</span><br><span class="line">                                       query=query,</span><br><span class="line">                                       knowledge_base_name=knowledge_base_name,</span><br><span class="line">                                       top_k=top_k,</span><br><span class="line">                                       score_threshold=score_threshold)</span><br></pre></td></tr></table></figure></li><li><p>第二步：使用BAAI/bge-reranker-large的交叉编码器，构造query和_doc的sentence_pairs,召回topk的document和relevance_score，然后再把doc.page_content通过换行符拼接在一块</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compress_documents</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            documents: <span class="type">Sequence</span>[Document],</span></span><br><span class="line"><span class="params">            query: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">            callbacks: <span class="type">Optional</span>[Callbacks] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Sequence</span>[Document]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compress documents using Cohere&#x27;s rerank API.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            documents: A sequence of documents to compress.</span></span><br><span class="line"><span class="string">            query: The query to use for compressing the documents.</span></span><br><span class="line"><span class="string">            callbacks: Callbacks to run during the compression process.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A sequence of compressed documents.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(documents) == <span class="number">0</span>:  <span class="comment"># to avoid empty api call</span></span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        doc_list = <span class="built_in">list</span>(documents)</span><br><span class="line">        _docs = [d.page_content <span class="keyword">for</span> d <span class="keyword">in</span> doc_list]</span><br><span class="line">        sentence_pairs = [[query, _doc] <span class="keyword">for</span> _doc <span class="keyword">in</span> _docs]</span><br><span class="line">        results = self._model.predict(sentences=sentence_pairs,</span><br><span class="line">                                      batch_size=self.batch_size,</span><br><span class="line">                                      <span class="comment">#  show_progress_bar=self.show_progress_bar,</span></span><br><span class="line">                                      num_workers=self.num_workers,</span><br><span class="line">                                      <span class="comment">#  activation_fct=self.activation_fct,</span></span><br><span class="line">                                      <span class="comment">#  apply_softmax=self.apply_softmax,</span></span><br><span class="line">                                      convert_to_tensor=<span class="literal">True</span></span><br><span class="line">                                      )</span><br><span class="line">        top_k = self.top_n <span class="keyword">if</span> self.top_n &lt; <span class="built_in">len</span>(results) <span class="keyword">else</span> <span class="built_in">len</span>(results)</span><br><span class="line"></span><br><span class="line">        values, indices = results.topk(top_k)</span><br><span class="line">        final_results = []</span><br><span class="line">        <span class="keyword">for</span> value, index <span class="keyword">in</span> <span class="built_in">zip</span>(values, indices):</span><br><span class="line">            doc = doc_list[index]</span><br><span class="line">            doc.metadata[<span class="string">&quot;relevance_score&quot;</span>] = value</span><br><span class="line">            final_results.append(doc)</span><br><span class="line">        <span class="keyword">return</span> final_results</span><br></pre></td></tr></table></figure></li><li><p>第三步: 使用历史对话记录构造prompt模板，初始化chain对象。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">history = [History.from_data(h) <span class="keyword">for</span> h <span class="keyword">in</span> history]</span><br><span class="line"></span><br><span class="line">context = <span class="string">&quot;\n&quot;</span>.join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(docs) == <span class="number">0</span>:  <span class="comment"># 如果没有找到相关文档，使用empty模板</span></span><br><span class="line">            prompt_template = get_prompt_template(<span class="string">&quot;knowledge_base_chat&quot;</span>, <span class="string">&quot;empty&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prompt_template = get_prompt_template(<span class="string">&quot;knowledge_base_chat&quot;</span>, prompt_name)</span><br><span class="line">        input_msg = History(role=<span class="string">&quot;user&quot;</span>, content=prompt_template).to_msg_template(<span class="literal">False</span>)</span><br><span class="line">        chat_prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">            [i.to_msg_template() <span class="keyword">for</span> i <span class="keyword">in</span> history] + [input_msg])</span><br><span class="line"></span><br><span class="line">        chain = LLMChain(prompt=chat_prompt, llm=model)</span><br></pre></td></tr></table></figure></li><li><p>将查询语句和上下文信息输入chain对象，通过asyncio库异步调用大模型进行处理</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Begin a task that runs in the background.</span></span><br><span class="line">task = asyncio.create_task(wrap_done(</span><br><span class="line">    chain.acall(&#123;<span class="string">&quot;context&quot;</span>: context, <span class="string">&quot;question&quot;</span>: query&#125;),</span><br><span class="line">    callback.done),</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>末尾部分，将回答和知识出处放入返回的json中</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> stream:</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> token <span class="keyword">in</span> callback.aiter():</span><br><span class="line">        <span class="comment"># Use server-sent-events to stream the response</span></span><br><span class="line">        <span class="keyword">yield</span> json.dumps(&#123;<span class="string">&quot;answer&quot;</span>: token&#125;, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">yield</span> json.dumps(&#123;<span class="string">&quot;docs&quot;</span>: source_documents&#125;, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    answer = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> token <span class="keyword">in</span> callback.aiter():</span><br><span class="line">        answer += token</span><br><span class="line">    <span class="keyword">yield</span> json.dumps(&#123;<span class="string">&quot;answer&quot;</span>: answer,</span><br><span class="line">                      <span class="string">&quot;docs&quot;</span>: source_documents&#125;,</span><br><span class="line">                     ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="文件对话"><a href="#文件对话" class="headerlink" title="文件对话"></a>文件对话</h2><ol><li>和知识库对话大体流程相同，唯一不同是直接根据query查询临时数据库中匹配的docs</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">embed_func = EmbeddingsFunAdapter()</span><br><span class="line">embeddings = <span class="keyword">await</span> embed_func.aembed_query(query)</span><br><span class="line"><span class="keyword">with</span> memo_faiss_pool.acquire(knowledge_id) <span class="keyword">as</span> vs:</span><br><span class="line">    docs = vs.similarity_search_with_score_by_vector(embeddings, k=top_k, score_threshold=score_threshold)</span><br><span class="line">    docs = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> docs]</span><br><span class="line"></span><br><span class="line">context = <span class="string">&quot;\n&quot;</span>.join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs])</span><br></pre></td></tr></table></figure><h2 id="多线程编程（并行）and-单线程异步编程（并发）"><a href="#多线程编程（并行）and-单线程异步编程（并发）" class="headerlink" title="多线程编程（并行）and 单线程异步编程（并发）"></a>多线程编程（并行）and 单线程异步编程（并发）</h2><ul><li>对于IO密集的应用程序适合使用单线程异步编程（cpu核心多数在等待IO操作、网络通讯）</li><li><p>多线程编程适合计算量大的应用，让每个cpu核心发挥最大的功效，而不是消耗在空闲的等待上</p></li><li><p>python的asynic语法就是单进程单线程的机制，计算核心是一个event loop</p></li></ul><ol><li>使用langchain的异步链函数，在后台将协程注册为task等待运行返回</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">chain = LLMChain(prompt=chat_prompt, llm=model)</span><br><span class="line"><span class="comment"># Begin a task that runs in the background.</span></span><br><span class="line">task = asyncio.create_task(wrap_done(</span><br><span class="line">    chain.acall(&#123;<span class="string">&quot;context&quot;</span>: context, <span class="string">&quot;question&quot;</span>: query&#125;),</span><br><span class="line">    callback.done),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ol><li>这是一个在server/utils.py中的功能函数，类似fn的装饰器，作用是告诉你event loop中的task完成了，或者没完成抛出异常</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">wrap_done</span>(<span class="params">fn: Awaitable, event: asyncio.Event</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Wrap an awaitable with a event to signal when it&#x27;s done or an exception is raised.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">await</span> fn</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logging.exception(e)</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> handle exception</span></span><br><span class="line">        msg = <span class="string">f&quot;Caught exception: <span class="subst">&#123;e&#125;</span>&quot;</span></span><br><span class="line">        logger.error(<span class="string">f&#x27;<span class="subst">&#123;e.__class__.__name__&#125;</span>: <span class="subst">&#123;msg&#125;</span>&#x27;</span>,</span><br><span class="line">                     exc_info=e <span class="keyword">if</span> log_verbose <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="comment"># Signal the aiter to stop.把当前这个awaitable fn放行</span></span><br><span class="line">        event.<span class="built_in">set</span>()</span><br></pre></td></tr></table></figure><ol><li>这是knowledge base chat模块中提供的默认prompt</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;default&quot;</span>:</span><br><span class="line">            <span class="string">&#x27;&lt;指令&gt;根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，&#x27;</span></span><br><span class="line">            <span class="string">&#x27;不允许在答案中添加编造成分，答案请使用中文。 &lt;/指令&gt;\n&#x27;</span></span><br><span class="line">            <span class="string">&#x27;&lt;已知信息&gt;&#123;&#123; context &#125;&#125;&lt;/已知信息&gt;\n&#x27;</span></span><br><span class="line">            <span class="string">&#x27;&lt;问题&gt;&#123;&#123; question &#125;&#125;&lt;/问题&gt;\n&#x27;</span>,</span><br></pre></td></tr></table></figure><ol><li><h2 id="异步编程基础"><a href="#异步编程基础" class="headerlink" title="异步编程基础"></a>异步编程基础</h2></li><li><p>asynic很适合解决网络通讯的问题，因为网络通讯很多时间是花在等待上的，即io bound task（真正做任务的时间是很少的，都花在io通信等待上）</p></li><li><p>调用协程(async def)返回的是协程对象（coroutine object），协程中的代码不会运行</p><ol><li>await 也可以创建协程对象、可以注册task、可以在event loop里运行task</li><li>await要干的活太多了，我们把它创建协程对象并注册task的那一步给到create_task来做，这样就可以把所有的task都先注册完，再让event loop来自己选择什么时候空闲做什么task</li></ol></li><li>从syncio变到asyncio的入口是asyncio.run(coroutine),这步操作可以进入event loop模式，然后把协程变成task</li><li>gather也可以把coroutine变成task注册进event loop，然后马上交回控制权，让主程序的进行并发</li><li>coroutine不变成task是没有办法执行的</li><li>拿到返回值是需要用await的</li><li><code>单线程异步的核心是控制权的交还</code>、<code>只有event loop中有多个task时候主函数才会并发，所以要先注册task</code></li><li><code>交还控制权的方式有await和函数运行完毕</code></li></ol><h2 id="SSE通信server-sent-events"><a href="#SSE通信server-sent-events" class="headerlink" title="SSE通信server-sent-events"></a>SSE通信server-sent-events</h2><blockquote><p>前后端数据通信模式，使用SSE，可以让服务端一边生成内容，一边将数据返回给客户端，这样客户端可以不用等待服务端将内容全部生成。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI, Request  </span><br><span class="line">from sse_starlette.sse import EventSourceResponse  </span><br><span class="line">import asyncio  </span><br><span class="line"></span><br><span class="line">app = FastAPI()  </span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)  </span><br><span class="line">async def root(request: Request):  </span><br><span class="line">    async def event_generator(request: Request):  </span><br><span class="line">        res_str = &quot;七夕情人节即将来临，我们为您准备了精美的鲜花和美味的蛋糕&quot;  </span><br><span class="line">        for i in res_str:  </span><br><span class="line">            if await request.is_disconnected():  </span><br><span class="line">                print(&quot;连接已中断&quot;)  </span><br><span class="line">                break  </span><br><span class="line">            yield &#123;  </span><br><span class="line">                &quot;event&quot;: &quot;message&quot;,  </span><br><span class="line">                &quot;retry&quot;: 15000,  </span><br><span class="line">                &quot;data&quot;: i  </span><br><span class="line">            &#125;  </span><br><span class="line"></span><br><span class="line">            await asyncio.sleep(0.1)  </span><br><span class="line">    g = event_generator(request)  </span><br><span class="line">    return EventSourceResponse(g)</span><br></pre></td></tr></table></figure><p>EventSourceResponse类可以传入异步生成器(generator)，这里为什么要传一个生成器呢？ 由于采用SSE响应的数据一般是一点一点的返回给客户端，不是一次性的返回，像上面的代码，EventSourceResponse对象每次从g这个生成器中获取到数据，再将数据组装成sse的标准格式。</p><h2 id="FastAPI路由"><a href="#FastAPI路由" class="headerlink" title="FastAPI路由"></a>FastAPI路由</h2><p>FastAPI 中定义路由的方式主要有两种，一种是使用 FastAPI 应用实例的方法（例如 <code>app.get()</code>），一种是使用<a href="https://so.csdn.net/so/search?q=装饰器&amp;spm=1001.2101.3001.7020">装饰器</a>（例如 <code>@app.get()</code>）</p><h2 id="私有化部署"><a href="#私有化部署" class="headerlink" title="私有化部署"></a>私有化部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/CodexDive/LLMChat.git</span><br><span class="line">or</span><br><span class="line">git <span class="built_in">clone</span> git@github.com:CodexDive/LLMChat.git</span><br><span class="line"></span><br><span class="line">docker login 10.101.12.128</span><br><span class="line"><span class="comment"># 机器：10.101.14.36</span></span><br><span class="line">docker pull 10.101.12.128/wxzhang/ragllm:v1.3</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g --platform linux/amd64 -v /Users/luke/LLMChat:/root/LLMChat -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luke 10.101.12.128/wxzhang/ragllm:v1.3 bash</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luke 10.101.12.128/wxzhang/ragllm:v1.3 bash</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">exec</span> -it ragllm_luke bash</span><br><span class="line"></span><br><span class="line">conda activate ragllm</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /root/LLMChat</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">sh start.sh <span class="comment"># 一键启动</span></span><br><span class="line"><span class="comment">#sh start_webui.sh #启动ui</span></span><br></pre></td></tr></table></figure><h2 id="命令行启动"><a href="#命令行启动" class="headerlink" title="命令行启动"></a>命令行启动</h2><ul><li>调试信息包含系统环境、使用显卡、拉起子进程数量、加载模型、大模型参数缓存文件（checkpoint shards)</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> 大模型应用工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/02/26/chatchat/"/>
      <url>/2024/02/26/chatchat/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd -u root --name paddle_docker_jupyter -p 8080:8080 --gpus all -v $PWD:/paddle --network=host registry.baidubce.com/paddlepaddle/paddle:2.6.0-gpu-cuda12.0-cudnn8.9-trt8.6</span><br><span class="line"></span><br><span class="line">docker exec -it paddle_docker_jupyter /bin/bash</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">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</span><br></pre></td></tr></table></figure><h2 id="私有化部署"><a href="#私有化部署" class="headerlink" title="私有化部署"></a>私有化部署</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/CodexDive/LLMChat.git</span><br><span class="line">or</span><br><span class="line">git clone git@github.com:CodexDive/LLMChat.git</span><br><span class="line"></span><br><span class="line">docker login 10.101.12.128</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">机器：10.101.14.36</span></span><br><span class="line">docker pull 10.101.12.128/wxzhang/ragllm:v1.3</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g --platform linux/amd64 -v /Users/luke/LLMChat:/root/LLMChat -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luke 10.101.12.128/wxzhang/ragllm:v1.3 bash</span><br><span class="line"></span><br><span class="line">docker run -itd --shm-size=32g -v /home/luchenhao/workspace/LLMChat:/root/LLMChat -p 28028:22 -p 28082:8080 -p 28086:8888 -p 28087:8077 -p 28068:8066 --name ragllm_luke 10.101.12.128/wxzhang/ragllm:v1.3 bash</span><br><span class="line"></span><br><span class="line">docker exec -it ragllm_luke bash</span><br><span class="line"></span><br><span class="line">conda activate ragllm</span><br><span class="line"></span><br><span class="line">cd /root/LLMChat</span><br><span class="line"></span><br><span class="line">sh start.sh # 一键启动</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">sh start_webui.sh <span class="comment">#启动ui</span></span></span><br></pre></td></tr></table></figure><h2 id="服务器代理设置的一些命令（没成功）"><a href="#服务器代理设置的一些命令（没成功）" class="headerlink" title="服务器代理设置的一些命令（没成功）"></a>服务器代理设置的一些命令（没成功）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">alias proxy=&#x27;export https_proxy=http://10.11.13.29:7890 http_proxy=http://10.11.13.29:7890 all_proxy=socks5://10.11.13.29:7890&#x27;</span><br><span class="line">alias unproxy=&#x27;unset all_proxy http_proxy https_proxy&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置代理</span></span><br><span class="line">git config --global http.proxy http://192.168.64.1:7890</span><br><span class="line">git config --global https.proxy https://192.168.64.1:7890</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重置取消代理</span></span><br><span class="line">git config --global --unset http.proxy</span><br><span class="line">git config --global --unset https.proxy</span><br><span class="line"></span><br><span class="line">nc -vz -w 2 10.11.13.29 7890</span><br><span class="line">telnet 10.11.13.29 7890</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">指定只允许特定 IP 访问端口</span></span><br><span class="line">pass in on en0 proto tcp from 192.168.1.2 to any port 7890</span><br><span class="line">pass out proto tcp from any to any port 7890</span><br><span class="line">sudo pfctl -f /etc/pf.conf</span><br><span class="line">sudo pfctl -e</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp /Users/luke/Downloads/logo/logo.png luchenhao@10.101.14.36:/home/luchenhao/workspace/LLMChat/img</span><br><span class="line"></span><br><span class="line">cp /mnt/nas_self-define/luchenhao/bge-large-zh/pytorch_model.bin ./</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">pip install python-multipart</span><br><span class="line">  481  sh init_database.sh</span><br><span class="line">  482  sh start.sh</span><br><span class="line">  483  sh init_database.sh</span><br><span class="line">  484  sh start.sh</span><br><span class="line">  485  cd /root/LLMChat</span><br><span class="line">  486  sh start.sh</span><br><span class="line">  487  pip install sentence_transformers</span><br><span class="line">  488  sh start.sh</span><br><span class="line">  489  pip install faiss-gpu</span><br><span class="line">  490  sh start.sh</span><br><span class="line">  491  cd /root/LLMChat</span><br><span class="line">  492  pip install unstructured[all-docs]==0.11.0</span><br><span class="line">  493  pip install langchain==0.0.354</span><br><span class="line">  494  pip install langchain-experimental==0.0.47</span><br><span class="line">  495  cd /root/LLMChat</span><br><span class="line">  496  sh start.sh</span><br><span class="line">  497  pip install --upgrade langchain</span><br><span class="line">  498  python</span><br><span class="line">  499  pip install --upgrade langchain</span><br><span class="line">  500  sh start.sh</span><br><span class="line">  501  ll</span><br><span class="line">  502  pip install -r requirements.txt</span><br><span class="line">  503  conda install faiss-gpu -c conda-forge</span><br><span class="line">  504  sh start.sh</span><br><span class="line">  505  pip install -r requirements.txt</span><br><span class="line">  506  cd /root/LLMChat</span><br><span class="line">  507  pip install -r requirements.txt</span><br><span class="line">  508  ll</span><br><span class="line">  509  sh start.sh</span><br><span class="line">  510  pip install fitz</span><br><span class="line">  511  sh start.sh</span><br><span class="line">  512  pip uninstall fitz</span><br><span class="line">  513  pip install -r requirements.txt</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>大模型流程构建工具调研</title>
      <link href="/2024/01/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E8%B0%83%E7%A0%94/"/>
      <url>/2024/01/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h3 id="通用大模型"><a href="#通用大模型" class="headerlink" title="通用大模型"></a>通用大模型</h3><h3 id="垂直大模型"><a href="#垂直大模型" class="headerlink" title="垂直大模型"></a>垂直大模型</h3><ul><li>在通用大模型基础上，结合自身行业数据进行微调，可能带有外挂知识库</li><li>结合特定行业的数据改变数据的分布，重新进行预训练，直接打造行业大模型</li><li>(文本，专属预言)pair对大模型进行微调，完成<strong>【用户输入 – 大模型 – 专属语言输出 – 自有 AI 模型 – 业务结果输出】的全过程</strong></li></ul><blockquote><p>什么样的问题必须依靠大模型的能力？</p></blockquote><p><strong>要用大模型，就必须关注到业务场景中是否有非结构化的数据需要处理</strong>。</p><h2 id="通义千问"><a href="#通义千问" class="headerlink" title="通义千问"></a>通义千问</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install dashscope --upgrade</span><br></pre></td></tr></table></figure><h2 id="ChatGLM-LangChain"><a href="#ChatGLM-LangChain" class="headerlink" title="ChatGLM + LangChain"></a>ChatGLM + LangChain</h2><ol><li><p>ChatGLM-6B</p><ul><li>自我认知能力</li><li>提纲写作能力</li><li>文案写作能力</li><li>信息抽取能力</li></ul></li><li><p>ChatGLM-6B应用：大语言模型基于通识知识进行训练，在面对实际场景时，需要借助模型微调或提示词工程提升语言模型应用效果，具体包括a.垂直领域知识 b.基于私有数据的问答</p></li></ol><div class="table-container"><table><thead><tr><th></th><th>是什么</th><th>使用场景</th></tr></thead><tbody><tr><td>微调/精调</td><td>针对预先训练的语言模型，在特定任务的少量数据集上对其进行进一步训练</td><td>当任务或域定义明确，并且有足够的标记数据可供训练时，通常使用微调过程。</td></tr><tr><td>提示词工程</td><td>涉及设计自然语言提示或指令，可以指导语言模型执行特定任务</td><td>最适合需要高精度和明确输出的任务。提示工程可用于制作引发所需输出的查询。</td></tr></tbody></table></div><ul><li>langchain-chatchat本身就是基于提示词工程的应用</li></ul><ol><li><p>LangChain：是一个用于开发由语言模型驱动的应用程序的框架</p><ul><li>功能模块：<ul><li>Modules：支持的模型类型和集成</li><li>Prompt：提示词管理、优化和序列化</li><li>Memory：内存是指在链/代理调用之间持续存在的状态</li><li>Indexes：当语言模型与特定于应用程序的数据相结合时，会变得更加强大-此模块包含用于加载、查询和更新外部数据的接口和集成</li><li>Chain：链是结构化的调用序列</li><li>Agents：代理是一个链，其中LLM在给定高级指令和一组工具的情况下，反复决定操作，执行操作并观察结果，直到高级指令完成</li><li>Callbacks：回调允许您记录和流式传输任何链的中间步骤，从而轻松观察、调试和评估应用程序的内部</li></ul></li><li>应用场景：文档问答、个人助理、查询表格数据、与API交互(newbing)、信息提取、文档总结</li></ul></li><li><p>基于单一文档问答的实现原理</p><p><img src="/2024/01/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E8%B0%83%E7%A0%94/截屏2024-01-25 15.21.51.png" alt="截屏2024-01-25 15.21.51"></p></li><li><p>LangChain早期项目的流程原理图</p><p><img src="/2024/01/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E8%B0%83%E7%A0%94/v2-c8bde0f196644089b53667994021bbcb_r.jpg" alt="img"></p><p>a) 简单来说就是把本地的一些文档( doc txt md csv json …) 先通过一系列处理（ 读取 分词 ）embedding模型编码成一定数量的高维向量 <strong>（图中 1到6）</strong></p><p>b) 而用户原本直接和LLM对话的文本 也会通过embedding 模型编码成高维向量 <strong>（图中 8 9）</strong></p><p>c) 然后通过计算<strong>余弦相似度</strong>的方式 （<strong>图中10和7）</strong> 来检索本地文档库中可能提供帮助的相关资料</p><p>d) 再和原用户的问题文本 结合 （<strong>图中11）</strong></p><p>e) 经过预先我们准备好的提示词模板 Prompt Template 组装成最后的 Prompt 提示词 （<strong>图中12 13）</strong></p><p>f) 去问LLM （<strong>图中14 15）</strong></p><p><strong><u>代码实现</u></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动模型</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).half().cuda()</span><br><span class="line">chatglm = model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> UnstructuredFileLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义文件路径</span></span><br><span class="line">filepath = <span class="string">&quot;test.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文件</span></span><br><span class="line">loader = UnstructuredFileLoader(filepath)</span><br><span class="line">docs = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本分割</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">500</span>, chunk_overlap=<span class="number">200</span>)</span><br><span class="line">docs = text_splitter.split_text(docs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建向量库</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vector_store = FAISS.from_documents(docs, embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据提问匹配上下文</span></span><br><span class="line">query = <span class="string">&quot;Langchain能够接入哪些数据类型？&quot;</span></span><br><span class="line">docs = vector_store.similarity_search(query)</span><br><span class="line">context = [doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造Prompt</span></span><br><span class="line">prompt = <span class="string">f&quot;已知信息: \n<span class="subst">&#123;context&#125;</span>\n根据已知信息回答问题：\n<span class="subst">&#123;query&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># llm生成回答</span></span><br><span class="line">chatglm.chat(tokenizer, prompt, history=&#123;&#125;)</span><br></pre></td></tr></table></figure></li><li><p>LangChain + ChatGLM</p><p><strong><em>针对langchain改进的点：</em></strong></p><ul><li>除了openai外可适配更多的开源模型</li><li>相比于langchain的document_loader可适配更多的数据源</li><li>在分句、文档读取等方面，针对中文使用场景进行了优化</li></ul><p><strong><em>具体模块体现在：</em></strong></p><ul><li>models：llm的接口类与实现类，原本langchain只支持chatgpt的流式输出，chatchat增加了对很多开源模型的流式输出支持</li><li>loader：文档加载器的实现类，预置了更多pdf_loaders，对中文的支持更好</li><li>Textsplitter: 文本切分的实现类，对中文的切分更好</li><li>chains：工作链路实现（后续会增加库表、知识图谱等chain），目前基于chains/local_doc_qa实现基于本地文档的问答实现</li><li>content：用于存储上传的原始文件，对原始信息的准确度匹配性检查</li><li>vector_store: 用于存储向量库文件，包括.index文件等，即本地知识库本体</li><li>configs：存储文件配置项信息</li></ul></li><li><p>做本地知识库的时候，值得优化的方向</p><ul><li>对llm使用整理好的具有明确标记的问答来进行微调</li><li>对通用领域的embedding模型进行基于专业领域数据的再训练</li><li>文档加工：1）文本分段重排 2）文档加工，在文本分段后，对每段分别进行总结，基于总结内容语义进行匹配</li><li>借助不同模型能力，数据库接入、知识图谱接入等场景可利用text2sql或text2cpyher等模型，再把结果给到语言模型来生成最终的回答</li></ul></li></ol><h2 id="LlamaIndex"><a href="#LlamaIndex" class="headerlink" title="LlamaIndex"></a>LlamaIndex</h2><blockquote><p><code>LlamaIndex</code> 是一个用于 <code>LLM</code> 应用程序的数据框架，用于注入，结构化，并访问私有或特定领域数据。</p></blockquote><ul><li>LlamaIndex的目的是使用私有或特定领域数据来增强这些模型。这些数据可能分布在不同的应用程序和数据存储中。它们可能存在于API之后、SQL数据库中，或者存在在PDF文件以及幻灯片中。</li></ul><h2 id="大模型生态"><a href="#大模型生态" class="headerlink" title="大模型生态"></a>大模型生态</h2><p><img src="/2024/01/25/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E8%B0%83%E7%A0%94/v2-c2720036906e2fb9c00850c75309dbd8_r.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大模型应用 </tag>
            
            <tag> LangChain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Hexo和Butterfly搭建个人博客</title>
      <link href="/2024/01/16/%E5%9F%BA%E4%BA%8EHexo%E5%92%8CButterfly%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2024/01/16/%E5%9F%BA%E4%BA%8EHexo%E5%92%8CButterfly%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><ol><li><p>创建一篇博客：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hexo new <span class="string">&quot;我的第一篇博客&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>本地部署：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure></li><li><p>远端部署：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hexo g &amp;&amp; sudo hexo d</span><br></pre></td></tr></table></figure></li><li><p>清理文件夹缓存：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hexo clean</span><br></pre></td></tr></table></figure></li></ol><h2 id="常见问题解决"><a href="#常见问题解决" class="headerlink" title="常见问题解决"></a>常见问题解决</h2><ol><li><p><img src="/2024/01/16/%E5%9F%BA%E4%BA%8EHexo%E5%92%8CButterfly%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/截屏2024-01-26 09.59.42.png" alt="截屏2024-01-26 09.59.42"></p></li></ol><p>The ssh key doesn’t add to cmd environment.Using ssh-add to add the key.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-add ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure><ol><li><p>表格无法正确显示</p><p> 解决办法：表格前面需要空一行，即可正确显示</p></li></ol><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://yushuaigee.gitee.io/2020/12/31/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2(%E4%B8%80">从零开始免费搭建自己的博客(一)——本地搭建hexo框架</a>%E2%80%94%E2%80%94%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAhexo%E6%A1%86%E6%9E%B6/)</li><li><a href="https://butterfly.js.org/posts/dc584b87/">Butterfly 安裝文檔</a></li><li><a href="https://blog.51cto.com/u_15127636/3257851">Butterfly美化</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 搭建博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGLM-6B推理</title>
      <link href="/2023/12/26/ChatGLM-6B%E6%8E%A8%E7%90%86/"/>
      <url>/2023/12/26/ChatGLM-6B%E6%8E%A8%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ssh luchenhao@10.101.14.36</span><br><span class="line">$ passwd <span class="comment">#设置密码</span></span><br><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub luchenhao@10.101.14.36 <span class="comment">#配置免密登录，本机传服务器</span></span><br><span class="line"><span class="comment">#修改~/.ssh/config文件</span></span><br><span class="line">$ vi ~/.ssh/config</span><br><span class="line">```</span><br><span class="line">Host 10.199.160.252</span><br><span class="line">  HostName 10.199.160.252</span><br><span class="line">  User song</span><br><span class="line">  PreferredAuthentications publickey</span><br><span class="line">  IdentityFile /Users/adenialzz/.ssh/id_rsa</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p><a href="https://www.runoob.com/docker/docker-container-usage.html">docker命令</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -itd -u root --name chatglm2-6b --gpus all 10.101.12.128/wxzhang/modelscope:ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-1.10.0_final</span><br><span class="line">docker exec -it a3e9 /bin/bash</span><br><span class="line">pip install protobuf transformers==4.30.2 cpm_kernels torch&gt;=2.0 gradio mdtex2html sentencepiece accelerate</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/mnt/nas_self-define/luchenhao/test</span><br><span class="line">mount -t nfs -o vers=3,nolock unifs-std-nas225.alkaidos.cn:/mnt/storctl-nfs/unifs-h001/dros/DROS/users/1723876407693508609/vol-7403253828236464128 /mnt/nas_self-define/luchenhao/LLMs</span><br><span class="line"></span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00001-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00002-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00003-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00004-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00005-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00006-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/pytorch_model-00007-of-00007.bin&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/tokenizer.model&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br><span class="line">scp &#x27;/Users/luke/Downloads/ChatGLM2-6B/MODEL_LICENSE&#x27; luchenhao@10.101.14.36:/mnt/public_model/ChatGLM2-6B/1/chatglm2-6b</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#退出容器</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line">#停止当前运行的容器</span><br><span class="line">docker stop $CONTAINER_ID</span><br><span class="line"></span><br><span class="line"># docker commit [CONTAINER_ID] [image_name:tag]</span><br><span class="line">docker commit a3e95b4d0754 luchenhao/ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-chatglm2_6b:v1</span><br><span class="line"></span><br><span class="line">docker run -itd -u root --name chatglm2-6b-v1 --gpus all -v /mnt/public_model/ChatGLM2-6B:/home/ChatGLM2-6B luchenhao/ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-chatglm2_6b:v1</span><br><span class="line"></span><br><span class="line">docker exec -it chatglm2-6b-v1 /bin/bash</span><br><span class="line"></span><br><span class="line">docker login 10.101.12.128</span><br><span class="line"></span><br><span class="line">docker tag luchenhao/ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-chatglm3_6b:v1 10.101.12.128/llm/chatglm3_6b:ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0</span><br><span class="line"></span><br><span class="line">docker push 10.101.12.128/llm/chatglm3_6b:ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd -u root --name chatglm3-6b --gpus all 10.101.12.128/llm/chatglm2_6b:ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0</span><br><span class="line">docker exec -it chatglm3-6b /bin/bash</span><br><span class="line">docker stop 37dceac07b9a</span><br><span class="line">docker commit 37dceac07b9a luchenhao/ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-chatglm3_6b:v1</span><br><span class="line">docker run -itd -u root --name chatglm3-6b-v1 --gpus all -v /mnt/public_model/ChatGLM3-6B:/home/ChatGLM3-6B luchenhao/ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0-chatglm3_6b:v1</span><br><span class="line">docker exec -it chatglm3-6b-v1 /bin/bash</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd -u root --name llama2-13b --gpus all -v /mnt/public_model/Llama-2-13b-chat-ms:/home/Llama-2-13b-chat-ms 10.101.12.128/llm/chatglm3_6b:ubuntu22.04-cuda11.8.0-py310-torch2.1.0-tf2.14.0</span><br><span class="line">docker exec -it llama2-13b /bin/bash</span><br></pre></td></tr></table></figure><h2 id="推理测评"><a href="#推理测评" class="headerlink" title="推理测评"></a>推理测评</h2>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ChatGLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GLM预训练</title>
      <link href="/2023/12/14/GLM%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
      <url>/2023/12/14/GLM%E9%A2%84%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<h1 id="GLM-7B预训练过程"><a href="#GLM-7B预训练过程" class="headerlink" title="GLM-7B预训练过程"></a>GLM-7B预训练过程</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ssh root@10.106.86.1 -p 31631 (pass:yangfei@2023)</span><br><span class="line">$ <span class="built_in">cat</span> /etc/mpi/hostfile</span><br><span class="line">$ ssh yangfei-science-computer-app-worker-0.yangfei-science-computer-app</span><br><span class="line">$ <span class="built_in">cd</span> /mnt/nas/1/GLM-main</span><br></pre></td></tr></table></figure><h2 id="准备hostfile"><a href="#准备hostfile" class="headerlink" title="准备hostfile"></a>准备hostfile</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi hostfile</span><br><span class="line">yangfei-science-computer-app-master-0.yangfei-science-computer-app slots=8</span><br><span class="line">yangfei-science-computer-app-worker-0.yangfei-science-computer-app slots=8</span><br><span class="line">yangfei-science-computer-app-worker-1.yangfei-science-computer-app slots=8</span><br><span class="line">yangfei-science-computer-app-worker-2.yangfei-science-computer-app slots=8</span><br></pre></td></tr></table></figure><h2 id="相关辅助工具库安装"><a href="#相关辅助工具库安装" class="headerlink" title="相关辅助工具库安装"></a>相关辅助工具库安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ pip install sentencepiece</span><br><span class="line">$ yum install pdsh</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><ol><li><p><code>scripts/ds_pretrain_nvidia.sh</code></p><p> 脚本 scripts/ds_pretrain_nvidia.sh 会启动 DeepSpeed 训练程序。应将 NUM_WORKERS 和 NUM_GPUS_PER_WORKER 更改为 Worker 数量和每个 Worker 的 GPU 数量。还应将 HOST_FILE_PATH 改为 OpenMPI 风格主机文件的路径。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Change <span class="keyword">for</span> multinode config</span></span><br><span class="line"></span><br><span class="line">NUM_WORKERS=1</span><br><span class="line">NUM_GPUS_PER_WORKER=8</span><br><span class="line">MP_SIZE=8 #模型并行参数</span><br><span class="line">MASTER_PORT=$(shuf -n 1 -i 10000-65535)</span><br><span class="line"></span><br><span class="line">source $1</span><br><span class="line">DATESTR=$(date +&quot;%m-%d-%H-%M&quot;)</span><br><span class="line"></span><br><span class="line">OPTIONS_NCCL=&quot;NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2&quot;</span><br><span class="line">HOST_FILE_PATH=&quot;/mnt/nas/1/GLM-main/hostfile&quot;</span><br><span class="line"></span><br><span class="line">mkdir logs</span><br><span class="line">run_cmd=&quot;$&#123;OPTIONS_NCCL&#125; deepspeed --master_port $&#123;MASTER_PORT&#125; --num_nodes $&#123;NUM_WORKERS&#125; --num_gpus $&#123;NUM_GPUS_PER_WORKER&#125; --hostfile $&#123;HOST_FILE_PATH&#125; pretrain_glm.py $&#123;gpt_options&#125; 2&gt;&amp;1 | tee logs/log-$&#123;DATESTR&#125;.txt&quot;</span><br><span class="line">echo $&#123;run_cmd&#125;</span><br><span class="line">eval $&#123;run_cmd&#125;</span><br><span class="line"></span><br><span class="line">set +x</span><br></pre></td></tr></table></figure></li></ol><ol><li><p><code>config/ds_block_10B.sh</code></p><p> 文件 config/ds_block_large.sh 定义了用于预训练的超参数。大部分参数不言自明。具体来说，—train-data 可以是 data_utils/corpora.py 中 NAMED_CORPORA 中定义的多个关键字。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">script_path=$(realpath $BASH_SOURCE)</span><br><span class="line">script_dir=$(dirname $script_path)</span><br><span class="line"></span><br><span class="line">config_json=&quot;$script_dir/config_block_large.json&quot;</span><br><span class="line">gpt_options=&quot; \</span><br><span class="line">       --block-lm \</span><br><span class="line">       --bert-prob 1.0 \</span><br><span class="line">       --avg-block-length 3 \</span><br><span class="line">       --experiment-name blocklm-large-blank \</span><br><span class="line">       --model-parallel-size $&#123;MP_SIZE&#125; \</span><br><span class="line">       --num-layers 28 \</span><br><span class="line">       --hidden-size 4096 \</span><br><span class="line">       --num-attention-heads 32 \</span><br><span class="line">       --seq-length 2048 \</span><br><span class="line">       --max-position-embeddings 2048 \</span><br><span class="line">       --save /root/data/checkpoints \</span><br><span class="line">       --train-iters 200000 \</span><br><span class="line">       --resume-dataloader \</span><br><span class="line">       --train-data gpt \</span><br><span class="line">       --tokenizer-type BertWordPieceTokenizer \</span><br><span class="line">       --tokenizer-model-type bert-large-uncased \</span><br><span class="line">       --split 949,50,1 \</span><br><span class="line">       --distributed-backend nccl \</span><br><span class="line">       --lr-decay-style cosine \</span><br><span class="line">       --lr-decay-iters 160000 \</span><br><span class="line">       --lr-decay-ratio 0.05 \</span><br><span class="line">       --warmup .05 \</span><br><span class="line">       --checkpoint-activations \</span><br><span class="line">       --deepspeed-activation-checkpointing \</span><br><span class="line">       --log-interval 1 \</span><br><span class="line">       --fp16 \</span><br><span class="line">&quot;</span><br><span class="line">gpt_options=&quot;$&#123;gpt_options&#125;</span><br><span class="line">               --deepspeed \</span><br><span class="line">               --deepspeed_config $&#123;config_json&#125; \</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure></li></ol><ol><li><p><code>config/config_block_10B.json</code>: DeepSpeed Configuration JSON，优化器的超参数定义</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;train_micro_batch_size_per_gpu&quot;: 4, #batch-size</span><br><span class="line">  &quot;gradient_accumulation_steps&quot;: 1,</span><br><span class="line">  &quot;steps_per_print&quot;: 100,</span><br><span class="line">  &quot;gradient_clipping&quot;: 1.0,</span><br><span class="line">  &quot;fp16&quot;: &#123;</span><br><span class="line">    &quot;enabled&quot;: true,</span><br><span class="line">    &quot;loss_scale&quot;: 0,</span><br><span class="line">    &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">    &quot;hysteresis&quot;: 2,</span><br><span class="line">    &quot;min_loss_scale&quot;: 1</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;optimizer&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;Adam&quot;,</span><br><span class="line">    &quot;params&quot;: &#123;</span><br><span class="line">      &quot;lr&quot;: 0.0002,</span><br><span class="line">      &quot;weight_decay&quot;: 0.1,</span><br><span class="line">      &quot;betas&quot;: [</span><br><span class="line">        0.9,</span><br><span class="line">        0.98</span><br><span class="line">      ],</span><br><span class="line">      &quot;eps&quot;: 1e-6</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;activation_checkpointing&quot;: &#123;</span><br><span class="line">    &quot;partition_activations&quot;: false,</span><br><span class="line">    &quot;contiguous_memory_optimization&quot;: false</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="GFLOPs运算"><a href="#GFLOPs运算" class="headerlink" title="GFLOPs运算"></a>GFLOPs运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方式</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> thop <span class="keyword">import</span> profile</span><br><span class="line"><span class="comment"># 创建ResNet-50模型</span></span><br><span class="line">model = models.resnet50()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个随机输入向量，模拟输入图像</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用thop的profile函数来统计计算量</span></span><br><span class="line">flops, params = profile(model, inputs=input_tensor,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印计算量信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;FLOPS: <span class="subst">&#123;flops / <span class="number">1e9</span>&#125;</span> GFLOPs&quot;</span>) <span class="comment"># 转换为GFLOPs（十亿次浮点操作）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#第二种方式</span></span><br><span class="line"><span class="comment"># General TFLOPs formula (borrowed from Equation 3 in Section 5.1 of</span></span><br><span class="line"><span class="comment"># https://arxiv.org/pdf/2104.04473.pdf).</span></span><br><span class="line"><span class="comment"># The factor of 4 is when used with activation check-pointing,</span></span><br><span class="line"><span class="comment"># otherwise it will be 3, but for 200B model, activation check-pointing will always be on.</span></span><br><span class="line">checkpoint_activations_factor = <span class="number">4</span> <span class="keyword">if</span> args.checkpoint_activations <span class="keyword">else</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># GLU activations double the hidden states in the upscaling feed-forward in each transformer layer</span></span><br><span class="line"><span class="comment"># This leads to 16bsh^2 instead of 8bsh^2 per first feed-forward layer in MLP, thus we increase the coefficient by 8.</span></span><br><span class="line"><span class="comment"># Refer to https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/283#issue-1260805063 for more details.</span></span><br><span class="line">coefficient = <span class="number">32</span> <span class="keyword">if</span> args.glu_activation <span class="keyword">else</span> <span class="number">24</span></span><br><span class="line">flops_per_iteration = (coefficient * checkpoint_activations_factor * args.batch_size * args.seq_length * args.num_layers * (args.hidden_size**<span class="number">2</span>)) * (<span class="number">1.</span> + (args.seq_length / (<span class="number">6.</span> * args.hidden_size)) + (args.vocab_size / (<span class="number">16.</span> * args.num_layers * args.hidden_size)))</span><br><span class="line">tflops = flops_per_iteration / (elapsed_time_per_iteration * args.world_size * (<span class="number">10</span>**<span class="number">12</span>))</span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_pretrain_nvidia.sh config/ds_block_large.sh</span><br></pre></td></tr></table></figure><h2 id="计算GPUs平均功率脚本"><a href="#计算GPUs平均功率脚本" class="headerlink" title="计算GPUs平均功率脚本"></a>计算GPUs平均功率脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="built_in">times</span>=<span class="variable">$1</span></span><br><span class="line"><span class="built_in">sum</span>=0</span><br><span class="line">cnt=0</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>((i=<span class="number">0</span>;i&lt;<span class="variable">$&#123;times&#125;</span>;i++));</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  out=`hy-smi -P --csv|<span class="built_in">cut</span> -d<span class="string">&#x27;,&#x27;</span> -f 3`</span><br><span class="line">  IFS=$<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">  <span class="keyword">for</span> val <span class="keyword">in</span> <span class="variable">$out</span>;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$val</span> =~ ^[0-9]*\.?[0-9]+$ ]];</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="variable">$val</span></span><br><span class="line">      <span class="built_in">sum</span>=`<span class="built_in">echo</span> <span class="variable">$sum</span> + <span class="variable">$val</span> | bc`</span><br><span class="line">      ((cnt += <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">  <span class="built_in">sleep</span> 0.5</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$sum</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$cnt</span></span><br><span class="line">ave=`<span class="built_in">echo</span> <span class="string">&quot;scale=2;<span class="variable">$sum</span>/<span class="variable">$cnt</span>&quot;</span> | bc`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;平均功率为：&quot;</span><span class="variable">$ave</span></span><br></pre></td></tr></table></figure><h2 id="测试参数配置及结果"><a href="#测试参数配置及结果" class="headerlink" title="测试参数配置及结果"></a>测试参数配置及结果</h2><div class="table-container"><table><thead><tr><th style="text-align:center">资源配置</th><th style="text-align:center">序列长度</th><th>模型并行参数</th><th>模型精度</th><th>时间（s)</th><th>吞吐率/gpu</th><th>加载时间（s）</th><th>平均功率/gpu（W）</th><th>空载(W)</th><th>实际算力/gpu（GFLOPs）</th><th>线性加速比</th><th>能效</th></tr></thead><tbody><tr><td style="text-align:center">1n8g4b</td><td style="text-align:center">2048</td><td>MP_SIZE=8</td><td>fp16</td><td>29.187</td><td>4/(29.187*8)</td><td>65.96</td><td>79.84</td><td>41.47</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">2n8g4b</td><td style="text-align:center"></td><td></td><td></td><td></td><td>4/(85.453*16)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">4n8g4b</td><td style="text-align:center"></td><td></td><td></td><td></td><td>4/(74.08*32)</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 大模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ChatGLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法题Day3</title>
      <link href="/2023/12/02/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day4/"/>
      <url>/2023/12/02/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day4/</url>
      
        <content type="html"><![CDATA[<h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h1><blockquote><p>链表是线性表的链式存储结构，数组是线性表的顺序存储结构，链表可以解决数组的不足</p></blockquote><ol><li>链表由两部分组成，一个是指针域(存放指向下一个节点的指针）、一个是数据域（存放元素）</li><li>链表的每个元素都存储了下一个元素的地址，因此在内存中不是连续的（分配机制取决于操作系统的内存管理）</li><li>相比数组的劣势：<ul><li>无法跳跃式的读取元素，因为你不能知道最后一个元素的内存地址，既只能<code>顺序访问</code></li></ul></li><li>相比数组的优势：<ul><li>只要有足够的内存空间，就能为链表分配内存。而数组就算有足够的内存空间，也不一定能放下，因为数组要求连续的空间。</li></ul></li><li>链表的类型：<ul><li>单链表</li><li>双链表（两个指针域）</li><li>循环链表</li></ul></li></ol><h3 id="常见链表操作"><a href="#常见链表操作" class="headerlink" title="常见链表操作"></a>常见链表操作</h3><ul><li><p>删除节点：只要将C节点的next指针指向E节点就可以，Python会自动回收D节点的内存，C++则需要手动释放这块内存，该操作的时间复杂度是$O(1)$，但是查到D元素的时间复杂度是$O(n)$，因为链表需要顺序访问</p></li><li><p>添加节点：将C指向F，F再指向D，时间复杂度是$O(1)$</p></li><li><p>查找节点：时间复杂度是$O(n)$</p></li></ul><h3 id="链表-vs-数组"><a href="#链表-vs-数组" class="headerlink" title="链表 vs 数组"></a>链表 vs 数组</h3><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:center">数组</th><th style="text-align:center">链表</th></tr></thead><tbody><tr><td style="text-align:right">读取</td><td style="text-align:center">$O(1)$</td><td style="text-align:center">$O(n)$</td></tr><tr><td style="text-align:right">插入</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td></tr><tr><td style="text-align:right">删除</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td></tr><tr><td style="text-align:right">适用场景</td><td style="text-align:center">数据量固定，频繁查询，较少增删</td><td style="text-align:center">数据量不固定，频繁增删，较少查询</td></tr></tbody></table></div><h3 id="链表的定义（要会写！！）"><a href="#链表的定义（要会写！！）" class="headerlink" title="链表的定义（要会写！！）"></a>链表的定义（要会写！！）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ListNode</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,val,<span class="built_in">next</span>=<span class="literal">None</span></span>):</span><br><span class="line">self.val = val <span class="comment">#数据域</span></span><br><span class="line">self.<span class="built_in">next</span> = <span class="built_in">next</span> <span class="comment">#指针域</span></span><br></pre></td></tr></table></figure><h2 id="203-移除链表元素"><a href="#203-移除链表元素" class="headerlink" title="203.移除链表元素"></a>203.移除链表元素</h2><ul><li><p><a href="https://leetcode.cn/problems/remove-linked-list-elements/">题目链接</a></p><p>给你一个链表的头节点 <code>head</code> 和一个整数 <code>val</code> ，请你删除链表中所有满足 <code>Node.val == val</code> 的节点，并返回 <strong>新的头节点</strong> 。</p><p>  <strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：head = [1,2,6,3,4,5,6], val = 6</span><br><span class="line">输出：[1,2,3,4,5]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：head = [7,7,7,7], val = 7</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure></li><li><p>题解：1. <strong>链表的删除操作</strong> 2. <strong>头结点和非头节点的处理</strong></p></li><li><p>视频总结：</p><ul><li><p><u><code>平方之后最大元素一定是在两边</code></u>，数组从两边往里慢慢缩小</p></li><li><p>返回新数组，不要求空间复杂度</p></li><li><p>头结点和非头结点的移除元素的操作是不一样的</p><ul><li><p>非头结点：让前一个节点指向后一个节点</p></li><li><p>头结点：直接指向下一个节点，<code>head = head.next</code></p><p>  <img src="/2023/12/02/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day4/20210316095512470-1766403.png" alt="203_链表删除元素4"></p></li><li><p>统一方法（虚拟头结点)：在链表的最前面加入一个dummy head</p><p>  <img src="/2023/12/02/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day4/20210316095619221.png" alt="203_链表删除元素6"></p></li></ul></li></ul></li></ul><ul><li><p>个人思路：</p></li><li><p>难点：</p><ul><li>平方之后最大的元素一定在两边</li><li>想到头尾双指针操作</li></ul></li><li><p>学习时长：40mins</p></li><li><p>LeetCode代码：</p><ul><li><p>第一种解法(头结点和非头结点分开处理)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElements</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment">#删除头结点，while而不是if的原因是可能有很多个头结点都是1，所以要持续移除</span></span><br><span class="line">        <span class="keyword">while</span> head <span class="keyword">and</span> head.val == val:</span><br><span class="line">        head = head.<span class="built_in">next</span></span><br><span class="line">        <span class="comment">#删除非头结点</span></span><br><span class="line">        <span class="comment">#利用cur指针进行遍历，head指针没有动</span></span><br><span class="line">        cur = head</span><br><span class="line">        <span class="keyword">while</span> cur <span class="keyword">and</span> cur.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.<span class="built_in">next</span>.val == val:</span><br><span class="line">                cur.<span class="built_in">next</span> = cur.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur = cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure><hr><p><em>时间复杂度：</em>$O(n)$</p><p><em>空间复杂度：</em>$O(1)$</p></li><li><p>第二种解法(头结点和非头结点一起处理)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">  </span><br></pre></td></tr></table></figure><hr><p><em>时间复杂度：</em>$O(n)$</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数组小结</title>
      <link href="/2023/12/01/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day3/"/>
      <url>/2023/12/01/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day3/</url>
      
        <content type="html"><![CDATA[<h1 id="数组小结"><a href="#数组小结" class="headerlink" title="数组小结"></a>数组小结</h1><blockquote><p>数组是存放在连续内存空间上的相同类型数据的集合。</p></blockquote><p><img src="/2023/12/01/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day3/算法通关数组-7920053.png" alt="算法通关数组"></p><ol><li><p>数组的元素在内存中都是<code>相连</code>的，超出空间需要挪动整个数组在内存中的位置，因此：</p><p> a. 需要预留内存空间，如果后期用不上会浪费</p></li><li><p>数组相比链表的优势：</p><p> a. 可以迅速找到数组中的任何元素<code>随机访问</code></p></li></ol><ol><li><p>数组相比链表的劣势：</p><p> a. 插入元素需要将后面的元素都向后移动，如果后面的内存空间被占用，还要将整个数组复制到其他地方</p><p> b. <code>增加</code>和<code>删除</code>元素的时候，难免要移动其他元素的地址</p><p> <img src="/2023/12/01/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day3/算法通关数组1.png" alt="算法通关数组1"></p></li></ol><p>其他：</p><ul><li>数组的元素是不能删的，只能<code>覆盖</code>，也就是删掉元素之后把后面的元素往前移动一位，进行覆盖</li><li>很多涉及到数组删除的库函数，时间复杂度都是$O(n)$，因为要移动元素</li><li>要养成预先定义数组的习惯！</li></ul><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:center">数组</th><th style="text-align:center">链表</th></tr></thead><tbody><tr><td style="text-align:right">读取</td><td style="text-align:center">$O(1)$</td><td style="text-align:center">$O(n)$</td></tr><tr><td style="text-align:right">插入</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td></tr><tr><td style="text-align:right">删除</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td></tr><tr><td style="text-align:right">适用场景</td><td style="text-align:center">数据量固定，频繁查询，较少增删</td><td style="text-align:center">数据量不固定，频繁增删，较少查询</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 算法题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法题Day2</title>
      <link href="/2023/11/30/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day2/"/>
      <url>/2023/11/30/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day2/</url>
      
        <content type="html"><![CDATA[<h1 id="代码随想录算法训练营第二天-977-有序数组的平方、209-长度最小的子数组、59-螺旋矩阵II。"><a href="#代码随想录算法训练营第二天-977-有序数组的平方、209-长度最小的子数组、59-螺旋矩阵II。" class="headerlink" title="代码随想录算法训练营第二天| 977. 有序数组的平方、209.长度最小的子数组、59.螺旋矩阵II。"></a>代码随想录算法训练营第二天| 977. 有序数组的平方、209.长度最小的子数组、59.螺旋矩阵II。</h1><h2 id="977-有序数组的平方"><a href="#977-有序数组的平方" class="headerlink" title="977. 有序数组的平方"></a>977. 有序数组的平方</h2><ul><li><p><a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">题目链接</a></p><p>给你一个按 <strong>非递减顺序</strong> 排序的整数数组 <code>nums</code>，返回 <strong>每个数字的平方</strong> 组成的新数组，要求也按 <strong>非递减顺序</strong> 排序。</p><p>  <strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [-4,-1,0,3,10]</span><br><span class="line">输出：[0,1,9,16,100]</span><br><span class="line">解释：平方后，数组变为 [16,1,0,9,100]</span><br><span class="line">排序后，数组变为 [0,1,9,16,100]</span><br></pre></td></tr></table></figure><p>  <strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [-7,-3,2,3,11]</span><br><span class="line">输出：[4,9,9,49,121]</span><br></pre></td></tr></table></figure></li><li><p>题解：1. <strong>有序数组</strong> 2. <strong>平方之后排序</strong></p></li><li><p>视频总结：</p><ul><li><u><code>平方之后最大元素一定是在两边</code></u>，数组从两边往里慢慢缩小</li><li>返回新数组，不要求空间复杂度</li></ul></li><li><p>个人思路：<code>sorted(x*x for x in nums)</code></p></li><li><p>难点：</p><ul><li>平方之后最大的元素一定在两边</li><li>想到头尾双指针操作</li></ul></li><li><p>学习时长：40mins</p></li><li><p>动画：</p></li><li><p><img src="/2023/11/30/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day2/977.有序数组的平方-20231204154122901.gif" alt="img"></p></li><li><p>LeetCode代码：</p><ul><li><p>第一种解法(暴力排序)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暴力解法，先平方，后排序</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedSquares</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            nums[i] *= nums[i]</span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><hr><p><em>时间复杂度：</em>$O(n+nlogn)$</p></li><li><p>第二种解法(双指针法)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedSquares</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        i, j, k = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        res = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * <span class="built_in">len</span>(nums) <span class="comment"># 需要提前定义列表，存放结果</span></span><br><span class="line">        <span class="keyword">while</span> i &lt;= j : <span class="comment"># i等于j的时候，还有一个元素要计算之后传入新数组，所以不能落下</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] * nums[j] &gt;= nums[i] * nums[i]:</span><br><span class="line">                res[k] = nums[j] * nums[j]</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[k] = nums[i] * nums[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><hr><p><em>时间复杂度：</em>$O(n)$</p></li></ul></li></ul><h2 id="209-长度最小的子数组"><a href="#209-长度最小的子数组" class="headerlink" title="209. 长度最小的子数组"></a>209. 长度最小的子数组</h2><ul><li><p><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">题目链接</a></p><p>  给定一个含有 <code>n</code> 个正整数的数组和一个正整数 <code>target</code> <strong>。</strong></p><p>  找出该数组中满足其总和大于等于 <code>target</code> 的长度最小的 <strong>连续子数组</strong> <code>[numsl, numsl+1, ..., numsr-1, numsr]</code> ，并返回其长度<strong>。</strong>如果不存在符合条件的子数组，返回 <code>0</code> 。</p><p>  <strong>示例 1：</strong></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：target = 7, nums = [2,3,1,2,4,3]</span><br><span class="line">输出：2</span><br><span class="line">解释：子数组 [4,3] 是该条件下的长度最小的子数组。</span><br></pre></td></tr></table></figure><p>  <strong>示例 2：</strong></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：target = 4, nums = [1,4,4]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure><p>  <strong>示例 3：</strong></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：target = 11, nums = [1,1,1,1,1,1,1,1]</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure></li><li><p>题解：1. 动态调节大小的滑动窗口（伸缩滑动窗口）</p></li><li><p>视频总结：</p><ul><li>如何用一个for循环做两个for循环的事情 -&gt; “滑动窗口”</li><li>for循环的j指向的是终止位置而非起始位置</li><li>while表示的是一个持续向后移动滑动窗口并且更新窗口大小的过程</li></ul></li><li><p>个人思路：暴力解法（leetcode判题超时）</p></li><li><p>难点：</p><ul><li>滑动窗口“伸出去”、“缩回来”</li></ul></li><li><p>总结：滑动窗口的大小不是固定的，而是通过“伸出去”“缩回来”两个步骤不断更新的动态大小</p></li><li><p>学习时长：30mins</p></li><li><p>LeetCode代码：</p><ul><li><p>暴力解法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        min_ = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        <span class="comment"># 一个for循环控制起始位置</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            cur_sum = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 一个for循环控制终止位置</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(nums)):</span><br><span class="line">                cur_sum += nums[j]</span><br><span class="line">                <span class="keyword">if</span> cur_sum &gt;= target:</span><br><span class="line">                    min_ = <span class="built_in">min</span>(min_, j-i+<span class="number">1</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> min_ == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> min_</span><br></pre></td></tr></table></figure><hr><p><em>时间复杂度：</em>$O(n^2)$ -&gt; 两个嵌套循环</p><p><em>空间复杂度</em>：$O(1)$</p><p><img src="/2023/11/30/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day2/截屏2023-12-04 16.09.09.png" alt="截屏2023-12-04 16.09.09" style="zoom: 50%;"></p></li><li><p>滑动窗口法：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        min_len = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        cur_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            cur_sum += nums[j]</span><br><span class="line">            <span class="keyword">while</span> cur_sum &gt;= target :</span><br><span class="line">                cur_sum -= nums[i]</span><br><span class="line">                min_len = <span class="built_in">min</span>(min_len, j - i + <span class="number">1</span>)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> min_len == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> min_len</span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(n)$ -&gt; 不要以为for里放一个while就以为是$O(n^2)$啊， 主要是看每一个元素被操作的次数，每个元素在滑动窗后进来操作一次，出去操作一次，每个元素都是被操作两次，所以时间复杂度是 $2 × n$ 也就是$O(n)$。</p><p>  <em>空间复杂度：</em>$O(1)$</p></li></ul></li></ul><h2 id="59-螺旋矩阵II"><a href="#59-螺旋矩阵II" class="headerlink" title="59. 螺旋矩阵II"></a>59. 螺旋矩阵II</h2><ul><li><p><a href="https://leetcode.cn/problems/spiral-matrix-ii/">题目链接</a></p><p>  给你一个正整数 <code>n</code> ，生成一个包含 <code>1</code> 到 <code>n2</code> 所有元素，且元素按顺时针顺序螺旋排列的 <code>n x n</code> 正方形矩阵 <code>matrix</code> 。</p><p>  <strong>示例 1：</strong></p><p>  <img src="/2023/11/30/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day2/spiraln.jpg" alt="img"></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：n = 3</span><br><span class="line">输出：[[1,2,3],[8,9,4],[7,6,5]]</span><br></pre></td></tr></table></figure><p>  <strong>示例 2：</strong></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：n = 1</span><br><span class="line">输出：[[1]]</span><br></pre></td></tr></table></figure></li><li><p>题解：1. 哪条边的数据就归哪条边处理，不要乱 2. 永远左闭右开</p></li><li><p>视频总结：</p><ul><li>每一条边的处理规则要统一，也就是要坚持循环不变量</li><li>模拟顺时针画矩阵的过程:<ul><li>填充上行从左到右</li><li>填充右列从上到下</li><li>填充下行从右到左</li><li>填充左列从下到上</li></ul></li></ul></li><li><p>个人思路：</p></li><li><p>难点：</p><ul><li>圈数、偏移量、循环不变量</li></ul></li><li><p>总结：循环不变量，处理规则不变很重要</p></li><li><p>学习时长：45mins</p></li><li><p>LeetCode代码：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateMatrix</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        nums = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="comment">#定义初始螺旋数组</span></span><br><span class="line">        startX, startY=<span class="number">0</span>, <span class="number">0</span> <span class="comment">#起始点</span></span><br><span class="line">        offset = <span class="number">1</span> <span class="comment">#偏移量，为了保证圈子循环缩小尺寸</span></span><br><span class="line">        loop = n // <span class="number">2</span> <span class="comment">#很好推理出来，n的数值和圈数的关系</span></span><br><span class="line">        count = <span class="number">1</span> <span class="comment">#从1开始计数</span></span><br><span class="line">        <span class="keyword">while</span> offset &lt;= loop :</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(startY, n-offset):</span><br><span class="line">                nums[startX][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(startX, n-offset):</span><br><span class="line">                nums[i][n-offset] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-offset, startY, -<span class="number">1</span>):</span><br><span class="line">                nums[n-offset][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-offset, startX, -<span class="number">1</span>):</span><br><span class="line">                nums[i][startX] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            startX += <span class="number">1</span></span><br><span class="line">            startY += <span class="number">1</span></span><br><span class="line">            offset += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            nums[loop][loop] = count <span class="comment">#n为奇数时，矩阵的中心点</span></span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(n^2)$ -&gt; n为数组尺寸，模拟遍历二维矩阵的时间</p><p>  <em>空间复杂度：</em>$O(1)$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法题Day1</title>
      <link href="/2023/11/29/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day1/"/>
      <url>/2023/11/29/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day1/</url>
      
        <content type="html"><![CDATA[<h1 id="代码随想录算法训练营第一天-704-二分查找、27-移除元素。"><a href="#代码随想录算法训练营第一天-704-二分查找、27-移除元素。" class="headerlink" title="代码随想录算法训练营第一天| 704. 二分查找、27. 移除元素。"></a><strong>代码随想录算法训练营第一天| 704. 二分查找、27. 移除元素</strong>。</h1><h2 id="大O表示法"><a href="#大O表示法" class="headerlink" title="大O表示法"></a>大O表示法</h2><blockquote><p><u>使用别人编写的算法时，知道运行时间是很有帮助的！</u></p></blockquote><ol><li>数据规模n，操作单元数f(n)，算法时间增长率和f(n)的增长率相同，这个增长率为O(f(n))，O学术上表示上界，业内一般表示“一般情况”，即数据形式一般的情况。<ul><li>可忽略常数项原因为，通常认为数据量级非常庞大，常数项系数不起决定作用</li><li>大O表示法log可以以任何数字为底，因为<script type="math/tex">log_2^N=log_2^{10}*log_{10}^N</script></li><li>线性时间(linear time)：最多需要猜测的次数与列表长度相同 -&gt; O(n)</li><li>对数时间(log time)：二分查找对应的运行时间为对数时间 -&gt; O(log_n)</li><li>时间排序：指数阶$O(n!)$ &gt; 立方阶$O(n^3)$ &gt; 平方阶$O(n^2)$ &gt; 线性对数阶$O(n*logn)$ &gt; 线性阶$O(n)$ &gt; 对数阶$O(logn)$ &gt; 常数阶$O(C)$</li></ul></li><li>大O表示法的意义在于，通过函数式告诉你运行时间的增速，即随着n增大，时间增长的==幅度==不同<ul><li>大O表示法算出的结果是操作数，n是要处理的元素数量，然后除以每秒可执行的操作得到需要的时间</li><li>大O表示法指出的运行时间是最糟情况下的运行时间，是上限；还有平均情况下的运行时间也要考虑</li></ul></li></ol><p><img src="/2023/11/29/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E8%90%A5Day1/大O表示法.svg" alt="大O表示法"></p><h2 id="数组理论基础"><a href="#数组理论基础" class="headerlink" title="数组理论基础"></a>数组理论基础</h2><ol><li><strong>数组是存放在连续内存空间上的相同类型数据的集合</strong></li><li><strong>数组内存空间的地址是连续的</strong></li><li><strong>数组的元素是不能删的，只能覆盖。</strong></li><li><strong>删除或者增添元素的时候，就难免要移动其他元素的地址。</strong></li></ol><h2 id="704-二分查找"><a href="#704-二分查找" class="headerlink" title="704. 二分查找"></a>704. 二分查找</h2><ul><li><p><a href="https://leetcode.cn/problems/binary-search/">题目链接</a></p></li><li><p>题解：1. <strong>有序数组</strong> 2. <strong>无重复元素</strong></p></li><li><p>视频总结：</p><ul><li><code>while</code>循环的控制条件符号$&lt;$、$&lt;=$</li><li>边界条件<code>right = middle</code> or <code>right = middle - 1</code></li><li>选择<code>左闭右开</code>或者<code>左闭右闭</code></li></ul></li><li><p>个人思路：全都使用闭合区间，每一次做判断的时候+1或者-1</p></li><li><p>难点：</p><ul><li>区间搜索的时候要明确区间定义：左闭右闭[]还是左闭右开[)？左开右闭比较少见。。</li><li>边界处理，left&lt;right还是left&lt;=right,right=middle还是right=middle-1</li><li>循环中根据区间定义做边界处理，就是循环不变量规则：在while循环中坚持一个区间，区间即不变量</li></ul></li><li><p>学习时长：30mins</p></li><li><p>代码展示(伪代码)：</p><ul><li><p>第一种写法（左闭右闭）</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 左闭右闭，合法区间的话left可能等于right</span></span><br><span class="line">left = <span class="number">0</span>, right = numsize - <span class="number">1</span>, target</span><br><span class="line"><span class="keyword">while</span>(left &lt;= right)：</span><br><span class="line">    middle = (left + right)/<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nums[middle] &gt; target:</span><br><span class="line">        right = middle - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> nums[middle] &lt; target:</span><br><span class="line">        left = middle + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> middle</span><br><span class="line"><span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(logn)$ -&gt; 对于包含n个元素的列表，用二分查找最多需要log2_n步，而简单的从头到尾查找需要n步。</p><p>  <em>空间复杂度：</em>$O(1)$</p></li><li><p>第二种写法（左闭右开）</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 左闭右开， 合法区间的话left要小于right</span></span><br><span class="line">left = <span class="number">0</span>, right = numsize, target</span><br><span class="line"><span class="keyword">while</span>(left &lt; right)：</span><br><span class="line">    middle = (left + right)/<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> nums[middle] &gt; target:</span><br><span class="line">        right = middle</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> nums[middle] &lt; target:</span><br><span class="line">        left = middle + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> middle</span><br><span class="line"><span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(logn)$ -&gt; 对于包含n个元素的列表，用二分查找最多需要log2_n步，而简单的从头到尾查找需要n步。</p><p>  <em>空间复杂度：</em>$O(1)$</p></li></ul></li><li><p>LeetCode代码：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right):</span><br><span class="line">            middle = (left+right)//<span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> (nums[middle] &gt; target):</span><br><span class="line">                right = middle - <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> (nums[middle] &lt; target):</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> middle</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="27-移除元素"><a href="#27-移除元素" class="headerlink" title="27. 移除元素"></a>27. 移除元素</h2><ul><li><p><a href="https://leetcode.cn/problems/remove-element/">题目链接</a></p></li><li><p>题解：1. <strong>限制O(1)的空间复杂度</strong></p></li><li><p>视频总结：</p><ul><li>数组元素不能删除，只能覆盖，物理内存空间不会发生改变，原来的位置多余出来变成了垃圾</li><li>如果使用库函数可以非常方便的完成解题，则不要使用库函数，除非对库函数内部实现和复杂度很清楚</li><li>快指针：寻找新数组的元素 ，新数组就是不含有目标元素的数组；慢指针：指向更新 新数组下标的位置</li></ul></li><li><p>个人思路：暴力解法</p></li><li><p>难点：</p><ul><li>如何用一个for循环完成两个for循环的工作</li></ul></li><li><p>总结：相比于暴力解法来说，双指针解法找的是不等于的情况（找需要的元素）暴力解法找的是等于的情况（找要排除的元素）</p></li><li><p>学习时长：30mins</p></li><li><p>LeetCode代码：</p><ul><li><p>暴力解法</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        i, j = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> i &lt; j:</span><br><span class="line">            <span class="keyword">if</span> nums[i] == val:</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, j): <span class="comment">#range右边是开区间，因此取不到，所以j=len(nums)而不是len(nums)-1</span></span><br><span class="line">                    nums[k-<span class="number">1</span>] = nums[k] <span class="comment">#后面整体往前移动一位</span></span><br><span class="line">                i -= <span class="number">1</span> <span class="comment">#因为元素被删掉之后，整体往前移动了，当前位置变成了一个新的数，所以还是要从当前位置开始查找，因此这里-1</span></span><br><span class="line">                j -= <span class="number">1</span> <span class="comment">#因为整体往前移动了，所以尾部缩短一位</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> j</span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(n^2)$ -&gt; 两个循环，遍历两遍</p><p>  <em>空间复杂度：</em>$O(1)$</p></li><li><p>双指针法：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow = fast = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> fast <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> nums[fast] != val:</span><br><span class="line">        nums[slow] = nums[fast]</span><br><span class="line">        slow += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slow <span class="comment">#slow正好在新数组index+1的位置上，所以就等于新数组的长度</span></span><br></pre></td></tr></table></figure><hr><p>  <em>时间复杂度：</em>$O(n)$ </p><p>  <em>空间复杂度：</em>$O(1)$</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
